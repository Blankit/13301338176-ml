{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('iris.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "msk = np.random.rand(len(data)) < 0.5\n",
    "labels = []\n",
    "labels = data['Species'].copy()\n",
    "#test_labels = test['Species'].copy()\n",
    "#df = df.drop(df.columns[[0, 5]], axis=1)\n",
    "data = data.drop(data.columns[[0, 5]], axis=1)\n",
    "labels = pd.get_dummies(labels, columns=['Species'])\n",
    "train_labels = np.array(labels[msk], dtype=np.float32)\n",
    "test_labels = np.array(labels[~msk], dtype=np.float32)\n",
    "train_data = np.array(data[msk], dtype=np.float32)\n",
    "test_data = np.array(data[~msk], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.input_data(shape=[None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.fully_connected(net, 8, activation='relu')\n",
    "net = tf.fully_connected(net, 3, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.regression(net)\n",
    "model = tf.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: QKMSMY\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 76\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.381s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 76/76\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.98914\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 002 | loss: 0.98914 - acc: 0.0000 -- iter: 76/76\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m1.07891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 003 | loss: 1.07891 - acc: 0.2799 -- iter: 76/76\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.09372\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 004 | loss: 1.09372 - acc: 0.3266 -- iter: 76/76\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.09701\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 005 | loss: 1.09701 - acc: 0.3373 -- iter: 76/76\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.09782\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 006 | loss: 1.09782 - acc: 0.3404 -- iter: 76/76\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.09799\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 007 | loss: 1.09799 - acc: 0.3414 -- iter: 76/76\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.09825\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 008 | loss: 1.09825 - acc: 0.3418 -- iter: 76/76\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.09798\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 009 | loss: 1.09798 - acc: 0.3420 -- iter: 76/76\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.09777\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 010 | loss: 1.09777 - acc: 0.3420 -- iter: 76/76\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.09761\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 011 | loss: 1.09761 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.09745\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 012 | loss: 1.09745 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.09729\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 013 | loss: 1.09729 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.09712\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 014 | loss: 1.09712 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.09695\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 015 | loss: 1.09695 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.09676\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 016 | loss: 1.09676 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.09657\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 017 | loss: 1.09657 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.09636\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 018 | loss: 1.09636 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.09615\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 019 | loss: 1.09615 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.09592\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 020 | loss: 1.09592 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.09568\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 021 | loss: 1.09568 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.09544\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 022 | loss: 1.09544 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.09518\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 023 | loss: 1.09518 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.09490\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 024 | loss: 1.09490 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.09462\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 025 | loss: 1.09462 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.09432\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 026 | loss: 1.09432 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.09401\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 027 | loss: 1.09401 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.09369\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 028 | loss: 1.09369 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.09335\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 029 | loss: 1.09335 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.09300\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 030 | loss: 1.09300 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.09264\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 031 | loss: 1.09264 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.09226\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 032 | loss: 1.09226 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.09187\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 033 | loss: 1.09187 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.09147\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 034 | loss: 1.09147 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.09105\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 035 | loss: 1.09105 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.09061\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 036 | loss: 1.09061 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m1.09016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 037 | loss: 1.09016 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.08969\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 038 | loss: 1.08969 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.08921\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 039 | loss: 1.08921 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.08871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 040 | loss: 1.08871 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.08820\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 041 | loss: 1.08820 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.08766\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 042 | loss: 1.08766 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.08711\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 043 | loss: 1.08711 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.08655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 044 | loss: 1.08655 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.08597\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 045 | loss: 1.08597 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.08537\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 046 | loss: 1.08537 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.08475\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 047 | loss: 1.08475 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.08411\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 048 | loss: 1.08411 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.08346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 049 | loss: 1.08346 - acc: 0.3421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.08279\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 050 | loss: 1.08279 - acc: 0.3441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.08211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 051 | loss: 1.08211 - acc: 0.3458 -- iter: 76/76\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.08140\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 052 | loss: 1.08140 - acc: 0.3473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.08068\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 053 | loss: 1.08068 - acc: 0.3484 -- iter: 76/76\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.07994\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 054 | loss: 1.07994 - acc: 0.3494 -- iter: 76/76\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.07919\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 055 | loss: 1.07919 - acc: 0.3503 -- iter: 76/76\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.07841\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 056 | loss: 1.07841 - acc: 0.3510 -- iter: 76/76\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.07762\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 057 | loss: 1.07762 - acc: 0.3497 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.07681\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 058 | loss: 1.07681 - acc: 0.3487 -- iter: 76/76\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.07599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 059 | loss: 1.07599 - acc: 0.3478 -- iter: 76/76\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.07515\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 060 | loss: 1.07515 - acc: 0.3471 -- iter: 76/76\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.07429\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 061 | loss: 1.07429 - acc: 0.3464 -- iter: 76/76\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.07791\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 062 | loss: 1.07791 - acc: 0.3357 -- iter: 76/76\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.07646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 063 | loss: 1.07646 - acc: 0.3365 -- iter: 76/76\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.07510\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 064 | loss: 1.07510 - acc: 0.3372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.07379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 065 | loss: 1.07379 - acc: 0.3378 -- iter: 76/76\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.07253\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 066 | loss: 1.07253 - acc: 0.3399 -- iter: 76/76\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.07131\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 067 | loss: 1.07131 - acc: 0.3418 -- iter: 76/76\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.07012\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 068 | loss: 1.07012 - acc: 0.3434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.06895\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 069 | loss: 1.06895 - acc: 0.3448 -- iter: 76/76\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.06779\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 070 | loss: 1.06779 - acc: 0.3460 -- iter: 76/76\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.06665\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 071 | loss: 1.06665 - acc: 0.3455 -- iter: 76/76\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.06995\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 072 | loss: 1.06995 - acc: 0.3496 -- iter: 76/76\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.06834\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 073 | loss: 1.06834 - acc: 0.3488 -- iter: 76/76\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.06680\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 074 | loss: 1.06680 - acc: 0.3480 -- iter: 76/76\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.06532\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 075 | loss: 1.06532 - acc: 0.3474 -- iter: 76/76\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.06388\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 076 | loss: 1.06388 - acc: 0.3468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.06249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 077 | loss: 1.06249 - acc: 0.3463 -- iter: 76/76\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.06112\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 078 | loss: 1.06112 - acc: 0.3459 -- iter: 76/76\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m1.05978\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 079 | loss: 1.05978 - acc: 0.3455 -- iter: 76/76\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m1.05845\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 080 | loss: 1.05845 - acc: 0.3451 -- iter: 76/76\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m1.05714\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 081 | loss: 1.05714 - acc: 0.3448 -- iter: 76/76\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m1.05584\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 082 | loss: 1.05584 - acc: 0.3446 -- iter: 76/76\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.05453\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 083 | loss: 1.05453 - acc: 0.3443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.05322\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 084 | loss: 1.05322 - acc: 0.3441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.05189\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 085 | loss: 1.05189 - acc: 0.3439 -- iter: 76/76\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.05055\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 086 | loss: 1.05055 - acc: 0.3437 -- iter: 76/76\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m1.04920\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 087 | loss: 1.04920 - acc: 0.3436 -- iter: 76/76\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m1.04783\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 088 | loss: 1.04783 - acc: 0.3434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m1.04646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 089 | loss: 1.04646 - acc: 0.3433 -- iter: 76/76\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m1.04506\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 090 | loss: 1.04506 - acc: 0.3432 -- iter: 76/76\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m1.04366\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 091 | loss: 1.04366 - acc: 0.3444 -- iter: 76/76\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m1.04224\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 092 | loss: 1.04224 - acc: 0.3455 -- iter: 76/76\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m1.04080\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 093 | loss: 1.04080 - acc: 0.3464 -- iter: 76/76\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m1.03935\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 094 | loss: 1.03935 - acc: 0.3473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m1.03789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 095 | loss: 1.03789 - acc: 0.3481 -- iter: 76/76\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m1.03641\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 096 | loss: 1.03641 - acc: 0.3488 -- iter: 76/76\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m1.03491\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 097 | loss: 1.03491 - acc: 0.3495 -- iter: 76/76\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m1.03340\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 098 | loss: 1.03340 - acc: 0.3514 -- iter: 76/76\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.03187\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 099 | loss: 1.03187 - acc: 0.3531 -- iter: 76/76\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.03033\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 100 | loss: 1.03033 - acc: 0.3559 -- iter: 76/76\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m1.02877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 101 | loss: 1.02877 - acc: 0.3598 -- iter: 76/76\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m1.02720\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 102 | loss: 1.02720 - acc: 0.3659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m1.02560\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 103 | loss: 1.02560 - acc: 0.3754 -- iter: 76/76\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m1.02400\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 104 | loss: 1.02400 - acc: 0.3865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m1.02238\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 105 | loss: 1.02238 - acc: 0.4031 -- iter: 76/76\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m1.02074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 106 | loss: 1.02074 - acc: 0.4207 -- iter: 76/76\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m1.01908\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 107 | loss: 1.01908 - acc: 0.4365 -- iter: 76/76\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m1.01741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 108 | loss: 1.01741 - acc: 0.4521 -- iter: 76/76\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m1.01572\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 109 | loss: 1.01572 - acc: 0.4687 -- iter: 76/76\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m1.01402\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 110 | loss: 1.01402 - acc: 0.4837 -- iter: 76/76\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m1.01230\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 111 | loss: 1.01230 - acc: 0.4985 -- iter: 76/76\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m1.01057\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 112 | loss: 1.01057 - acc: 0.5131 -- iter: 76/76\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m1.00882\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 113 | loss: 1.00882 - acc: 0.5263 -- iter: 76/76\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m1.00705\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 114 | loss: 1.00705 - acc: 0.5394 -- iter: 76/76\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m1.00527\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 115 | loss: 1.00527 - acc: 0.5513 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m1.01512\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 116 | loss: 1.01512 - acc: 0.5264 -- iter: 76/76\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m1.01217\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 117 | loss: 1.01217 - acc: 0.5396 -- iter: 76/76\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m1.00933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 118 | loss: 1.00933 - acc: 0.5514 -- iter: 76/76\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m1.00661\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 119 | loss: 1.00661 - acc: 0.5620 -- iter: 76/76\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m1.00397\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 120 | loss: 1.00397 - acc: 0.5716 -- iter: 76/76\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m1.00143\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 121 | loss: 1.00143 - acc: 0.5803 -- iter: 76/76\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.99895\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 122 | loss: 0.99895 - acc: 0.5880 -- iter: 76/76\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.99655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 123 | loss: 0.99655 - acc: 0.5950 -- iter: 76/76\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m1.00519\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 124 | loss: 1.00519 - acc: 0.5776 -- iter: 76/76\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m1.00181\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 125 | loss: 1.00181 - acc: 0.5856 -- iter: 76/76\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.99860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 126 | loss: 0.99860 - acc: 0.5929 -- iter: 76/76\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.99553\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 127 | loss: 0.99553 - acc: 0.5994 -- iter: 76/76\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m1.00872\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 128 | loss: 1.00872 - acc: 0.5684 -- iter: 76/76\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m1.00432\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 129 | loss: 1.00432 - acc: 0.5773 -- iter: 76/76\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m1.00021\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 130 | loss: 1.00021 - acc: 0.5854 -- iter: 76/76\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.99636\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 131 | loss: 0.99636 - acc: 0.5926 -- iter: 76/76\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.99272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 132 | loss: 0.99272 - acc: 0.5992 -- iter: 76/76\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.98928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 133 | loss: 0.98928 - acc: 0.6050 -- iter: 76/76\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.98601\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 134 | loss: 0.98601 - acc: 0.6103 -- iter: 76/76\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.98289\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 135 | loss: 0.98289 - acc: 0.6151 -- iter: 76/76\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.97990\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 136 | loss: 0.97990 - acc: 0.6194 -- iter: 76/76\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.97702\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 137 | loss: 0.97702 - acc: 0.6232 -- iter: 76/76\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.97424\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 138 | loss: 0.97424 - acc: 0.6267 -- iter: 76/76\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.97156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 139 | loss: 0.97156 - acc: 0.6298 -- iter: 76/76\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.96896\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 140 | loss: 0.96896 - acc: 0.6326 -- iter: 76/76\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.96643\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 141 | loss: 0.96643 - acc: 0.6351 -- iter: 76/76\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.96397\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 142 | loss: 0.96397 - acc: 0.6374 -- iter: 76/76\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.96156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 143 | loss: 0.96156 - acc: 0.6395 -- iter: 76/76\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.95921\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 144 | loss: 0.95921 - acc: 0.6413 -- iter: 76/76\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.95691\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 145 | loss: 0.95691 - acc: 0.6430 -- iter: 76/76\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.95465\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 146 | loss: 0.95465 - acc: 0.6445 -- iter: 76/76\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.95242\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 147 | loss: 0.95242 - acc: 0.6458 -- iter: 76/76\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.96799\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 148 | loss: 0.96799 - acc: 0.6167 -- iter: 76/76\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.96406\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 149 | loss: 0.96406 - acc: 0.6209 -- iter: 76/76\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.98201\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 150 | loss: 0.98201 - acc: 0.5851 -- iter: 76/76\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.97638\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 151 | loss: 0.97638 - acc: 0.5924 -- iter: 76/76\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.97116\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 152 | loss: 0.97116 - acc: 0.5989 -- iter: 76/76\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.96633\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 153 | loss: 0.96633 - acc: 0.6048 -- iter: 76/76\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.98024\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 154 | loss: 0.98024 - acc: 0.5838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.97423\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 155 | loss: 0.97423 - acc: 0.5912 -- iter: 76/76\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.96869\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 156 | loss: 0.96869 - acc: 0.5979 -- iter: 76/76\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.96356\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 157 | loss: 0.96356 - acc: 0.6039 -- iter: 76/76\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.95881\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 158 | loss: 0.95881 - acc: 0.6093 -- iter: 76/76\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.95438\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 159 | loss: 0.95438 - acc: 0.6142 -- iter: 76/76\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.95023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 160 | loss: 0.95023 - acc: 0.6185 -- iter: 76/76\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.94633\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 161 | loss: 0.94633 - acc: 0.6225 -- iter: 76/76\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.94265\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 162 | loss: 0.94265 - acc: 0.6260 -- iter: 76/76\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.93917\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 163 | loss: 0.93917 - acc: 0.6292 -- iter: 76/76\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.93586\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 164 | loss: 0.93586 - acc: 0.6321 -- iter: 76/76\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.93271\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 165 | loss: 0.93271 - acc: 0.6346 -- iter: 76/76\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.92970\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 166 | loss: 0.92970 - acc: 0.6370 -- iter: 76/76\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.92681\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 167 | loss: 0.92681 - acc: 0.6391 -- iter: 76/76\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.92404\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 168 | loss: 0.92404 - acc: 0.6409 -- iter: 76/76\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.92137\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 169 | loss: 0.92137 - acc: 0.6426 -- iter: 76/76\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.91879\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 170 | loss: 0.91879 - acc: 0.6442 -- iter: 76/76\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.91630\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 171 | loss: 0.91630 - acc: 0.6455 -- iter: 76/76\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.94577\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 172 | loss: 0.94577 - acc: 0.6060 -- iter: 76/76\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.94027\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 173 | loss: 0.94027 - acc: 0.6112 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.93517\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 174 | loss: 0.93517 - acc: 0.6158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.93044\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 175 | loss: 0.93044 - acc: 0.6201 -- iter: 76/76\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.92603\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 176 | loss: 0.92603 - acc: 0.6238 -- iter: 76/76\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.92191\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 177 | loss: 0.92191 - acc: 0.6272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.91804\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 178 | loss: 0.91804 - acc: 0.6303 -- iter: 76/76\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.91441\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 179 | loss: 0.91441 - acc: 0.6331 -- iter: 76/76\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.91099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 180 | loss: 0.91099 - acc: 0.6355 -- iter: 76/76\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.90775\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 181 | loss: 0.90775 - acc: 0.6378 -- iter: 76/76\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.93464\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 182 | loss: 0.93464 - acc: 0.6095 -- iter: 76/76\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.92874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 183 | loss: 0.92874 - acc: 0.6144 -- iter: 76/76\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.92332\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 184 | loss: 0.92332 - acc: 0.6187 -- iter: 76/76\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.91830\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 185 | loss: 0.91830 - acc: 0.6226 -- iter: 76/76\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.91365\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 186 | loss: 0.91365 - acc: 0.6262 -- iter: 76/76\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.90932\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 187 | loss: 0.90932 - acc: 0.6293 -- iter: 76/76\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.93783\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 188 | loss: 0.93783 - acc: 0.5967 -- iter: 76/76\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.93083\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 189 | loss: 0.93083 - acc: 0.6028 -- iter: 76/76\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.92443\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 190 | loss: 0.92443 - acc: 0.6083 -- iter: 76/76\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.91855\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 191 | loss: 0.91855 - acc: 0.6133 -- iter: 76/76\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.91313\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 192 | loss: 0.91313 - acc: 0.6177 -- iter: 76/76\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.90810\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 193 | loss: 0.90810 - acc: 0.6217 -- iter: 76/76\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.90344\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 194 | loss: 0.90344 - acc: 0.6254 -- iter: 76/76\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.89909\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 195 | loss: 0.89909 - acc: 0.6286 -- iter: 76/76\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.89503\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 196 | loss: 0.89503 - acc: 0.6315 -- iter: 76/76\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.89122\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 197 | loss: 0.89122 - acc: 0.6342 -- iter: 76/76\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.92064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 198 | loss: 0.92064 - acc: 0.5984 -- iter: 76/76\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.91401\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 199 | loss: 0.91401 - acc: 0.6043 -- iter: 76/76\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.90792\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 200 | loss: 0.90792 - acc: 0.6097 -- iter: 76/76\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.90232\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 201 | loss: 0.90232 - acc: 0.6145 -- iter: 76/76\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.89715\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 202 | loss: 0.89715 - acc: 0.6189 -- iter: 76/76\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.89237\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 203 | loss: 0.89237 - acc: 0.6228 -- iter: 76/76\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.88794\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 204 | loss: 0.88794 - acc: 0.6263 -- iter: 76/76\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.88381\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 205 | loss: 0.88381 - acc: 0.6294 -- iter: 76/76\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.87996\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 206 | loss: 0.87996 - acc: 0.6323 -- iter: 76/76\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.87636\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 207 | loss: 0.87636 - acc: 0.6348 -- iter: 76/76\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.90008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 208 | loss: 0.90008 - acc: 0.6082 -- iter: 76/76\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.89421\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 209 | loss: 0.89421 - acc: 0.6132 -- iter: 76/76\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.88881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 210 | loss: 0.88881 - acc: 0.6176 -- iter: 76/76\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.88382\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 211 | loss: 0.88382 - acc: 0.6217 -- iter: 76/76\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.91295\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 212 | loss: 0.91295 - acc: 0.5950 -- iter: 76/76\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.90532\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 213 | loss: 0.90532 - acc: 0.6013 -- iter: 76/76\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.89836\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 214 | loss: 0.89836 - acc: 0.6070 -- iter: 76/76\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.89199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 215 | loss: 0.89199 - acc: 0.6121 -- iter: 76/76\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.88615\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 216 | loss: 0.88615 - acc: 0.6166 -- iter: 76/76\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.88078\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 217 | loss: 0.88078 - acc: 0.6208 -- iter: 76/76\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.87582\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 218 | loss: 0.87582 - acc: 0.6245 -- iter: 76/76\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.87124\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 219 | loss: 0.87124 - acc: 0.6278 -- iter: 76/76\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.86699\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 220 | loss: 0.86699 - acc: 0.6308 -- iter: 76/76\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.86303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 221 | loss: 0.86303 - acc: 0.6335 -- iter: 76/76\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.85933\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 222 | loss: 0.85933 - acc: 0.6360 -- iter: 76/76\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.85587\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 223 | loss: 0.85587 - acc: 0.6382 -- iter: 76/76\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.89082\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 224 | loss: 0.89082 - acc: 0.6059 -- iter: 76/76\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.88398\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 225 | loss: 0.88398 - acc: 0.6111 -- iter: 76/76\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.87772\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 226 | loss: 0.87772 - acc: 0.6158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.87197\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 227 | loss: 0.87197 - acc: 0.6200 -- iter: 76/76\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.86669\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 228 | loss: 0.86669 - acc: 0.6238 -- iter: 76/76\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.86182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 229 | loss: 0.86182 - acc: 0.6272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.85732\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 230 | loss: 0.85732 - acc: 0.6303 -- iter: 76/76\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.85315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 231 | loss: 0.85315 - acc: 0.6330 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.84928\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 232 | loss: 0.84928 - acc: 0.6355 -- iter: 76/76\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.84567\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 233 | loss: 0.84567 - acc: 0.6378 -- iter: 76/76\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.84230\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 234 | loss: 0.84230 - acc: 0.6398 -- iter: 76/76\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.83915\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 235 | loss: 0.83915 - acc: 0.6416 -- iter: 76/76\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.83618\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 236 | loss: 0.83618 - acc: 0.6432 -- iter: 76/76\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.83338\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 237 | loss: 0.83338 - acc: 0.6447 -- iter: 76/76\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.83074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 238 | loss: 0.83074 - acc: 0.6460 -- iter: 76/76\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.82823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 239 | loss: 0.82823 - acc: 0.6472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.82585\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 240 | loss: 0.82585 - acc: 0.6483 -- iter: 76/76\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.82358\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 241 | loss: 0.82358 - acc: 0.6492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.82141\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 242 | loss: 0.82141 - acc: 0.6501 -- iter: 76/76\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.81933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 243 | loss: 0.81933 - acc: 0.6509 -- iter: 76/76\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.81733\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 244 | loss: 0.81733 - acc: 0.6516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.81541\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 245 | loss: 0.81541 - acc: 0.6522 -- iter: 76/76\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.81355\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 246 | loss: 0.81355 - acc: 0.6528 -- iter: 76/76\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.81175\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 247 | loss: 0.81175 - acc: 0.6533 -- iter: 76/76\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.81000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 248 | loss: 0.81000 - acc: 0.6537 -- iter: 76/76\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.80831\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 249 | loss: 0.80831 - acc: 0.6542 -- iter: 76/76\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.85955\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 250 | loss: 0.85955 - acc: 0.6137 -- iter: 76/76\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.85269\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 251 | loss: 0.85269 - acc: 0.6182 -- iter: 76/76\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.84643\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 252 | loss: 0.84643 - acc: 0.6221 -- iter: 76/76\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.84071\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 253 | loss: 0.84071 - acc: 0.6257 -- iter: 76/76\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.87206\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 254 | loss: 0.87206 - acc: 0.6026 -- iter: 76/76\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.86362\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 255 | loss: 0.86362 - acc: 0.6081 -- iter: 76/76\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.85595\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 256 | loss: 0.85595 - acc: 0.6131 -- iter: 76/76\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.84897\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 257 | loss: 0.84897 - acc: 0.6176 -- iter: 76/76\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.84261\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 258 | loss: 0.84261 - acc: 0.6216 -- iter: 76/76\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.83680\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 259 | loss: 0.83680 - acc: 0.6253 -- iter: 76/76\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.83148\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 260 | loss: 0.83148 - acc: 0.6285 -- iter: 76/76\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.82659\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 261 | loss: 0.82659 - acc: 0.6315 -- iter: 76/76\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.82209\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 262 | loss: 0.82209 - acc: 0.6341 -- iter: 76/76\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.81793\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 263 | loss: 0.81793 - acc: 0.6365 -- iter: 76/76\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.81407\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 264 | loss: 0.81407 - acc: 0.6386 -- iter: 76/76\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.81050\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 265 | loss: 0.81050 - acc: 0.6405 -- iter: 76/76\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.86054\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 266 | loss: 0.86054 - acc: 0.6028 -- iter: 76/76\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.85214\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 267 | loss: 0.85214 - acc: 0.6083 -- iter: 76/76\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.84450\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 268 | loss: 0.84450 - acc: 0.6133 -- iter: 76/76\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.83756\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 269 | loss: 0.83756 - acc: 0.6177 -- iter: 76/76\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.83122\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 270 | loss: 0.83122 - acc: 0.6218 -- iter: 76/76\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.82544\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 271 | loss: 0.82544 - acc: 0.6254 -- iter: 76/76\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.82014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 272 | loss: 0.82014 - acc: 0.6286 -- iter: 76/76\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.81528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 273 | loss: 0.81528 - acc: 0.6315 -- iter: 76/76\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.81081\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 274 | loss: 0.81081 - acc: 0.6342 -- iter: 76/76\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.80668\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 275 | loss: 0.80668 - acc: 0.6366 -- iter: 76/76\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.80288\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 276 | loss: 0.80288 - acc: 0.6387 -- iter: 76/76\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.79935\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 277 | loss: 0.79935 - acc: 0.6406 -- iter: 76/76\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.79607\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 278 | loss: 0.79607 - acc: 0.6423 -- iter: 76/76\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.79303\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 279 | loss: 0.79303 - acc: 0.6439 -- iter: 76/76\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.79018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 280 | loss: 0.79018 - acc: 0.6453 -- iter: 76/76\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.78752\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 281 | loss: 0.78752 - acc: 0.6466 -- iter: 76/76\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.78502\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 282 | loss: 0.78502 - acc: 0.6477 -- iter: 76/76\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.78267\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 283 | loss: 0.78267 - acc: 0.6487 -- iter: 76/76\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.78046\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 284 | loss: 0.78046 - acc: 0.6496 -- iter: 76/76\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.77836\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 285 | loss: 0.77836 - acc: 0.6505 -- iter: 76/76\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.77638\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 286 | loss: 0.77638 - acc: 0.6512 -- iter: 76/76\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.77449\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 287 | loss: 0.77449 - acc: 0.6519 -- iter: 76/76\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.77269\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 288 | loss: 0.77269 - acc: 0.6525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.77098\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 289 | loss: 0.77098 - acc: 0.6530 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.76933\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 290 | loss: 0.76933 - acc: 0.6535 -- iter: 76/76\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.76776\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 291 | loss: 0.76776 - acc: 0.6539 -- iter: 76/76\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.76624\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 292 | loss: 0.76624 - acc: 0.6543 -- iter: 76/76\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.76477\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 293 | loss: 0.76477 - acc: 0.6547 -- iter: 76/76\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.76336\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 294 | loss: 0.76336 - acc: 0.6550 -- iter: 76/76\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.76199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 295 | loss: 0.76199 - acc: 0.6553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.76066\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 296 | loss: 0.76066 - acc: 0.6556 -- iter: 76/76\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.75936\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 297 | loss: 0.75936 - acc: 0.6558 -- iter: 76/76\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.75810\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 298 | loss: 0.75810 - acc: 0.6560 -- iter: 76/76\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.75687\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 299 | loss: 0.75687 - acc: 0.6562 -- iter: 76/76\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.75567\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 300 | loss: 0.75567 - acc: 0.6564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.75449\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 301 | loss: 0.75449 - acc: 0.6565 -- iter: 76/76\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.75334\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 302 | loss: 0.75334 - acc: 0.6567 -- iter: 76/76\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.75221\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 303 | loss: 0.75221 - acc: 0.6568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.75109\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 304 | loss: 0.75109 - acc: 0.6569 -- iter: 76/76\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.75000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 305 | loss: 0.75000 - acc: 0.6570 -- iter: 76/76\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.74892\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 306 | loss: 0.74892 - acc: 0.6571 -- iter: 76/76\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.74785\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 307 | loss: 0.74785 - acc: 0.6572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.74680\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 308 | loss: 0.74680 - acc: 0.6572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.74577\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 309 | loss: 0.74577 - acc: 0.6573 -- iter: 76/76\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.74474\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 310 | loss: 0.74474 - acc: 0.6574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.74372\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 311 | loss: 0.74372 - acc: 0.6574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.74272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 312 | loss: 0.74272 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.74172\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 313 | loss: 0.74172 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.74073\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 314 | loss: 0.74073 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.73975\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 315 | loss: 0.73975 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.73878\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 316 | loss: 0.73878 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.73782\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 317 | loss: 0.73782 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.73686\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 318 | loss: 0.73686 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.73591\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 319 | loss: 0.73591 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.73497\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 320 | loss: 0.73497 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.73404\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 321 | loss: 0.73404 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.73312\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 322 | loss: 0.73312 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.73221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 323 | loss: 0.73221 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.73130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 324 | loss: 0.73130 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.73039\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 325 | loss: 0.73039 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.72950\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 326 | loss: 0.72950 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.72860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 327 | loss: 0.72860 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.72772\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 328 | loss: 0.72772 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.72684\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 329 | loss: 0.72684 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.72596\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 330 | loss: 0.72596 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.72509\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 331 | loss: 0.72509 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.72422\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 332 | loss: 0.72422 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.72336\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 333 | loss: 0.72336 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.72251\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 334 | loss: 0.72251 - acc: 0.6579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.72166\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 335 | loss: 0.72166 - acc: 0.6579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.72082\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 336 | loss: 0.72082 - acc: 0.6579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.71998\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 337 | loss: 0.71998 - acc: 0.6579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.71915\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 338 | loss: 0.71915 - acc: 0.6579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.71832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 339 | loss: 0.71832 - acc: 0.6579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.77874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 340 | loss: 0.77874 - acc: 0.6263 -- iter: 76/76\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.77183\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 341 | loss: 0.77183 - acc: 0.6295 -- iter: 76/76\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.76555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 342 | loss: 0.76555 - acc: 0.6323 -- iter: 76/76\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.75986\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 343 | loss: 0.75986 - acc: 0.6349 -- iter: 76/76\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.75468\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 344 | loss: 0.75468 - acc: 0.6372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.74996\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 345 | loss: 0.74996 - acc: 0.6392 -- iter: 76/76\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.74566\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 346 | loss: 0.74566 - acc: 0.6411 -- iter: 76/76\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.74173\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 347 | loss: 0.74173 - acc: 0.6428 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.73813\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 348 | loss: 0.73813 - acc: 0.6443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.73483\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 349 | loss: 0.73483 - acc: 0.6457 -- iter: 76/76\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.73178\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 350 | loss: 0.73178 - acc: 0.6469 -- iter: 76/76\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.72897\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 351 | loss: 0.72897 - acc: 0.6480 -- iter: 76/76\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.72637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 352 | loss: 0.72637 - acc: 0.6490 -- iter: 76/76\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.72396\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 353 | loss: 0.72396 - acc: 0.6499 -- iter: 76/76\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.72171\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 354 | loss: 0.72171 - acc: 0.6507 -- iter: 76/76\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.71962\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 355 | loss: 0.71962 - acc: 0.6514 -- iter: 76/76\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.71766\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 356 | loss: 0.71766 - acc: 0.6520 -- iter: 76/76\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.71583\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 357 | loss: 0.71583 - acc: 0.6526 -- iter: 76/76\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.71411\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 358 | loss: 0.71411 - acc: 0.6532 -- iter: 76/76\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.71250\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 359 | loss: 0.71250 - acc: 0.6536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.71097\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 360 | loss: 0.71097 - acc: 0.6541 -- iter: 76/76\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.70954\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 361 | loss: 0.70954 - acc: 0.6544 -- iter: 76/76\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.70817\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 362 | loss: 0.70817 - acc: 0.6548 -- iter: 76/76\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.70688\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 363 | loss: 0.70688 - acc: 0.6551 -- iter: 76/76\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.77427\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 364 | loss: 0.77427 - acc: 0.6238 -- iter: 76/76\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.76625\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 365 | loss: 0.76625 - acc: 0.6272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.75899\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 366 | loss: 0.75899 - acc: 0.6303 -- iter: 76/76\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.75241\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 367 | loss: 0.75241 - acc: 0.6330 -- iter: 76/76\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.74645\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 368 | loss: 0.74645 - acc: 0.6355 -- iter: 76/76\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.74102\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 369 | loss: 0.74102 - acc: 0.6378 -- iter: 76/76\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.73610\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 370 | loss: 0.73610 - acc: 0.6398 -- iter: 76/76\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.73161\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 371 | loss: 0.73161 - acc: 0.6416 -- iter: 76/76\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.78644\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 372 | loss: 0.78644 - acc: 0.6143 -- iter: 76/76\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.77684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 373 | loss: 0.77684 - acc: 0.6186 -- iter: 76/76\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.76818\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 374 | loss: 0.76818 - acc: 0.6226 -- iter: 76/76\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.76036\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 375 | loss: 0.76036 - acc: 0.6261 -- iter: 76/76\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.75328\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 376 | loss: 0.75328 - acc: 0.6293 -- iter: 76/76\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.74688\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 377 | loss: 0.74688 - acc: 0.6321 -- iter: 76/76\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.74107\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 378 | loss: 0.74107 - acc: 0.6347 -- iter: 76/76\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.73579\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 379 | loss: 0.73579 - acc: 0.6370 -- iter: 76/76\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.73098\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 380 | loss: 0.73098 - acc: 0.6391 -- iter: 76/76\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.72660\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 381 | loss: 0.72660 - acc: 0.6410 -- iter: 76/76\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.79609\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 382 | loss: 0.79609 - acc: 0.6098 -- iter: 76/76\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.78512\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 383 | loss: 0.78512 - acc: 0.6146 -- iter: 76/76\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.77523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 384 | loss: 0.77523 - acc: 0.6189 -- iter: 76/76\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.76630\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 385 | loss: 0.76630 - acc: 0.6228 -- iter: 76/76\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.83289\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 386 | loss: 0.83289 - acc: 0.5882 -- iter: 76/76\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.81817\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 387 | loss: 0.81817 - acc: 0.5951 -- iter: 76/76\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.80493\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 388 | loss: 0.80493 - acc: 0.6014 -- iter: 76/76\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.79301\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 389 | loss: 0.79301 - acc: 0.6071 -- iter: 76/76\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.83366\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 390 | loss: 0.83366 - acc: 0.5832 -- iter: 76/76\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.81887\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 391 | loss: 0.81887 - acc: 0.5907 -- iter: 76/76\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.80557\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 392 | loss: 0.80557 - acc: 0.5974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.79359\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 393 | loss: 0.79359 - acc: 0.6034 -- iter: 76/76\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.78278\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 394 | loss: 0.78278 - acc: 0.6089 -- iter: 76/76\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.77302\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 395 | loss: 0.77302 - acc: 0.6138 -- iter: 76/76\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.76419\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 396 | loss: 0.76419 - acc: 0.6182 -- iter: 76/76\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.75619\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 397 | loss: 0.75619 - acc: 0.6222 -- iter: 76/76\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.74892\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 398 | loss: 0.74892 - acc: 0.6257 -- iter: 76/76\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.74229\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 399 | loss: 0.74229 - acc: 0.6290 -- iter: 76/76\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.73625\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 400 | loss: 0.73625 - acc: 0.6319 -- iter: 76/76\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.73074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 401 | loss: 0.73074 - acc: 0.6345 -- iter: 76/76\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.72569\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 402 | loss: 0.72569 - acc: 0.6368 -- iter: 76/76\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.72105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 403 | loss: 0.72105 - acc: 0.6389 -- iter: 76/76\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.71681\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 404 | loss: 0.71681 - acc: 0.6408 -- iter: 76/76\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.71292\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 405 | loss: 0.71292 - acc: 0.6425 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.70935\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 406 | loss: 0.70935 - acc: 0.6441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.70606\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 407 | loss: 0.70606 - acc: 0.6454 -- iter: 76/76\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.70304\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 408 | loss: 0.70304 - acc: 0.6467 -- iter: 76/76\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.70026\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 409 | loss: 0.70026 - acc: 0.6478 -- iter: 76/76\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.69771\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 410 | loss: 0.69771 - acc: 0.6488 -- iter: 76/76\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.69535\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 411 | loss: 0.69535 - acc: 0.6497 -- iter: 76/76\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.69317\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 412 | loss: 0.69317 - acc: 0.6505 -- iter: 76/76\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.69116\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 413 | loss: 0.69116 - acc: 0.6513 -- iter: 76/76\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.76677\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 414 | loss: 0.76677 - acc: 0.6204 -- iter: 76/76\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.75731\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 415 | loss: 0.75731 - acc: 0.6241 -- iter: 76/76\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.74877\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 416 | loss: 0.74877 - acc: 0.6275 -- iter: 76/76\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.74103\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 417 | loss: 0.74103 - acc: 0.6305 -- iter: 76/76\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.73403\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 418 | loss: 0.73403 - acc: 0.6333 -- iter: 76/76\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.72769\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 419 | loss: 0.72769 - acc: 0.6357 -- iter: 76/76\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.72194\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 420 | loss: 0.72194 - acc: 0.6379 -- iter: 76/76\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.71672\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 421 | loss: 0.71672 - acc: 0.6399 -- iter: 76/76\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.71198\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 422 | loss: 0.71198 - acc: 0.6417 -- iter: 76/76\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.70767\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 423 | loss: 0.70767 - acc: 0.6434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.70375\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 424 | loss: 0.70375 - acc: 0.6448 -- iter: 76/76\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.70017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 425 | loss: 0.70017 - acc: 0.6461 -- iter: 76/76\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.69690\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 426 | loss: 0.69690 - acc: 0.6473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.69390\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 427 | loss: 0.69390 - acc: 0.6484 -- iter: 76/76\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.75768\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 428 | loss: 0.75768 - acc: 0.6190 -- iter: 76/76\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.74853\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 429 | loss: 0.74853 - acc: 0.6229 -- iter: 76/76\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.74027\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 430 | loss: 0.74027 - acc: 0.6264 -- iter: 76/76\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.73280\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 431 | loss: 0.73280 - acc: 0.6296 -- iter: 76/76\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.72605\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 432 | loss: 0.72605 - acc: 0.6324 -- iter: 76/76\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.71994\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 433 | loss: 0.71994 - acc: 0.6350 -- iter: 76/76\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.71440\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 434 | loss: 0.71440 - acc: 0.6372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.70938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 435 | loss: 0.70938 - acc: 0.6393 -- iter: 76/76\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.70481\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 436 | loss: 0.70481 - acc: 0.6412 -- iter: 76/76\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.70066\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 437 | loss: 0.70066 - acc: 0.6428 -- iter: 76/76\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.69688\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 438 | loss: 0.69688 - acc: 0.6443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.69343\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 439 | loss: 0.69343 - acc: 0.6457 -- iter: 76/76\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.69027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 440 | loss: 0.69027 - acc: 0.6469 -- iter: 76/76\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.68738\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 441 | loss: 0.68738 - acc: 0.6480 -- iter: 76/76\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.75838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 442 | loss: 0.75838 - acc: 0.6174 -- iter: 76/76\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.74860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 443 | loss: 0.74860 - acc: 0.6215 -- iter: 76/76\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.73979\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 444 | loss: 0.73979 - acc: 0.6251 -- iter: 76/76\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.73182\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 445 | loss: 0.73182 - acc: 0.6284 -- iter: 76/76\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.79654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 446 | loss: 0.79654 - acc: 0.6024 -- iter: 76/76\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.78288\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 447 | loss: 0.78288 - acc: 0.6079 -- iter: 76/76\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.77059\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 448 | loss: 0.77059 - acc: 0.6129 -- iter: 76/76\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.75953\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 449 | loss: 0.75953 - acc: 0.6174 -- iter: 76/76\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.74956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 450 | loss: 0.74956 - acc: 0.6215 -- iter: 76/76\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.74057\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 451 | loss: 0.74057 - acc: 0.6251 -- iter: 76/76\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.73244\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 452 | loss: 0.73244 - acc: 0.6284 -- iter: 76/76\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.72510\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 453 | loss: 0.72510 - acc: 0.6314 -- iter: 76/76\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.71844\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 454 | loss: 0.71844 - acc: 0.6340 -- iter: 76/76\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.71241\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 455 | loss: 0.71241 - acc: 0.6364 -- iter: 76/76\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.77981\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 456 | loss: 0.77981 - acc: 0.6109 -- iter: 76/76\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.76757\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 457 | loss: 0.76757 - acc: 0.6156 -- iter: 76/76\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.75655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 458 | loss: 0.75655 - acc: 0.6198 -- iter: 76/76\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.74659\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 459 | loss: 0.74659 - acc: 0.6236 -- iter: 76/76\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.73760\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 460 | loss: 0.73760 - acc: 0.6271 -- iter: 76/76\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.72947\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 461 | loss: 0.72947 - acc: 0.6302 -- iter: 76/76\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.72211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 462 | loss: 0.72211 - acc: 0.6329 -- iter: 76/76\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.71544\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 463 | loss: 0.71544 - acc: 0.6354 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.70938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 464 | loss: 0.70938 - acc: 0.6377 -- iter: 76/76\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.70387\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 465 | loss: 0.70387 - acc: 0.6397 -- iter: 76/76\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.69886\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 466 | loss: 0.69886 - acc: 0.6415 -- iter: 76/76\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.69429\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 467 | loss: 0.69429 - acc: 0.6432 -- iter: 76/76\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.78298\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 468 | loss: 0.78298 - acc: 0.6065 -- iter: 76/76\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.76994\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 469 | loss: 0.76994 - acc: 0.6116 -- iter: 76/76\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.75818\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 470 | loss: 0.75818 - acc: 0.6162 -- iter: 76/76\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.74759\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 471 | loss: 0.74759 - acc: 0.6204 -- iter: 76/76\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.73802\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 472 | loss: 0.73802 - acc: 0.6242 -- iter: 76/76\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.72939\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 473 | loss: 0.72939 - acc: 0.6275 -- iter: 76/76\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.72159\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 474 | loss: 0.72159 - acc: 0.6306 -- iter: 76/76\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.71453\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 475 | loss: 0.71453 - acc: 0.6333 -- iter: 76/76\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.70813\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 476 | loss: 0.70813 - acc: 0.6358 -- iter: 76/76\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.70234\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 477 | loss: 0.70234 - acc: 0.6380 -- iter: 76/76\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.69708\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 478 | loss: 0.69708 - acc: 0.6400 -- iter: 76/76\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.69231\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 479 | loss: 0.69231 - acc: 0.6418 -- iter: 76/76\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.68796\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 480 | loss: 0.68796 - acc: 0.6434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.68401\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 481 | loss: 0.68401 - acc: 0.6448 -- iter: 76/76\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.68041\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 482 | loss: 0.68041 - acc: 0.6461 -- iter: 76/76\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.67711\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 483 | loss: 0.67711 - acc: 0.6473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.67410\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 484 | loss: 0.67410 - acc: 0.6484 -- iter: 76/76\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.67135\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 485 | loss: 0.67135 - acc: 0.6493 -- iter: 76/76\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.66882\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 486 | loss: 0.66882 - acc: 0.6502 -- iter: 76/76\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.66650\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 487 | loss: 0.66650 - acc: 0.6509 -- iter: 76/76\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.66436\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 488 | loss: 0.66436 - acc: 0.6516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.66240\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 489 | loss: 0.66240 - acc: 0.6523 -- iter: 76/76\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.66058\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 490 | loss: 0.66058 - acc: 0.6528 -- iter: 76/76\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.65889\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 491 | loss: 0.65889 - acc: 0.6533 -- iter: 76/76\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.73856\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 492 | loss: 0.73856 - acc: 0.6262 -- iter: 76/76\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.72901\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 493 | loss: 0.72901 - acc: 0.6293 -- iter: 76/76\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.72039\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 494 | loss: 0.72039 - acc: 0.6322 -- iter: 76/76\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.71260\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 495 | loss: 0.71260 - acc: 0.6348 -- iter: 76/76\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.70556\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 496 | loss: 0.70556 - acc: 0.6371 -- iter: 76/76\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.69919\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 497 | loss: 0.69919 - acc: 0.6392 -- iter: 76/76\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.69343\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 498 | loss: 0.69343 - acc: 0.6410 -- iter: 76/76\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.68821\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 499 | loss: 0.68821 - acc: 0.6427 -- iter: 76/76\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.68347\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 500 | loss: 0.68347 - acc: 0.6442 -- iter: 76/76\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.67918\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 501 | loss: 0.67918 - acc: 0.6456 -- iter: 76/76\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.67527\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 502 | loss: 0.67527 - acc: 0.6468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.67172\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 503 | loss: 0.67172 - acc: 0.6479 -- iter: 76/76\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.66848\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 504 | loss: 0.66848 - acc: 0.6489 -- iter: 76/76\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.66552\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 505 | loss: 0.66552 - acc: 0.6498 -- iter: 76/76\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.66282\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 506 | loss: 0.66282 - acc: 0.6506 -- iter: 76/76\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.66035\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 507 | loss: 0.66035 - acc: 0.6514 -- iter: 76/76\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.65809\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 508 | loss: 0.65809 - acc: 0.6520 -- iter: 76/76\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.65600\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 509 | loss: 0.65600 - acc: 0.6526 -- iter: 76/76\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.65409\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 510 | loss: 0.65409 - acc: 0.6531 -- iter: 76/76\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.65232\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 511 | loss: 0.65232 - acc: 0.6536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.65069\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 512 | loss: 0.65069 - acc: 0.6540 -- iter: 76/76\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.64918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 513 | loss: 0.64918 - acc: 0.6544 -- iter: 76/76\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.64777\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 514 | loss: 0.64777 - acc: 0.6548 -- iter: 76/76\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.64647\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 515 | loss: 0.64647 - acc: 0.6551 -- iter: 76/76\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.64525\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 516 | loss: 0.64525 - acc: 0.6554 -- iter: 76/76\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.64411\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 517 | loss: 0.64411 - acc: 0.6556 -- iter: 76/76\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.73121\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 518 | loss: 0.73121 - acc: 0.6190 -- iter: 76/76\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.72141\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 519 | loss: 0.72141 - acc: 0.6229 -- iter: 76/76\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.71257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 520 | loss: 0.71257 - acc: 0.6264 -- iter: 76/76\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.70459\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 521 | loss: 0.70459 - acc: 0.6295 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.69739\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 522 | loss: 0.69739 - acc: 0.6324 -- iter: 76/76\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.69087\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 523 | loss: 0.69087 - acc: 0.6349 -- iter: 76/76\n",
      "--\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.68499\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 524 | loss: 0.68499 - acc: 0.6372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.67966\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 525 | loss: 0.67966 - acc: 0.6393 -- iter: 76/76\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.67483\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 526 | loss: 0.67483 - acc: 0.6412 -- iter: 76/76\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.67046\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 527 | loss: 0.67046 - acc: 0.6428 -- iter: 76/76\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.66649\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 528 | loss: 0.66649 - acc: 0.6443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.66288\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 529 | loss: 0.66288 - acc: 0.6457 -- iter: 76/76\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.72334\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 530 | loss: 0.72334 - acc: 0.6180 -- iter: 76/76\n",
      "--\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.71400\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 531 | loss: 0.71400 - acc: 0.6220 -- iter: 76/76\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.70558\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 532 | loss: 0.70558 - acc: 0.6256 -- iter: 76/76\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.69798\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 533 | loss: 0.69798 - acc: 0.6288 -- iter: 76/76\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.69112\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 534 | loss: 0.69112 - acc: 0.6317 -- iter: 76/76\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.68492\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 535 | loss: 0.68492 - acc: 0.6343 -- iter: 76/76\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.67931\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 536 | loss: 0.67931 - acc: 0.6367 -- iter: 76/76\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.67424\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 537 | loss: 0.67424 - acc: 0.6401 -- iter: 76/76\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.66963\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 538 | loss: 0.66963 - acc: 0.6432 -- iter: 76/76\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.66544\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 539 | loss: 0.66544 - acc: 0.6447 -- iter: 76/76\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.66163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 540 | loss: 0.66163 - acc: 0.6460 -- iter: 76/76\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.65816\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 541 | loss: 0.65816 - acc: 0.6472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.65499\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 542 | loss: 0.65499 - acc: 0.6483 -- iter: 76/76\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.65209\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 543 | loss: 0.65209 - acc: 0.6492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.64945\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 544 | loss: 0.64945 - acc: 0.6501 -- iter: 76/76\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.64702\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 545 | loss: 0.64702 - acc: 0.6509 -- iter: 76/76\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.64479\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 546 | loss: 0.64479 - acc: 0.6516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.64275\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 547 | loss: 0.64275 - acc: 0.6522 -- iter: 76/76\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.64087\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 548 | loss: 0.64087 - acc: 0.6528 -- iter: 76/76\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.63913\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 549 | loss: 0.63913 - acc: 0.6533 -- iter: 76/76\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.63753\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 550 | loss: 0.63753 - acc: 0.6537 -- iter: 76/76\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.63605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 551 | loss: 0.63605 - acc: 0.6542 -- iter: 76/76\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.63468\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 552 | loss: 0.63468 - acc: 0.6545 -- iter: 76/76\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.63340\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 553 | loss: 0.63340 - acc: 0.6549 -- iter: 76/76\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.63222\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 554 | loss: 0.63222 - acc: 0.6552 -- iter: 76/76\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.63111\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 555 | loss: 0.63111 - acc: 0.6554 -- iter: 76/76\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.63008\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 556 | loss: 0.63008 - acc: 0.6557 -- iter: 76/76\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.62911\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 557 | loss: 0.62911 - acc: 0.6559 -- iter: 76/76\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.62820\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 558 | loss: 0.62820 - acc: 0.6561 -- iter: 76/76\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.62734\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 559 | loss: 0.62734 - acc: 0.6563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.62653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 560 | loss: 0.62653 - acc: 0.6564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.62577\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 561 | loss: 0.62577 - acc: 0.6566 -- iter: 76/76\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.62504\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 562 | loss: 0.62504 - acc: 0.6567 -- iter: 76/76\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.62435\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 563 | loss: 0.62435 - acc: 0.6568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.62368\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 564 | loss: 0.62368 - acc: 0.6569 -- iter: 76/76\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.62305\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 565 | loss: 0.62305 - acc: 0.6570 -- iter: 76/76\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.62245\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 566 | loss: 0.62245 - acc: 0.6571 -- iter: 76/76\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.62186\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 567 | loss: 0.62186 - acc: 0.6572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.62130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 568 | loss: 0.62130 - acc: 0.6573 -- iter: 76/76\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.62076\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 569 | loss: 0.62076 - acc: 0.6573 -- iter: 76/76\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.62023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 570 | loss: 0.62023 - acc: 0.6574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.61972\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 571 | loss: 0.61972 - acc: 0.6574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.61923\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 572 | loss: 0.61923 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.61874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 573 | loss: 0.61874 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.61827\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 574 | loss: 0.61827 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.61781\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 575 | loss: 0.61781 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.61736\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 576 | loss: 0.61736 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.61691\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 577 | loss: 0.61691 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.61648\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 578 | loss: 0.61648 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.61605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 579 | loss: 0.61605 - acc: 0.6577 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.61563\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 580 | loss: 0.61563 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.61522\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 581 | loss: 0.61522 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.61481\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 582 | loss: 0.61481 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.61440\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 583 | loss: 0.61440 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.70982\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 584 | loss: 0.70982 - acc: 0.6236 -- iter: 76/76\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.69986\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 585 | loss: 0.69986 - acc: 0.6283 -- iter: 76/76\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.69088\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 586 | loss: 0.69088 - acc: 0.6326 -- iter: 76/76\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.68277\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 587 | loss: 0.68277 - acc: 0.6364 -- iter: 76/76\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.67545\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 588 | loss: 0.67545 - acc: 0.6399 -- iter: 76/76\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.66884\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 589 | loss: 0.66884 - acc: 0.6430 -- iter: 76/76\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.66287\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 590 | loss: 0.66287 - acc: 0.6458 -- iter: 76/76\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.65747\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 591 | loss: 0.65747 - acc: 0.6483 -- iter: 76/76\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.74658\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 592 | loss: 0.74658 - acc: 0.6151 -- iter: 76/76\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.73280\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 593 | loss: 0.73280 - acc: 0.6207 -- iter: 76/76\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.72040\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 594 | loss: 0.72040 - acc: 0.6257 -- iter: 76/76\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.70925\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 595 | loss: 0.70925 - acc: 0.6303 -- iter: 76/76\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.69921\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 596 | loss: 0.69921 - acc: 0.6343 -- iter: 76/76\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.69017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 597 | loss: 0.69017 - acc: 0.6380 -- iter: 76/76\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.68202\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 598 | loss: 0.68202 - acc: 0.6413 -- iter: 76/76\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.67467\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 599 | loss: 0.67467 - acc: 0.6443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.66803\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 600 | loss: 0.66803 - acc: 0.6470 -- iter: 76/76\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.66203\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 601 | loss: 0.66203 - acc: 0.6494 -- iter: 76/76\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.65661\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 602 | loss: 0.65661 - acc: 0.6515 -- iter: 76/76\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.65169\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 603 | loss: 0.65169 - acc: 0.6535 -- iter: 76/76\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.64723\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 604 | loss: 0.64723 - acc: 0.6552 -- iter: 76/76\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.64317\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 605 | loss: 0.64317 - acc: 0.6568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.63948\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 606 | loss: 0.63948 - acc: 0.6582 -- iter: 76/76\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.63612\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 607 | loss: 0.63612 - acc: 0.6595 -- iter: 76/76\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.63305\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 608 | loss: 0.63305 - acc: 0.6607 -- iter: 76/76\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.63025\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 609 | loss: 0.63025 - acc: 0.6617 -- iter: 76/76\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.62768\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 610 | loss: 0.62768 - acc: 0.6627 -- iter: 76/76\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.62533\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 611 | loss: 0.62533 - acc: 0.6635 -- iter: 76/76\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.62318\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 612 | loss: 0.62318 - acc: 0.6642 -- iter: 76/76\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.62119\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 613 | loss: 0.62119 - acc: 0.6649 -- iter: 76/76\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.61937\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 614 | loss: 0.61937 - acc: 0.6655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.61770\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 615 | loss: 0.61770 - acc: 0.6661 -- iter: 76/76\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.61616\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 616 | loss: 0.61616 - acc: 0.6666 -- iter: 76/76\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.61474\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 617 | loss: 0.61474 - acc: 0.6670 -- iter: 76/76\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.61343\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 618 | loss: 0.61343 - acc: 0.6674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.61221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 619 | loss: 0.61221 - acc: 0.6678 -- iter: 76/76\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.71323\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 620 | loss: 0.71323 - acc: 0.6300 -- iter: 76/76\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.70199\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 621 | loss: 0.70199 - acc: 0.6341 -- iter: 76/76\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.69186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 622 | loss: 0.69186 - acc: 0.6378 -- iter: 76/76\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.68272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 623 | loss: 0.68272 - acc: 0.6411 -- iter: 76/76\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.67447\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 624 | loss: 0.67447 - acc: 0.6441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.66704\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 625 | loss: 0.66704 - acc: 0.6468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.66032\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 626 | loss: 0.66032 - acc: 0.6492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.65426\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 627 | loss: 0.65426 - acc: 0.6514 -- iter: 76/76\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.64878\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 628 | loss: 0.64878 - acc: 0.6534 -- iter: 76/76\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.64383\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 629 | loss: 0.64383 - acc: 0.6551 -- iter: 76/76\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.63934\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 630 | loss: 0.63934 - acc: 0.6567 -- iter: 76/76\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.63528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 631 | loss: 0.63528 - acc: 0.6582 -- iter: 76/76\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.63160\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 632 | loss: 0.63160 - acc: 0.6594 -- iter: 76/76\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.62825\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 633 | loss: 0.62825 - acc: 0.6606 -- iter: 76/76\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.62521\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 634 | loss: 0.62521 - acc: 0.6617 -- iter: 76/76\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.62244\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 635 | loss: 0.62244 - acc: 0.6626 -- iter: 76/76\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.61992\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 636 | loss: 0.61992 - acc: 0.6634 -- iter: 76/76\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.61762\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 637 | loss: 0.61762 - acc: 0.6642 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.61552\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 638 | loss: 0.61552 - acc: 0.6649 -- iter: 76/76\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.61360\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 639 | loss: 0.61360 - acc: 0.6655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.61183\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 640 | loss: 0.61183 - acc: 0.6661 -- iter: 76/76\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.61022\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 641 | loss: 0.61022 - acc: 0.6666 -- iter: 76/76\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.60873\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 642 | loss: 0.60873 - acc: 0.6670 -- iter: 76/76\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.60736\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 643 | loss: 0.60736 - acc: 0.6674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.70249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 644 | loss: 0.70249 - acc: 0.6349 -- iter: 76/76\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.69170\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 645 | loss: 0.69170 - acc: 0.6385 -- iter: 76/76\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.76667\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 646 | loss: 0.76667 - acc: 0.6128 -- iter: 76/76\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.74946\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 647 | loss: 0.74946 - acc: 0.6186 -- iter: 76/76\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.73400\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 648 | loss: 0.73400 - acc: 0.6239 -- iter: 76/76\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.72009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 649 | loss: 0.72009 - acc: 0.6286 -- iter: 76/76\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.70758\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 650 | loss: 0.70758 - acc: 0.6328 -- iter: 76/76\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.69633\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 651 | loss: 0.69633 - acc: 0.6367 -- iter: 76/76\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.68620\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 652 | loss: 0.68620 - acc: 0.6401 -- iter: 76/76\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.67708\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 653 | loss: 0.67708 - acc: 0.6432 -- iter: 76/76\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.66885\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 654 | loss: 0.66885 - acc: 0.6460 -- iter: 76/76\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.66144\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 655 | loss: 0.66144 - acc: 0.6485 -- iter: 76/76\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.75779\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 656 | loss: 0.75779 - acc: 0.6165 -- iter: 76/76\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.74148\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 657 | loss: 0.74148 - acc: 0.6220 -- iter: 76/76\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.72681\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 658 | loss: 0.72681 - acc: 0.6269 -- iter: 76/76\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.71362\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 659 | loss: 0.71362 - acc: 0.6313 -- iter: 76/76\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.70175\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 660 | loss: 0.70175 - acc: 0.6353 -- iter: 76/76\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.69107\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 661 | loss: 0.69107 - acc: 0.6389 -- iter: 76/76\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.68144\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 662 | loss: 0.68144 - acc: 0.6421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.67276\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 663 | loss: 0.67276 - acc: 0.6463 -- iter: 76/76\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.66492\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 664 | loss: 0.66492 - acc: 0.6501 -- iter: 76/76\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.65785\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 665 | loss: 0.65785 - acc: 0.6522 -- iter: 76/76\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.65146\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 666 | loss: 0.65146 - acc: 0.6541 -- iter: 76/76\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.64567\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 667 | loss: 0.64567 - acc: 0.6558 -- iter: 76/76\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.64043\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 668 | loss: 0.64043 - acc: 0.6573 -- iter: 76/76\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.63568\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 669 | loss: 0.63568 - acc: 0.6587 -- iter: 76/76\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.63137\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 670 | loss: 0.63137 - acc: 0.6599 -- iter: 76/76\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.62745\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 671 | loss: 0.62745 - acc: 0.6610 -- iter: 76/76\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.62387\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 672 | loss: 0.62387 - acc: 0.6620 -- iter: 76/76\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.62062\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 673 | loss: 0.62062 - acc: 0.6629 -- iter: 76/76\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.69793\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 674 | loss: 0.69793 - acc: 0.6374 -- iter: 76/76\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.68721\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 675 | loss: 0.68721 - acc: 0.6408 -- iter: 76/76\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.67756\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 676 | loss: 0.67756 - acc: 0.6438 -- iter: 76/76\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.66886\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 677 | loss: 0.66886 - acc: 0.6465 -- iter: 76/76\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.66100\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 678 | loss: 0.66100 - acc: 0.6490 -- iter: 76/76\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.65391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 679 | loss: 0.65391 - acc: 0.6512 -- iter: 76/76\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.64750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 680 | loss: 0.64750 - acc: 0.6532 -- iter: 76/76\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.64170\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 681 | loss: 0.64170 - acc: 0.6550 -- iter: 76/76\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.63646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 682 | loss: 0.63646 - acc: 0.6566 -- iter: 76/76\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.63170\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 683 | loss: 0.63170 - acc: 0.6580 -- iter: 76/76\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.62738\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 684 | loss: 0.62738 - acc: 0.6593 -- iter: 76/76\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.62346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 685 | loss: 0.62346 - acc: 0.6605 -- iter: 76/76\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.72268\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 686 | loss: 0.72268 - acc: 0.6260 -- iter: 76/76\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.70920\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 687 | loss: 0.70920 - acc: 0.6305 -- iter: 76/76\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.69705\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 688 | loss: 0.69705 - acc: 0.6346 -- iter: 76/76\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.68612\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 689 | loss: 0.68612 - acc: 0.6382 -- iter: 76/76\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.67627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 690 | loss: 0.67627 - acc: 0.6428 -- iter: 76/76\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.66738\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 691 | loss: 0.66738 - acc: 0.6470 -- iter: 76/76\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.65937\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 692 | loss: 0.65937 - acc: 0.6507 -- iter: 76/76\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.65213\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 693 | loss: 0.65213 - acc: 0.6540 -- iter: 76/76\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.64560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 694 | loss: 0.64560 - acc: 0.6571 -- iter: 76/76\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.63969\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 695 | loss: 0.63969 - acc: 0.6598 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.63434\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 696 | loss: 0.63434 - acc: 0.6622 -- iter: 76/76\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.62949\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 697 | loss: 0.62949 - acc: 0.6644 -- iter: 76/76\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.62510\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 698 | loss: 0.62510 - acc: 0.6664 -- iter: 76/76\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.62111\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 699 | loss: 0.62111 - acc: 0.6682 -- iter: 76/76\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.61748\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 700 | loss: 0.61748 - acc: 0.6698 -- iter: 76/76\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.61418\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 701 | loss: 0.61418 - acc: 0.6712 -- iter: 76/76\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.61117\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 702 | loss: 0.61117 - acc: 0.6725 -- iter: 76/76\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.60843\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 703 | loss: 0.60843 - acc: 0.6737 -- iter: 76/76\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.68356\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 704 | loss: 0.68356 - acc: 0.6471 -- iter: 76/76\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.67353\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 705 | loss: 0.67353 - acc: 0.6508 -- iter: 76/76\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.66449\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 706 | loss: 0.66449 - acc: 0.6542 -- iter: 76/76\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.65634\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 707 | loss: 0.65634 - acc: 0.6572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.64898\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 708 | loss: 0.64898 - acc: 0.6599 -- iter: 76/76\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.64234\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 709 | loss: 0.64234 - acc: 0.6623 -- iter: 76/76\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.74741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 710 | loss: 0.74741 - acc: 0.6224 -- iter: 76/76\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.73092\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 711 | loss: 0.73092 - acc: 0.6286 -- iter: 76/76\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.71609\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 712 | loss: 0.71609 - acc: 0.6341 -- iter: 76/76\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.70274\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 713 | loss: 0.70274 - acc: 0.6391 -- iter: 76/76\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.69073\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 714 | loss: 0.69073 - acc: 0.6450 -- iter: 76/76\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.67991\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 715 | loss: 0.67991 - acc: 0.6502 -- iter: 76/76\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.77585\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 716 | loss: 0.77585 - acc: 0.6194 -- iter: 76/76\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.75654\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 717 | loss: 0.75654 - acc: 0.6272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.87141\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 718 | loss: 0.87141 - acc: 0.5882 -- iter: 76/76\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.84263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 719 | loss: 0.84263 - acc: 0.5991 -- iter: 76/76\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.81679\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 720 | loss: 0.81679 - acc: 0.6089 -- iter: 76/76\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.79358\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 721 | loss: 0.79358 - acc: 0.6178 -- iter: 76/76\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.77272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 722 | loss: 0.77272 - acc: 0.6257 -- iter: 76/76\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.75399\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 723 | loss: 0.75399 - acc: 0.6329 -- iter: 76/76\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.73714\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 724 | loss: 0.73714 - acc: 0.6393 -- iter: 76/76\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.72199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 725 | loss: 0.72199 - acc: 0.6451 -- iter: 76/76\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.81683\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 726 | loss: 0.81683 - acc: 0.6109 -- iter: 76/76\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.79377\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 727 | loss: 0.79377 - acc: 0.6195 -- iter: 76/76\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.77305\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 728 | loss: 0.77305 - acc: 0.6273 -- iter: 76/76\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.75444\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 729 | loss: 0.75444 - acc: 0.6343 -- iter: 76/76\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.73771\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 730 | loss: 0.73771 - acc: 0.6406 -- iter: 76/76\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.72266\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 731 | loss: 0.72266 - acc: 0.6463 -- iter: 76/76\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.70912\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 732 | loss: 0.70912 - acc: 0.6514 -- iter: 76/76\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.69692\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 733 | loss: 0.69692 - acc: 0.6560 -- iter: 76/76\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.77737\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 734 | loss: 0.77737 - acc: 0.6259 -- iter: 76/76\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.75837\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 735 | loss: 0.75837 - acc: 0.6331 -- iter: 76/76\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.74129\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 736 | loss: 0.74129 - acc: 0.6395 -- iter: 76/76\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.72593\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 737 | loss: 0.72593 - acc: 0.6453 -- iter: 76/76\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.80070\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 738 | loss: 0.80070 - acc: 0.6202 -- iter: 76/76\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.77944\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 739 | loss: 0.77944 - acc: 0.6279 -- iter: 76/76\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.76035\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 740 | loss: 0.76035 - acc: 0.6349 -- iter: 76/76\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.74319\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 741 | loss: 0.74319 - acc: 0.6411 -- iter: 76/76\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.72775\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 742 | loss: 0.72775 - acc: 0.6468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.71386\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 743 | loss: 0.71386 - acc: 0.6518 -- iter: 76/76\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.70134\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 744 | loss: 0.70134 - acc: 0.6564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.69005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 745 | loss: 0.69005 - acc: 0.6605 -- iter: 76/76\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.67987\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 746 | loss: 0.67987 - acc: 0.6642 -- iter: 76/76\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.67067\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 747 | loss: 0.67067 - acc: 0.6675 -- iter: 76/76\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.66235\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 748 | loss: 0.66235 - acc: 0.6705 -- iter: 76/76\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.65482\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 749 | loss: 0.65482 - acc: 0.6732 -- iter: 76/76\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.64799\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 750 | loss: 0.64799 - acc: 0.6756 -- iter: 76/76\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.64178\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 751 | loss: 0.64178 - acc: 0.6778 -- iter: 76/76\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.63615\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 752 | loss: 0.63615 - acc: 0.6797 -- iter: 76/76\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.63102\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 753 | loss: 0.63102 - acc: 0.6815 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.62635\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 754 | loss: 0.62635 - acc: 0.6818 -- iter: 76/76\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.62209\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 755 | loss: 0.62209 - acc: 0.6820 -- iter: 76/76\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.61820\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 756 | loss: 0.61820 - acc: 0.6822 -- iter: 76/76\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.61463\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 757 | loss: 0.61463 - acc: 0.6824 -- iter: 76/76\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.61137\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 758 | loss: 0.61137 - acc: 0.6826 -- iter: 76/76\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.60837\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 759 | loss: 0.60837 - acc: 0.6828 -- iter: 76/76\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.60562\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 760 | loss: 0.60562 - acc: 0.6829 -- iter: 76/76\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.60308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 761 | loss: 0.60308 - acc: 0.6830 -- iter: 76/76\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.60074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 762 | loss: 0.60074 - acc: 0.6832 -- iter: 76/76\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.59857\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 763 | loss: 0.59857 - acc: 0.6833 -- iter: 76/76\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.59657\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 764 | loss: 0.59657 - acc: 0.6834 -- iter: 76/76\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.59471\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 765 | loss: 0.59471 - acc: 0.6834 -- iter: 76/76\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.59298\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 766 | loss: 0.59298 - acc: 0.6835 -- iter: 76/76\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.59137\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 767 | loss: 0.59137 - acc: 0.6836 -- iter: 76/76\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.58987\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 768 | loss: 0.58987 - acc: 0.6836 -- iter: 76/76\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.58847\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 769 | loss: 0.58847 - acc: 0.6837 -- iter: 76/76\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.58715\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 770 | loss: 0.58715 - acc: 0.6838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.58592\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 771 | loss: 0.58592 - acc: 0.6838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.58476\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 772 | loss: 0.58476 - acc: 0.6838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.58367\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 773 | loss: 0.58367 - acc: 0.6839 -- iter: 76/76\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.58263\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 774 | loss: 0.58263 - acc: 0.6839 -- iter: 76/76\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.58166\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 775 | loss: 0.58166 - acc: 0.6839 -- iter: 76/76\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.65701\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 776 | loss: 0.65701 - acc: 0.6590 -- iter: 76/76\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.64854\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 777 | loss: 0.64854 - acc: 0.6615 -- iter: 76/76\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.64089\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 778 | loss: 0.64089 - acc: 0.6638 -- iter: 76/76\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.63398\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 779 | loss: 0.63398 - acc: 0.6658 -- iter: 76/76\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.62775\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 780 | loss: 0.62775 - acc: 0.6676 -- iter: 76/76\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.62211\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 781 | loss: 0.62211 - acc: 0.6693 -- iter: 76/76\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.61700\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 782 | loss: 0.61700 - acc: 0.6708 -- iter: 76/76\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.61238\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 783 | loss: 0.61238 - acc: 0.6721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.60820\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 784 | loss: 0.60820 - acc: 0.6747 -- iter: 76/76\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.60441\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 785 | loss: 0.60441 - acc: 0.6769 -- iter: 76/76\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.69792\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 786 | loss: 0.69792 - acc: 0.6461 -- iter: 76/76\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.68513\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 787 | loss: 0.68513 - acc: 0.6512 -- iter: 76/76\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.67360\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 788 | loss: 0.67360 - acc: 0.6558 -- iter: 76/76\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.66322\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 789 | loss: 0.66322 - acc: 0.6600 -- iter: 76/76\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.65387\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 790 | loss: 0.65387 - acc: 0.6637 -- iter: 76/76\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.64544\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 791 | loss: 0.64544 - acc: 0.6671 -- iter: 76/76\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.74882\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 792 | loss: 0.74882 - acc: 0.6306 -- iter: 76/76\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.73090\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 793 | loss: 0.73090 - acc: 0.6373 -- iter: 76/76\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.71478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 794 | loss: 0.71478 - acc: 0.6433 -- iter: 76/76\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.70028\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 795 | loss: 0.70028 - acc: 0.6487 -- iter: 76/76\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.68723\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 796 | loss: 0.68723 - acc: 0.6536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.67550\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 797 | loss: 0.67550 - acc: 0.6580 -- iter: 76/76\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.66493\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 798 | loss: 0.66493 - acc: 0.6619 -- iter: 76/76\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.65540\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 799 | loss: 0.65540 - acc: 0.6655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.64682\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 800 | loss: 0.64682 - acc: 0.6700 -- iter: 76/76\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.63908\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 801 | loss: 0.63908 - acc: 0.6740 -- iter: 76/76\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.63210\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 802 | loss: 0.63210 - acc: 0.6777 -- iter: 76/76\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.62579\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 803 | loss: 0.62579 - acc: 0.6810 -- iter: 76/76\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.62009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 804 | loss: 0.62009 - acc: 0.6839 -- iter: 76/76\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.61494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 805 | loss: 0.61494 - acc: 0.6866 -- iter: 76/76\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.61027\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 806 | loss: 0.61027 - acc: 0.6890 -- iter: 76/76\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.60604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 807 | loss: 0.60604 - acc: 0.6911 -- iter: 76/76\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.60220\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 808 | loss: 0.60220 - acc: 0.6931 -- iter: 76/76\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.59872\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 809 | loss: 0.59872 - acc: 0.6935 -- iter: 76/76\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.59555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 810 | loss: 0.59555 - acc: 0.6939 -- iter: 76/76\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.59266\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 811 | loss: 0.59266 - acc: 0.6942 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.59003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 812 | loss: 0.59003 - acc: 0.6945 -- iter: 76/76\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.58763\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 813 | loss: 0.58763 - acc: 0.6948 -- iter: 76/76\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.68742\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 814 | loss: 0.68742 - acc: 0.6622 -- iter: 76/76\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.67524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 815 | loss: 0.67524 - acc: 0.6657 -- iter: 76/76\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.66428\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 816 | loss: 0.66428 - acc: 0.6689 -- iter: 76/76\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.65440\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 817 | loss: 0.65440 - acc: 0.6717 -- iter: 76/76\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.64550\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 818 | loss: 0.64550 - acc: 0.6756 -- iter: 76/76\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.63747\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 819 | loss: 0.63747 - acc: 0.6791 -- iter: 76/76\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.63023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 820 | loss: 0.63023 - acc: 0.6822 -- iter: 76/76\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.62369\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 821 | loss: 0.62369 - acc: 0.6851 -- iter: 76/76\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.61779\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 822 | loss: 0.61779 - acc: 0.6876 -- iter: 76/76\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.61245\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 823 | loss: 0.61245 - acc: 0.6899 -- iter: 76/76\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.60762\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 824 | loss: 0.60762 - acc: 0.6920 -- iter: 76/76\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.60325\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 825 | loss: 0.60325 - acc: 0.6938 -- iter: 76/76\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.59929\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 826 | loss: 0.59929 - acc: 0.6955 -- iter: 76/76\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.59570\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 827 | loss: 0.59570 - acc: 0.6970 -- iter: 76/76\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.59244\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 828 | loss: 0.59244 - acc: 0.6983 -- iter: 76/76\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.58948\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 829 | loss: 0.58948 - acc: 0.6996 -- iter: 76/76\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.58679\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 830 | loss: 0.58679 - acc: 0.7007 -- iter: 76/76\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.58434\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 831 | loss: 0.58434 - acc: 0.7016 -- iter: 76/76\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.58210\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 832 | loss: 0.58210 - acc: 0.7025 -- iter: 76/76\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.58006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 833 | loss: 0.58006 - acc: 0.7033 -- iter: 76/76\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.68819\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 834 | loss: 0.68819 - acc: 0.6659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.67551\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 835 | loss: 0.67551 - acc: 0.6704 -- iter: 76/76\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.66409\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 836 | loss: 0.66409 - acc: 0.6744 -- iter: 76/76\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.65381\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 837 | loss: 0.65381 - acc: 0.6780 -- iter: 76/76\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.64454\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 838 | loss: 0.64454 - acc: 0.6812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.63619\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 839 | loss: 0.63619 - acc: 0.6842 -- iter: 76/76\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.62866\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 840 | loss: 0.62866 - acc: 0.6868 -- iter: 76/76\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.62187\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 841 | loss: 0.62187 - acc: 0.6892 -- iter: 76/76\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.61574\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 842 | loss: 0.61574 - acc: 0.6913 -- iter: 76/76\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.61020\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 843 | loss: 0.61020 - acc: 0.6932 -- iter: 76/76\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.60520\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 844 | loss: 0.60520 - acc: 0.6950 -- iter: 76/76\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.60068\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 845 | loss: 0.60068 - acc: 0.6965 -- iter: 76/76\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.59658\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 846 | loss: 0.59658 - acc: 0.6979 -- iter: 76/76\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.59287\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 847 | loss: 0.59287 - acc: 0.6992 -- iter: 76/76\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.58951\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 848 | loss: 0.58951 - acc: 0.7003 -- iter: 76/76\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.58646\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 849 | loss: 0.58646 - acc: 0.7013 -- iter: 76/76\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.58369\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 850 | loss: 0.58369 - acc: 0.7023 -- iter: 76/76\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.58117\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 851 | loss: 0.58117 - acc: 0.7031 -- iter: 76/76\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.57888\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 852 | loss: 0.57888 - acc: 0.7038 -- iter: 76/76\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.57679\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 853 | loss: 0.57679 - acc: 0.7045 -- iter: 76/76\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.57489\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 854 | loss: 0.57489 - acc: 0.7051 -- iter: 76/76\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.57315\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 855 | loss: 0.57315 - acc: 0.7056 -- iter: 76/76\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.70429\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 856 | loss: 0.70429 - acc: 0.6653 -- iter: 76/76\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.68959\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 857 | loss: 0.68959 - acc: 0.6699 -- iter: 76/76\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.67635\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 858 | loss: 0.67635 - acc: 0.6739 -- iter: 76/76\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.66444\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 859 | loss: 0.66444 - acc: 0.6776 -- iter: 76/76\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.65371\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 860 | loss: 0.65371 - acc: 0.6809 -- iter: 76/76\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.64405\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 861 | loss: 0.64405 - acc: 0.6838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.63534\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 862 | loss: 0.63534 - acc: 0.6865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.62750\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 863 | loss: 0.62750 - acc: 0.6889 -- iter: 76/76\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.62043\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 864 | loss: 0.62043 - acc: 0.6911 -- iter: 76/76\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.61404\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 865 | loss: 0.61404 - acc: 0.6930 -- iter: 76/76\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.60828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 866 | loss: 0.60828 - acc: 0.6948 -- iter: 76/76\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.60308\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 867 | loss: 0.60308 - acc: 0.6963 -- iter: 76/76\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.72694\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 868 | loss: 0.72694 - acc: 0.6530 -- iter: 76/76\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.70986\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 869 | loss: 0.70986 - acc: 0.6588 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.79532\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 870 | loss: 0.79532 - acc: 0.6324 -- iter: 76/76\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.77146\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 871 | loss: 0.77146 - acc: 0.6402 -- iter: 76/76\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.75001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 872 | loss: 0.75001 - acc: 0.6472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.73073\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 873 | loss: 0.73073 - acc: 0.6536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.71340\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 874 | loss: 0.71340 - acc: 0.6593 -- iter: 76/76\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.69782\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 875 | loss: 0.69782 - acc: 0.6644 -- iter: 76/76\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.68380\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 876 | loss: 0.68380 - acc: 0.6690 -- iter: 76/76\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.67119\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 877 | loss: 0.67119 - acc: 0.6731 -- iter: 76/76\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.65984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 878 | loss: 0.65984 - acc: 0.6769 -- iter: 76/76\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.64962\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 879 | loss: 0.64962 - acc: 0.6802 -- iter: 76/76\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.64041\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 880 | loss: 0.64041 - acc: 0.6833 -- iter: 76/76\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.63211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 881 | loss: 0.63211 - acc: 0.6860 -- iter: 76/76\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.74287\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 882 | loss: 0.74287 - acc: 0.6516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.72433\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 883 | loss: 0.72433 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.70766\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 884 | loss: 0.70766 - acc: 0.6628 -- iter: 76/76\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.69266\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 885 | loss: 0.69266 - acc: 0.6676 -- iter: 76/76\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.67918\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 886 | loss: 0.67918 - acc: 0.6719 -- iter: 76/76\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.66704\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 887 | loss: 0.66704 - acc: 0.6771 -- iter: 76/76\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.65611\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 888 | loss: 0.65611 - acc: 0.6817 -- iter: 76/76\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.64626\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 889 | loss: 0.64626 - acc: 0.6859 -- iter: 76/76\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.63738\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 890 | loss: 0.63738 - acc: 0.6884 -- iter: 76/76\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.62938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 891 | loss: 0.62938 - acc: 0.6906 -- iter: 76/76\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.62215\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 892 | loss: 0.62215 - acc: 0.6926 -- iter: 76/76\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.61562\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 893 | loss: 0.61562 - acc: 0.6944 -- iter: 76/76\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.60972\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 894 | loss: 0.60972 - acc: 0.6960 -- iter: 76/76\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.60438\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 895 | loss: 0.60438 - acc: 0.6974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.59955\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 896 | loss: 0.59955 - acc: 0.6988 -- iter: 76/76\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.59516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 897 | loss: 0.59516 - acc: 0.6999 -- iter: 76/76\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.59118\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 898 | loss: 0.59118 - acc: 0.7010 -- iter: 76/76\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.58757\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 899 | loss: 0.58757 - acc: 0.7019 -- iter: 76/76\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.58428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 900 | loss: 0.58428 - acc: 0.7028 -- iter: 76/76\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.58129\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 901 | loss: 0.58129 - acc: 0.7036 -- iter: 76/76\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.57855\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 902 | loss: 0.57855 - acc: 0.7043 -- iter: 76/76\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.57606\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 903 | loss: 0.57606 - acc: 0.7049 -- iter: 76/76\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.57378\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 904 | loss: 0.57378 - acc: 0.7055 -- iter: 76/76\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.57168\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 905 | loss: 0.57168 - acc: 0.7060 -- iter: 76/76\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.56976\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 906 | loss: 0.56976 - acc: 0.7064 -- iter: 76/76\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.56800\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 907 | loss: 0.56800 - acc: 0.7068 -- iter: 76/76\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.56637\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 908 | loss: 0.56637 - acc: 0.7072 -- iter: 76/76\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.56487\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 909 | loss: 0.56487 - acc: 0.7075 -- iter: 76/76\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.56349\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 910 | loss: 0.56349 - acc: 0.7078 -- iter: 76/76\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.56220\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 911 | loss: 0.56220 - acc: 0.7081 -- iter: 76/76\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.56101\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 912 | loss: 0.56101 - acc: 0.7083 -- iter: 76/76\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.55990\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 913 | loss: 0.55990 - acc: 0.7086 -- iter: 76/76\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.55887\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 914 | loss: 0.55887 - acc: 0.7088 -- iter: 76/76\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.55791\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 915 | loss: 0.55791 - acc: 0.7089 -- iter: 76/76\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.55700\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 916 | loss: 0.55700 - acc: 0.7091 -- iter: 76/76\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.55616\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 917 | loss: 0.55616 - acc: 0.7092 -- iter: 76/76\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.55537\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 918 | loss: 0.55537 - acc: 0.7094 -- iter: 76/76\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.55463\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 919 | loss: 0.55463 - acc: 0.7095 -- iter: 76/76\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.55393\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 920 | loss: 0.55393 - acc: 0.7096 -- iter: 76/76\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.55327\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 921 | loss: 0.55327 - acc: 0.7097 -- iter: 76/76\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.55265\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 922 | loss: 0.55265 - acc: 0.7098 -- iter: 76/76\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.55207\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 923 | loss: 0.55207 - acc: 0.7098 -- iter: 76/76\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.55152\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 924 | loss: 0.55152 - acc: 0.7099 -- iter: 76/76\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.55100\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 925 | loss: 0.55100 - acc: 0.7100 -- iter: 76/76\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.55051\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 926 | loss: 0.55051 - acc: 0.7100 -- iter: 76/76\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.55005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 927 | loss: 0.55005 - acc: 0.7101 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.54961\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 928 | loss: 0.54961 - acc: 0.7101 -- iter: 76/76\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.54919\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 929 | loss: 0.54919 - acc: 0.7102 -- iter: 76/76\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.54879\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 930 | loss: 0.54879 - acc: 0.7102 -- iter: 76/76\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.54840\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 931 | loss: 0.54840 - acc: 0.7102 -- iter: 76/76\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.54804\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 932 | loss: 0.54804 - acc: 0.7103 -- iter: 76/76\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.54768\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 933 | loss: 0.54768 - acc: 0.7103 -- iter: 76/76\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.54734\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 934 | loss: 0.54734 - acc: 0.7103 -- iter: 76/76\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.54702\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 935 | loss: 0.54702 - acc: 0.7116 -- iter: 76/76\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.54670\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 936 | loss: 0.54670 - acc: 0.7129 -- iter: 76/76\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.54639\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 937 | loss: 0.54639 - acc: 0.7139 -- iter: 76/76\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.54609\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 938 | loss: 0.54609 - acc: 0.7149 -- iter: 76/76\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.54580\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 939 | loss: 0.54580 - acc: 0.7158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.54552\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 940 | loss: 0.54552 - acc: 0.7166 -- iter: 76/76\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.54524\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 941 | loss: 0.54524 - acc: 0.7173 -- iter: 76/76\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.54497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 942 | loss: 0.54497 - acc: 0.7192 -- iter: 76/76\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.54470\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 943 | loss: 0.54470 - acc: 0.7223 -- iter: 76/76\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.54444\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 944 | loss: 0.54444 - acc: 0.7251 -- iter: 76/76\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.54418\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 945 | loss: 0.54418 - acc: 0.7276 -- iter: 76/76\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.54392\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 946 | loss: 0.54392 - acc: 0.7298 -- iter: 76/76\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.54367\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 947 | loss: 0.54367 - acc: 0.7318 -- iter: 76/76\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.54343\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 948 | loss: 0.54343 - acc: 0.7337 -- iter: 76/76\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.54318\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 949 | loss: 0.54318 - acc: 0.7353 -- iter: 76/76\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.54294\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 950 | loss: 0.54294 - acc: 0.7368 -- iter: 76/76\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.54270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 951 | loss: 0.54270 - acc: 0.7381 -- iter: 76/76\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.64013\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 952 | loss: 0.64013 - acc: 0.6998 -- iter: 76/76\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.63013\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 953 | loss: 0.63013 - acc: 0.7048 -- iter: 76/76\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.62113\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 954 | loss: 0.62113 - acc: 0.7093 -- iter: 76/76\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.61301\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 955 | loss: 0.61301 - acc: 0.7134 -- iter: 76/76\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.60570\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 956 | loss: 0.60570 - acc: 0.7171 -- iter: 76/76\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.59911\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 957 | loss: 0.59911 - acc: 0.7204 -- iter: 76/76\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.59316\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 958 | loss: 0.59316 - acc: 0.7233 -- iter: 76/76\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.58780\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 959 | loss: 0.58780 - acc: 0.7260 -- iter: 76/76\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.58296\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 960 | loss: 0.58296 - acc: 0.7284 -- iter: 76/76\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.57860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 961 | loss: 0.57860 - acc: 0.7306 -- iter: 76/76\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.57465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 962 | loss: 0.57465 - acc: 0.7325 -- iter: 76/76\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.57108\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 963 | loss: 0.57108 - acc: 0.7342 -- iter: 76/76\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.56785\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 964 | loss: 0.56785 - acc: 0.7358 -- iter: 76/76\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.56493\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 965 | loss: 0.56493 - acc: 0.7372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.56227\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 966 | loss: 0.56227 - acc: 0.7385 -- iter: 76/76\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.55987\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 967 | loss: 0.55987 - acc: 0.7397 -- iter: 76/76\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.55768\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 968 | loss: 0.55768 - acc: 0.7407 -- iter: 76/76\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.55569\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 969 | loss: 0.55569 - acc: 0.7416 -- iter: 76/76\n",
      "--\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.55387\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 970 | loss: 0.55387 - acc: 0.7425 -- iter: 76/76\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.55222\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 971 | loss: 0.55222 - acc: 0.7432 -- iter: 76/76\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.55071\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 972 | loss: 0.55071 - acc: 0.7439 -- iter: 76/76\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.54933\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 973 | loss: 0.54933 - acc: 0.7445 -- iter: 76/76\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.54806\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 974 | loss: 0.54806 - acc: 0.7451 -- iter: 76/76\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.54690\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 975 | loss: 0.54690 - acc: 0.7456 -- iter: 76/76\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.54583\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 976 | loss: 0.54583 - acc: 0.7460 -- iter: 76/76\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.54485\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 977 | loss: 0.54485 - acc: 0.7464 -- iter: 76/76\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.54394\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 978 | loss: 0.54394 - acc: 0.7468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.54310\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 979 | loss: 0.54310 - acc: 0.7471 -- iter: 76/76\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.54233\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 980 | loss: 0.54233 - acc: 0.7474 -- iter: 76/76\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.54161\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 981 | loss: 0.54161 - acc: 0.7476 -- iter: 76/76\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.65862\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 982 | loss: 0.65862 - acc: 0.7018 -- iter: 76/76\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.64625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 983 | loss: 0.64625 - acc: 0.7066 -- iter: 76/76\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.63511\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 984 | loss: 0.63511 - acc: 0.7110 -- iter: 76/76\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.62508\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 985 | loss: 0.62508 - acc: 0.7149 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.61605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 986 | loss: 0.61605 - acc: 0.7184 -- iter: 76/76\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.60792\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 987 | loss: 0.60792 - acc: 0.7215 -- iter: 76/76\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.60059\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 988 | loss: 0.60059 - acc: 0.7244 -- iter: 76/76\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.59398\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 989 | loss: 0.59398 - acc: 0.7270 -- iter: 76/76\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.58803\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 990 | loss: 0.58803 - acc: 0.7293 -- iter: 76/76\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.58266\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 991 | loss: 0.58266 - acc: 0.7313 -- iter: 76/76\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.57781\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 992 | loss: 0.57781 - acc: 0.7332 -- iter: 76/76\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.57343\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 993 | loss: 0.57343 - acc: 0.7362 -- iter: 76/76\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.56947\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 994 | loss: 0.56947 - acc: 0.7389 -- iter: 76/76\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.56589\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 995 | loss: 0.56589 - acc: 0.7400 -- iter: 76/76\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.68410\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 996 | loss: 0.68410 - acc: 0.6976 -- iter: 76/76\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.66904\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 997 | loss: 0.66904 - acc: 0.7041 -- iter: 76/76\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.65550\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 998 | loss: 0.65550 - acc: 0.7100 -- iter: 76/76\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.64331\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 999 | loss: 0.64331 - acc: 0.7154 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.63235\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1000 | loss: 0.63235 - acc: 0.7201 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.62247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1001 | loss: 0.62247 - acc: 0.7244 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.61358\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1002 | loss: 0.61358 - acc: 0.7283 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.60556\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1003 | loss: 0.60556 - acc: 0.7318 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.59834\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1004 | loss: 0.59834 - acc: 0.7349 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.59182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1005 | loss: 0.59182 - acc: 0.7378 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.58594\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1006 | loss: 0.58594 - acc: 0.7403 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.58063\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1007 | loss: 0.58063 - acc: 0.7426 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.57584\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1008 | loss: 0.57584 - acc: 0.7446 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.57150\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1009 | loss: 0.57150 - acc: 0.7452 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.56758\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1010 | loss: 0.56758 - acc: 0.7457 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.56403\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1011 | loss: 0.56403 - acc: 0.7461 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.56081\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1012 | loss: 0.56081 - acc: 0.7465 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.55789\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1013 | loss: 0.55789 - acc: 0.7468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.55524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1014 | loss: 0.55524 - acc: 0.7472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.55283\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1015 | loss: 0.55283 - acc: 0.7474 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.55064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1016 | loss: 0.55064 - acc: 0.7477 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.54865\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1017 | loss: 0.54865 - acc: 0.7479 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.54683\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1018 | loss: 0.54683 - acc: 0.7481 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.54517\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1019 | loss: 0.54517 - acc: 0.7483 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.54365\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1020 | loss: 0.54365 - acc: 0.7485 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.54226\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1021 | loss: 0.54226 - acc: 0.7486 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.54099\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1022 | loss: 0.54099 - acc: 0.7488 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.53982\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1023 | loss: 0.53982 - acc: 0.7489 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.53874\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1024 | loss: 0.53874 - acc: 0.7490 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.53775\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1025 | loss: 0.53775 - acc: 0.7491 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.53684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1026 | loss: 0.53684 - acc: 0.7492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.53599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1027 | loss: 0.53599 - acc: 0.7493 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.53521\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1028 | loss: 0.53521 - acc: 0.7493 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.53448\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1029 | loss: 0.53448 - acc: 0.7494 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.53381\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1030 | loss: 0.53381 - acc: 0.7495 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.53318\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1031 | loss: 0.53318 - acc: 0.7495 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.66109\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1032 | loss: 0.66109 - acc: 0.7048 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.64771\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1033 | loss: 0.64771 - acc: 0.7094 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.63566\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1034 | loss: 0.63566 - acc: 0.7134 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.62481\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1035 | loss: 0.62481 - acc: 0.7171 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.61504\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1036 | loss: 0.61504 - acc: 0.7204 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.60625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1037 | loss: 0.60625 - acc: 0.7233 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.59832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1038 | loss: 0.59832 - acc: 0.7260 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.59117\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1039 | loss: 0.59117 - acc: 0.7297 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.70536\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1040 | loss: 0.70536 - acc: 0.6910 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.68751\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1041 | loss: 0.68751 - acc: 0.6982 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.67146\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1042 | loss: 0.67146 - acc: 0.7047 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.65703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1043 | loss: 0.65703 - acc: 0.7105 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.76721\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1044 | loss: 0.76721 - acc: 0.6710 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.74324\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1045 | loss: 0.74324 - acc: 0.6816 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.72170\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1046 | loss: 0.72170 - acc: 0.6910 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.70233\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1047 | loss: 0.70233 - acc: 0.6996 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.82315\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1048 | loss: 0.82315 - acc: 0.6572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.79371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1049 | loss: 0.79371 - acc: 0.6705 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.76725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1050 | loss: 0.76725 - acc: 0.6824 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.74348\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1051 | loss: 0.74348 - acc: 0.6944 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.72211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1052 | loss: 0.72211 - acc: 0.7052 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.70291\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1053 | loss: 0.70291 - acc: 0.7150 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.68564\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1054 | loss: 0.68564 - acc: 0.7237 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.67011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1055 | loss: 0.67011 - acc: 0.7316 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.75494\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1056 | loss: 0.75494 - acc: 0.7006 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.73252\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1057 | loss: 0.73252 - acc: 0.7108 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.82198\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1058 | loss: 0.82198 - acc: 0.6686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.79295\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1059 | loss: 0.79295 - acc: 0.6820 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.76687\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1060 | loss: 0.76687 - acc: 0.6941 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.74344\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1061 | loss: 0.74344 - acc: 0.7050 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.84551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1062 | loss: 0.84551 - acc: 0.6595 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.81434\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1063 | loss: 0.81434 - acc: 0.6751 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.78636\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1064 | loss: 0.78636 - acc: 0.6892 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.76124\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1065 | loss: 0.76124 - acc: 0.7018 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.73868\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1066 | loss: 0.73868 - acc: 0.7132 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.71840\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1067 | loss: 0.71840 - acc: 0.7235 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.70018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1068 | loss: 0.70018 - acc: 0.7327 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.68379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1069 | loss: 0.68379 - acc: 0.7397 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.66905\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1070 | loss: 0.66905 - acc: 0.7460 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.65577\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1071 | loss: 0.65577 - acc: 0.7517 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.64381\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1072 | loss: 0.64381 - acc: 0.7568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.63303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1073 | loss: 0.63303 - acc: 0.7600 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.62330\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1074 | loss: 0.62330 - acc: 0.7630 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.61451\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1075 | loss: 0.61451 - acc: 0.7656 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.71845\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1076 | loss: 0.71845 - acc: 0.7272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.70013\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1077 | loss: 0.70013 - acc: 0.7321 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.68365\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1078 | loss: 0.68365 - acc: 0.7365 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.66882\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1079 | loss: 0.66882 - acc: 0.7405 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.65546\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1080 | loss: 0.65546 - acc: 0.7441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.64343\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1081 | loss: 0.64343 - acc: 0.7473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.63258\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1082 | loss: 0.63258 - acc: 0.7489 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.62279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1083 | loss: 0.62279 - acc: 0.7503 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.61395\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1084 | loss: 0.61395 - acc: 0.7516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.60596\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1085 | loss: 0.60596 - acc: 0.7528 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.59873\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1086 | loss: 0.59873 - acc: 0.7538 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.59219\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1087 | loss: 0.59219 - acc: 0.7547 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.71133\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1088 | loss: 0.71133 - acc: 0.7122 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.69350\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1089 | loss: 0.69350 - acc: 0.7173 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.67746\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1090 | loss: 0.67746 - acc: 0.7219 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.66302\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1091 | loss: 0.66302 - acc: 0.7260 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.65001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1092 | loss: 0.65001 - acc: 0.7297 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.63829\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1093 | loss: 0.63829 - acc: 0.7330 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.62772\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1094 | loss: 0.62772 - acc: 0.7361 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.61818\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1095 | loss: 0.61818 - acc: 0.7388 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.60957\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1096 | loss: 0.60957 - acc: 0.7412 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.60179\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1097 | loss: 0.60179 - acc: 0.7434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.59475\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1098 | loss: 0.59475 - acc: 0.7454 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.58838\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1099 | loss: 0.58838 - acc: 0.7472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.58261\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1100 | loss: 0.58261 - acc: 0.7488 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.57737\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1101 | loss: 0.57737 - acc: 0.7502 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.57262\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1102 | loss: 0.57262 - acc: 0.7515 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.56829\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1103 | loss: 0.56829 - acc: 0.7527 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.56436\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1104 | loss: 0.56436 - acc: 0.7537 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.56078\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1105 | loss: 0.56078 - acc: 0.7547 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.55751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1106 | loss: 0.55751 - acc: 0.7555 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.55452\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1107 | loss: 0.55452 - acc: 0.7563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.68310\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1108 | loss: 0.68310 - acc: 0.7096 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.66751\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1109 | loss: 0.66751 - acc: 0.7149 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.65348\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1110 | loss: 0.65348 - acc: 0.7198 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.64084\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1111 | loss: 0.64084 - acc: 0.7241 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.62945\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1112 | loss: 0.62945 - acc: 0.7280 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.61919\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1113 | loss: 0.61919 - acc: 0.7315 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.60994\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1114 | loss: 0.60994 - acc: 0.7360 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.60160\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1115 | loss: 0.60160 - acc: 0.7400 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.59406\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1116 | loss: 0.59406 - acc: 0.7437 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.58725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1117 | loss: 0.58725 - acc: 0.7469 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.58110\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1118 | loss: 0.58110 - acc: 0.7499 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.57553\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1119 | loss: 0.57553 - acc: 0.7525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.68213\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1120 | loss: 0.68213 - acc: 0.7049 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.66644\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1121 | loss: 0.66644 - acc: 0.7120 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.74721\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1122 | loss: 0.74721 - acc: 0.6843 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.72505\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1123 | loss: 0.72505 - acc: 0.6948 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.70512\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1124 | loss: 0.70512 - acc: 0.7042 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.68721\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1125 | loss: 0.68721 - acc: 0.7128 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.67110\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1126 | loss: 0.67110 - acc: 0.7204 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.65660\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1127 | loss: 0.65660 - acc: 0.7287 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.75752\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1128 | loss: 0.75752 - acc: 0.6874 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.73442\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1129 | loss: 0.73442 - acc: 0.6989 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.71366\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1130 | loss: 0.71366 - acc: 0.7093 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.69501\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1131 | loss: 0.69501 - acc: 0.7186 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.67823\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1132 | loss: 0.67823 - acc: 0.7283 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.66314\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1133 | loss: 0.66314 - acc: 0.7371 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.64956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1134 | loss: 0.64956 - acc: 0.7449 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.63733\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1135 | loss: 0.63733 - acc: 0.7520 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.62631\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1136 | loss: 0.62631 - acc: 0.7571 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.61637\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1137 | loss: 0.61637 - acc: 0.7616 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.60741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1138 | loss: 0.60741 - acc: 0.7657 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.59931\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1139 | loss: 0.59931 - acc: 0.7694 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.59200\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1140 | loss: 0.59200 - acc: 0.7728 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.58539\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1141 | loss: 0.58539 - acc: 0.7757 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.57940\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1142 | loss: 0.57940 - acc: 0.7784 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.57397\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1143 | loss: 0.57397 - acc: 0.7795 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.56905\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1144 | loss: 0.56905 - acc: 0.7805 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.56459\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1145 | loss: 0.56459 - acc: 0.7814 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.56052\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1146 | loss: 0.56052 - acc: 0.7822 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.55683\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1147 | loss: 0.55683 - acc: 0.7816 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.55347\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1148 | loss: 0.55347 - acc: 0.7811 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.55040\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1149 | loss: 0.55040 - acc: 0.7806 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.68520\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1150 | loss: 0.68520 - acc: 0.7289 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.66892\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1151 | loss: 0.66892 - acc: 0.7336 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.65427\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1152 | loss: 0.65427 - acc: 0.7379 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.64109\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1153 | loss: 0.64109 - acc: 0.7417 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.62921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1154 | loss: 0.62921 - acc: 0.7452 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.61851\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1155 | loss: 0.61851 - acc: 0.7483 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.60886\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1156 | loss: 0.60886 - acc: 0.7524 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.60016\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1157 | loss: 0.60016 - acc: 0.7561 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.59231\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1158 | loss: 0.59231 - acc: 0.7595 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.58522\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1159 | loss: 0.58522 - acc: 0.7625 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.57881\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1160 | loss: 0.57881 - acc: 0.7652 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.57302\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1161 | loss: 0.57302 - acc: 0.7676 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.56778\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1162 | loss: 0.56778 - acc: 0.7698 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.56304\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1163 | loss: 0.56304 - acc: 0.7704 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.55873\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1164 | loss: 0.55873 - acc: 0.7710 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.55483\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1165 | loss: 0.55483 - acc: 0.7716 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.55129\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1166 | loss: 0.55129 - acc: 0.7720 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.54807\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1167 | loss: 0.54807 - acc: 0.7725 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.54515\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1168 | loss: 0.54515 - acc: 0.7728 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.54248\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1169 | loss: 0.54248 - acc: 0.7732 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.54005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1170 | loss: 0.54005 - acc: 0.7735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.53783\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1171 | loss: 0.53783 - acc: 0.7738 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.53579\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1172 | loss: 0.53579 - acc: 0.7740 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.53394\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1173 | loss: 0.53394 - acc: 0.7743 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.53223\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1174 | loss: 0.53223 - acc: 0.7745 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.53067\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1175 | loss: 0.53067 - acc: 0.7747 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.52923\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1176 | loss: 0.52923 - acc: 0.7748 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.52790\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1177 | loss: 0.52790 - acc: 0.7750 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.52667\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1178 | loss: 0.52667 - acc: 0.7751 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.52554\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1179 | loss: 0.52554 - acc: 0.7752 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.52449\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1180 | loss: 0.52449 - acc: 0.7753 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.52352\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1181 | loss: 0.52352 - acc: 0.7754 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.67224\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1182 | loss: 0.67224 - acc: 0.7229 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.65647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1183 | loss: 0.65647 - acc: 0.7282 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.64228\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1184 | loss: 0.64228 - acc: 0.7330 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.62950\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1185 | loss: 0.62950 - acc: 0.7374 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.61800\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1186 | loss: 0.61800 - acc: 0.7426 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.60765\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1187 | loss: 0.60765 - acc: 0.7473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.59832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1188 | loss: 0.59832 - acc: 0.7515 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.58991\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1189 | loss: 0.58991 - acc: 0.7553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.58233\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1190 | loss: 0.58233 - acc: 0.7587 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.57550\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1191 | loss: 0.57550 - acc: 0.7618 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.56934\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1192 | loss: 0.56934 - acc: 0.7646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.56377\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1193 | loss: 0.56377 - acc: 0.7670 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.68036\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1194 | loss: 0.68036 - acc: 0.7219 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.66368\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1195 | loss: 0.66368 - acc: 0.7300 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.64868\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1196 | loss: 0.64868 - acc: 0.7373 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.63518\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1197 | loss: 0.63518 - acc: 0.7438 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.62303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1198 | loss: 0.62303 - acc: 0.7497 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.61210\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1199 | loss: 0.61210 - acc: 0.7550 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.60225\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1200 | loss: 0.60225 - acc: 0.7597 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.59337\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1201 | loss: 0.59337 - acc: 0.7640 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.58537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1202 | loss: 0.58537 - acc: 0.7679 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.57816\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1203 | loss: 0.57816 - acc: 0.7714 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.57166\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1204 | loss: 0.57166 - acc: 0.7745 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.56578\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1205 | loss: 0.56578 - acc: 0.7773 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.56048\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1206 | loss: 0.56048 - acc: 0.7798 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.55569\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1207 | loss: 0.55569 - acc: 0.7821 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.55135\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1208 | loss: 0.55135 - acc: 0.7842 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.54743\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1209 | loss: 0.54743 - acc: 0.7860 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.54387\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1210 | loss: 0.54387 - acc: 0.7877 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.54065\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1211 | loss: 0.54065 - acc: 0.7892 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.53773\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1212 | loss: 0.53773 - acc: 0.7892 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.53507\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1213 | loss: 0.53507 - acc: 0.7892 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.53266\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1214 | loss: 0.53266 - acc: 0.7893 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.53046\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1215 | loss: 0.53046 - acc: 0.7893 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.52846\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1216 | loss: 0.52846 - acc: 0.7893 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.52664\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1217 | loss: 0.52664 - acc: 0.7893 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.52497\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1218 | loss: 0.52497 - acc: 0.7893 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.52344\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1219 | loss: 0.52344 - acc: 0.7893 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.52204\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1220 | loss: 0.52204 - acc: 0.7894 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.52076\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1221 | loss: 0.52076 - acc: 0.7894 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.51958\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1222 | loss: 0.51958 - acc: 0.7881 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.51849\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1223 | loss: 0.51849 - acc: 0.7869 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.51749\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1224 | loss: 0.51749 - acc: 0.7858 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.51657\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1225 | loss: 0.51657 - acc: 0.7849 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.51571\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1226 | loss: 0.51571 - acc: 0.7840 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.51491\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1227 | loss: 0.51491 - acc: 0.7833 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.51417\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1228 | loss: 0.51417 - acc: 0.7826 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.51348\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1229 | loss: 0.51348 - acc: 0.7819 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.51284\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1230 | loss: 0.51284 - acc: 0.7814 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.51223\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1231 | loss: 0.51223 - acc: 0.7809 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.51167\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1232 | loss: 0.51167 - acc: 0.7804 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.51114\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1233 | loss: 0.51114 - acc: 0.7800 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.63754\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1234 | loss: 0.63754 - acc: 0.7349 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.62440\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1235 | loss: 0.62440 - acc: 0.7404 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.61256\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1236 | loss: 0.61256 - acc: 0.7453 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.60191\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1237 | loss: 0.60191 - acc: 0.7497 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.59232\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1238 | loss: 0.59232 - acc: 0.7537 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.58368\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1239 | loss: 0.58368 - acc: 0.7572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.57589\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1240 | loss: 0.57589 - acc: 0.7618 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.56887\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1241 | loss: 0.56887 - acc: 0.7659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.56254\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1242 | loss: 0.56254 - acc: 0.7695 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.55684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1243 | loss: 0.55684 - acc: 0.7729 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.55169\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1244 | loss: 0.55169 - acc: 0.7758 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.54704\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1245 | loss: 0.54704 - acc: 0.7785 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.54283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1246 | loss: 0.54283 - acc: 0.7809 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.53904\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1247 | loss: 0.53904 - acc: 0.7831 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.53560\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1248 | loss: 0.53560 - acc: 0.7850 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.53250\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1249 | loss: 0.53250 - acc: 0.7868 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.52968\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1250 | loss: 0.52968 - acc: 0.7884 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.52713\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1251 | loss: 0.52713 - acc: 0.7898 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.62707\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1252 | loss: 0.62707 - acc: 0.7529 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.61476\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1253 | loss: 0.61476 - acc: 0.7579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.60368\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1254 | loss: 0.60368 - acc: 0.7624 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.59370\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1255 | loss: 0.59370 - acc: 0.7664 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.58471\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1256 | loss: 0.58471 - acc: 0.7700 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.57662\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1257 | loss: 0.57662 - acc: 0.7733 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.56933\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1258 | loss: 0.56933 - acc: 0.7762 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.56275\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1259 | loss: 0.56275 - acc: 0.7789 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.55682\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1260 | loss: 0.55682 - acc: 0.7812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.55148\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1261 | loss: 0.55148 - acc: 0.7834 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.54665\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1262 | loss: 0.54665 - acc: 0.7853 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.54229\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1263 | loss: 0.54229 - acc: 0.7870 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.53835\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1264 | loss: 0.53835 - acc: 0.7886 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.53479\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1265 | loss: 0.53479 - acc: 0.7900 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.53156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1266 | loss: 0.53156 - acc: 0.7913 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.52865\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1267 | loss: 0.52865 - acc: 0.7924 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.65327\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1268 | loss: 0.65327 - acc: 0.7487 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.63817\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1269 | loss: 0.63817 - acc: 0.7541 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.62458\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1270 | loss: 0.62458 - acc: 0.7603 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.61235\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1271 | loss: 0.61235 - acc: 0.7658 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.60134\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1272 | loss: 0.60134 - acc: 0.7708 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.59143\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1273 | loss: 0.59143 - acc: 0.7753 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.58251\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1274 | loss: 0.58251 - acc: 0.7794 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.57446\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1275 | loss: 0.57446 - acc: 0.7830 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.56722\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1276 | loss: 0.56722 - acc: 0.7863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.56069\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1277 | loss: 0.56069 - acc: 0.7892 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.55480\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1278 | loss: 0.55480 - acc: 0.7919 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.54948\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1279 | loss: 0.54948 - acc: 0.7943 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.54468\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1280 | loss: 0.54468 - acc: 0.7964 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.54035\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1281 | loss: 0.54035 - acc: 0.7984 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.53643\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1282 | loss: 0.53643 - acc: 0.8001 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.53289\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1283 | loss: 0.53289 - acc: 0.8017 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.52968\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1284 | loss: 0.52968 - acc: 0.8031 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.52678\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1285 | loss: 0.52678 - acc: 0.8044 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.52415\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1286 | loss: 0.52415 - acc: 0.8055 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.52176\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1287 | loss: 0.52176 - acc: 0.8065 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.51959\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1288 | loss: 0.51959 - acc: 0.8075 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.51762\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1289 | loss: 0.51762 - acc: 0.8083 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.51582\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1290 | loss: 0.51582 - acc: 0.8077 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.51419\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1291 | loss: 0.51419 - acc: 0.8072 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.51270\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1292 | loss: 0.51270 - acc: 0.8068 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.51134\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1293 | loss: 0.51134 - acc: 0.8063 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.51009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1294 | loss: 0.51009 - acc: 0.8060 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.50895\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1295 | loss: 0.50895 - acc: 0.8056 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.50790\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1296 | loss: 0.50790 - acc: 0.8053 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.50694\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1297 | loss: 0.50694 - acc: 0.8051 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.50606\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1298 | loss: 0.50606 - acc: 0.8048 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.50524\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1299 | loss: 0.50524 - acc: 0.8046 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.50449\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1300 | loss: 0.50449 - acc: 0.8044 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.50379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1301 | loss: 0.50379 - acc: 0.8042 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.64524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1302 | loss: 0.64524 - acc: 0.7541 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.63046\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1303 | loss: 0.63046 - acc: 0.7589 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.61715\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1304 | loss: 0.61715 - acc: 0.7697 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.60517\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1305 | loss: 0.60517 - acc: 0.7697 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.59438\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1306 | loss: 0.59438 - acc: 0.7743 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.58467\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1307 | loss: 0.58467 - acc: 0.7785 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.57593\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1308 | loss: 0.57593 - acc: 0.7822 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.56805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1309 | loss: 0.56805 - acc: 0.7869 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.56096\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1310 | loss: 0.56096 - acc: 0.7911 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.55456\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1311 | loss: 0.55456 - acc: 0.7949 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.54880\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1312 | loss: 0.54880 - acc: 0.7983 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.54359\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1313 | loss: 0.54359 - acc: 0.8014 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.53890\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1314 | loss: 0.53890 - acc: 0.8041 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.53466\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1315 | loss: 0.53466 - acc: 0.8066 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.53083\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1316 | loss: 0.53083 - acc: 0.8088 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.52737\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1317 | loss: 0.52737 - acc: 0.8108 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.66179\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1318 | loss: 0.66179 - acc: 0.7653 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.64522\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1319 | loss: 0.64522 - acc: 0.7716 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.63032\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1320 | loss: 0.63032 - acc: 0.7774 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.61691\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1321 | loss: 0.61691 - acc: 0.7839 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.60484\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1322 | loss: 0.60484 - acc: 0.7897 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.59398\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1323 | loss: 0.59398 - acc: 0.7949 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.58420\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1324 | loss: 0.58420 - acc: 0.7996 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.57539\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1325 | loss: 0.57539 - acc: 0.8039 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.56745\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1326 | loss: 0.56745 - acc: 0.8077 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.56030\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1327 | loss: 0.56030 - acc: 0.8111 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.55385\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1328 | loss: 0.55385 - acc: 0.8142 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.54803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1329 | loss: 0.54803 - acc: 0.8170 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.54279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1330 | loss: 0.54279 - acc: 0.8195 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.53805\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1331 | loss: 0.53805 - acc: 0.8205 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.53377\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1332 | loss: 0.53377 - acc: 0.8213 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.52990\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1333 | loss: 0.52990 - acc: 0.8221 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.52640\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1334 | loss: 0.52640 - acc: 0.8228 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.52324\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1335 | loss: 0.52324 - acc: 0.8234 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.52037\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1336 | loss: 0.52037 - acc: 0.8239 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.51777\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1337 | loss: 0.51777 - acc: 0.8244 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.51542\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1338 | loss: 0.51542 - acc: 0.8236 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.51328\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1339 | loss: 0.51328 - acc: 0.8228 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.51133\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1340 | loss: 0.51133 - acc: 0.8221 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.50956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1341 | loss: 0.50956 - acc: 0.8215 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.50795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1342 | loss: 0.50795 - acc: 0.8209 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.50648\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1343 | loss: 0.50648 - acc: 0.8204 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.50514\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1344 | loss: 0.50514 - acc: 0.8199 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.50391\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1345 | loss: 0.50391 - acc: 0.8195 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.50278\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1346 | loss: 0.50278 - acc: 0.8191 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.50175\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1347 | loss: 0.50175 - acc: 0.8188 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.50081\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1348 | loss: 0.50081 - acc: 0.8185 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.49994\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1349 | loss: 0.49994 - acc: 0.8182 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.59483\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1350 | loss: 0.59483 - acc: 0.7772 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.58453\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1351 | loss: 0.58453 - acc: 0.7811 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.71475\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1352 | loss: 0.71475 - acc: 0.7319 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.69247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1353 | loss: 0.69247 - acc: 0.7403 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.67243\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1354 | loss: 0.67243 - acc: 0.7492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.65441\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1355 | loss: 0.65441 - acc: 0.7571 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.63821\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1356 | loss: 0.63821 - acc: 0.7656 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.62363\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1357 | loss: 0.62363 - acc: 0.7746 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.61051\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1358 | loss: 0.61051 - acc: 0.7827 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.59871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1359 | loss: 0.59871 - acc: 0.7912 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.58808\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1360 | loss: 0.58808 - acc: 0.7990 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.57852\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1361 | loss: 0.57852 - acc: 0.8059 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.56991\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1362 | loss: 0.56991 - acc: 0.8122 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.56215\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1363 | loss: 0.56215 - acc: 0.8178 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.55516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1364 | loss: 0.55516 - acc: 0.8228 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.54885\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1365 | loss: 0.54885 - acc: 0.8274 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.54317\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1366 | loss: 0.54317 - acc: 0.8315 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.53804\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1367 | loss: 0.53804 - acc: 0.8352 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.53340\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1368 | loss: 0.53340 - acc: 0.8385 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.52922\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1369 | loss: 0.52922 - acc: 0.8415 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.67521\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1370 | loss: 0.67521 - acc: 0.7863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.65684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1371 | loss: 0.65684 - acc: 0.7945 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.64032\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1372 | loss: 0.64032 - acc: 0.8019 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.62546\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1373 | loss: 0.62546 - acc: 0.8086 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.61209\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1374 | loss: 0.61209 - acc: 0.8145 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.60006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1375 | loss: 0.60006 - acc: 0.8199 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.58923\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1376 | loss: 0.58923 - acc: 0.8248 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.57948\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1377 | loss: 0.57948 - acc: 0.8291 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.57070\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1378 | loss: 0.57070 - acc: 0.8331 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.56278\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1379 | loss: 0.56278 - acc: 0.8366 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.55565\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1380 | loss: 0.55565 - acc: 0.8398 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.54921\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1381 | loss: 0.54921 - acc: 0.8427 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.54341\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1382 | loss: 0.54341 - acc: 0.8452 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.53817\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1383 | loss: 0.53817 - acc: 0.8475 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.53343\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1384 | loss: 0.53343 - acc: 0.8496 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.52916\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1385 | loss: 0.52916 - acc: 0.8515 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.68090\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1386 | loss: 0.68090 - acc: 0.7953 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.66187\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1387 | loss: 0.66187 - acc: 0.8026 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.64476\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1388 | loss: 0.64476 - acc: 0.8092 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.62936\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1389 | loss: 0.62936 - acc: 0.8151 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.61551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1390 | loss: 0.61551 - acc: 0.8205 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.60304\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1391 | loss: 0.60304 - acc: 0.8252 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.59182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1392 | loss: 0.59182 - acc: 0.8296 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.58172\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1393 | loss: 0.58172 - acc: 0.8335 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.57262\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1394 | loss: 0.57262 - acc: 0.8369 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.56442\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1395 | loss: 0.56442 - acc: 0.8401 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.55703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1396 | loss: 0.55703 - acc: 0.8429 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.55037\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1397 | loss: 0.55037 - acc: 0.8455 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.54435\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1398 | loss: 0.54435 - acc: 0.8478 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.53892\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1399 | loss: 0.53892 - acc: 0.8485 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.53402\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1400 | loss: 0.53402 - acc: 0.8492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.52959\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1401 | loss: 0.52959 - acc: 0.8485 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.66077\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1402 | loss: 0.66077 - acc: 0.7965 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.64365\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1403 | loss: 0.64365 - acc: 0.8024 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.62826\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1404 | loss: 0.62826 - acc: 0.8090 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.61441\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1405 | loss: 0.61441 - acc: 0.8149 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.60194\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1406 | loss: 0.60194 - acc: 0.8203 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.59072\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1407 | loss: 0.59072 - acc: 0.8251 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.58062\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1408 | loss: 0.58062 - acc: 0.8294 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.57152\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1409 | loss: 0.57152 - acc: 0.8333 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.56332\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1410 | loss: 0.56332 - acc: 0.8368 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.55593\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1411 | loss: 0.55593 - acc: 0.8400 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.54927\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1412 | loss: 0.54927 - acc: 0.8428 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.54325\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1413 | loss: 0.54325 - acc: 0.8454 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.53782\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1414 | loss: 0.53782 - acc: 0.8477 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.53292\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1415 | loss: 0.53292 - acc: 0.8498 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.52848\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1416 | loss: 0.52848 - acc: 0.8516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.52447\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1417 | loss: 0.52447 - acc: 0.8533 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.52084\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1418 | loss: 0.52084 - acc: 0.8548 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.51755\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1419 | loss: 0.51755 - acc: 0.8562 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m0.51457\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1420 | loss: 0.51457 - acc: 0.8574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.51186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1421 | loss: 0.51186 - acc: 0.8585 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.50940\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1422 | loss: 0.50940 - acc: 0.8595 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.50717\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1423 | loss: 0.50717 - acc: 0.8591 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.50513\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1424 | loss: 0.50513 - acc: 0.8587 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.50328\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1425 | loss: 0.50328 - acc: 0.8570 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.50158\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1426 | loss: 0.50158 - acc: 0.8555 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.50003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1427 | loss: 0.50003 - acc: 0.8542 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.49862\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1428 | loss: 0.49862 - acc: 0.8530 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.49732\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1429 | loss: 0.49732 - acc: 0.8519 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.49613\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1430 | loss: 0.49613 - acc: 0.8509 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.49503\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1431 | loss: 0.49503 - acc: 0.8500 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.49402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1432 | loss: 0.49402 - acc: 0.8492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.49309\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1433 | loss: 0.49309 - acc: 0.8485 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.49222\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1434 | loss: 0.49222 - acc: 0.8479 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.49143\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1435 | loss: 0.49143 - acc: 0.8473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.49068\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1436 | loss: 0.49068 - acc: 0.8468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.48999\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1437 | loss: 0.48999 - acc: 0.8463 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.63680\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1438 | loss: 0.63680 - acc: 0.7946 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.62148\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1439 | loss: 0.62148 - acc: 0.7993 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.60769\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1440 | loss: 0.60769 - acc: 0.8062 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.59528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1441 | loss: 0.59528 - acc: 0.8125 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.58411\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1442 | loss: 0.58411 - acc: 0.8181 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.57405\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1443 | loss: 0.57405 - acc: 0.8231 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.56499\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1444 | loss: 0.56499 - acc: 0.8276 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.55683\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1445 | loss: 0.55683 - acc: 0.8317 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.54948\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1446 | loss: 0.54948 - acc: 0.8354 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.54285\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1447 | loss: 0.54285 - acc: 0.8387 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.53687\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1448 | loss: 0.53687 - acc: 0.8417 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.53148\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1449 | loss: 0.53148 - acc: 0.8443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.68341\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1450 | loss: 0.68341 - acc: 0.7875 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.66336\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1451 | loss: 0.66336 - acc: 0.7956 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.64534\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1452 | loss: 0.64534 - acc: 0.8029 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.62912\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1453 | loss: 0.62912 - acc: 0.8095 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.61453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1454 | loss: 0.61453 - acc: 0.8153 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.60141\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1455 | loss: 0.60141 - acc: 0.8207 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.58960\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1456 | loss: 0.58960 - acc: 0.8254 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.57896\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1457 | loss: 0.57896 - acc: 0.8297 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.56939\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1458 | loss: 0.56939 - acc: 0.8336 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.56077\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1459 | loss: 0.56077 - acc: 0.8371 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.55300\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1460 | loss: 0.55300 - acc: 0.8402 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.54599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1461 | loss: 0.54599 - acc: 0.8430 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.53968\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1462 | loss: 0.53968 - acc: 0.8456 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.53398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1463 | loss: 0.53398 - acc: 0.8479 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.52884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1464 | loss: 0.52884 - acc: 0.8499 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.52419\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1465 | loss: 0.52419 - acc: 0.8518 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.51999\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1466 | loss: 0.51999 - acc: 0.8534 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.51620\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1467 | loss: 0.51620 - acc: 0.8549 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.51277\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1468 | loss: 0.51277 - acc: 0.8563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.50966\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1469 | loss: 0.50966 - acc: 0.8575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.50684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1470 | loss: 0.50684 - acc: 0.8586 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.50429\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1471 | loss: 0.50429 - acc: 0.8596 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.50197\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1472 | loss: 0.50197 - acc: 0.8605 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.49986\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1473 | loss: 0.49986 - acc: 0.8613 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.49794\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1474 | loss: 0.49794 - acc: 0.8620 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.49620\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1475 | loss: 0.49620 - acc: 0.8626 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.49460\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1476 | loss: 0.49460 - acc: 0.8632 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.49315\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1477 | loss: 0.49315 - acc: 0.8637 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.49182\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1478 | loss: 0.49182 - acc: 0.8642 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.49061\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1479 | loss: 0.49061 - acc: 0.8646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.48949\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1480 | loss: 0.48949 - acc: 0.8650 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.48847\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1481 | loss: 0.48847 - acc: 0.8653 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.48752\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1482 | loss: 0.48752 - acc: 0.8656 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.48665\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1483 | loss: 0.48665 - acc: 0.8659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.48585\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1484 | loss: 0.48585 - acc: 0.8662 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.48511\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1485 | loss: 0.48511 - acc: 0.8664 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.48442\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1486 | loss: 0.48442 - acc: 0.8666 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.48378\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1487 | loss: 0.48378 - acc: 0.8668 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.62200\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1488 | loss: 0.62200 - acc: 0.8104 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.60759\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1489 | loss: 0.60759 - acc: 0.8162 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.59461\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1490 | loss: 0.59461 - acc: 0.8214 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.58294\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1491 | loss: 0.58294 - acc: 0.8261 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.57243\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1492 | loss: 0.57243 - acc: 0.8303 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.56296\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1493 | loss: 0.56296 - acc: 0.8341 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.55444\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1494 | loss: 0.55444 - acc: 0.8376 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.54677\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1495 | loss: 0.54677 - acc: 0.8407 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.69202\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1496 | loss: 0.69202 - acc: 0.7895 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.67060\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1497 | loss: 0.67060 - acc: 0.7974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.65134\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1498 | loss: 0.65134 - acc: 0.8045 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.63401\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1499 | loss: 0.63401 - acc: 0.8109 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.61843\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1500 | loss: 0.61843 - acc: 0.8166 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.60441\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1501 | loss: 0.60441 - acc: 0.8218 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.59179\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1502 | loss: 0.59179 - acc: 0.8278 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.58044\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1503 | loss: 0.58044 - acc: 0.8332 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.57022\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1504 | loss: 0.57022 - acc: 0.8380 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.56101\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1505 | loss: 0.56101 - acc: 0.8424 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.55272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1506 | loss: 0.55272 - acc: 0.8463 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.54525\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1507 | loss: 0.54525 - acc: 0.8485 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.53851\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1508 | loss: 0.53851 - acc: 0.8505 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.53244\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1509 | loss: 0.53244 - acc: 0.8523 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.52696\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1510 | loss: 0.52696 - acc: 0.8539 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.52201\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1511 | loss: 0.52201 - acc: 0.8554 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.63615\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1512 | loss: 0.63615 - acc: 0.8067 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.62027\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1513 | loss: 0.62027 - acc: 0.8128 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.60599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1514 | loss: 0.60599 - acc: 0.8184 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.59313\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1515 | loss: 0.59313 - acc: 0.8234 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.58157\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1516 | loss: 0.58157 - acc: 0.8279 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.57115\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1517 | loss: 0.57115 - acc: 0.8320 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.56178\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1518 | loss: 0.56178 - acc: 0.8356 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.55333\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1519 | loss: 0.55333 - acc: 0.8389 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.67984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1520 | loss: 0.67984 - acc: 0.7853 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.65960\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1521 | loss: 0.65960 - acc: 0.7936 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.64139\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1522 | loss: 0.64139 - acc: 0.8011 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.62502\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1523 | loss: 0.62502 - acc: 0.8091 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.61029\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1524 | loss: 0.61029 - acc: 0.8164 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.59704\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1525 | loss: 0.59704 - acc: 0.8229 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.58512\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1526 | loss: 0.58512 - acc: 0.8287 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.57438\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1527 | loss: 0.57438 - acc: 0.8340 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.56471\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1528 | loss: 0.56471 - acc: 0.8388 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.55600\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1529 | loss: 0.55600 - acc: 0.8431 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.54815\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1530 | loss: 0.54815 - acc: 0.8469 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.54108\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1531 | loss: 0.54108 - acc: 0.8504 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.53469\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1532 | loss: 0.53469 - acc: 0.8535 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.52893\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1533 | loss: 0.52893 - acc: 0.8550 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.52373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1534 | loss: 0.52373 - acc: 0.8563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.51902\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1535 | loss: 0.51902 - acc: 0.8575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.51477\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1536 | loss: 0.51477 - acc: 0.8586 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.51093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1537 | loss: 0.51093 - acc: 0.8596 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.50745\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1538 | loss: 0.50745 - acc: 0.8605 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.50429\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1539 | loss: 0.50429 - acc: 0.8613 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.50143\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1540 | loss: 0.50143 - acc: 0.8620 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.49883\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1541 | loss: 0.49883 - acc: 0.8626 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.49647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1542 | loss: 0.49647 - acc: 0.8632 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.49433\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1543 | loss: 0.49433 - acc: 0.8637 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.49237\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1544 | loss: 0.49237 - acc: 0.8642 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.49059\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1545 | loss: 0.49059 - acc: 0.8646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.48897\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1546 | loss: 0.48897 - acc: 0.8650 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.48748\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1547 | loss: 0.48748 - acc: 0.8653 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.48612\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1548 | loss: 0.48612 - acc: 0.8657 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.48488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1549 | loss: 0.48488 - acc: 0.8659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.48373\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1550 | loss: 0.48373 - acc: 0.8662 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.48268\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1551 | loss: 0.48268 - acc: 0.8664 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.48171\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1552 | loss: 0.48171 - acc: 0.8666 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.48082\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1553 | loss: 0.48082 - acc: 0.8668 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.47999\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1554 | loss: 0.47999 - acc: 0.8670 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.47923\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1555 | loss: 0.47923 - acc: 0.8671 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.47852\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1556 | loss: 0.47852 - acc: 0.8672 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.47786\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1557 | loss: 0.47786 - acc: 0.8674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.47724\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1558 | loss: 0.47724 - acc: 0.8675 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.47666\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1559 | loss: 0.47666 - acc: 0.8676 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.47612\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1560 | loss: 0.47612 - acc: 0.8676 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.47562\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1561 | loss: 0.47562 - acc: 0.8677 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.62335\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1562 | loss: 0.62335 - acc: 0.8165 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.60811\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1563 | loss: 0.60811 - acc: 0.8217 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.59439\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1564 | loss: 0.59439 - acc: 0.8263 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.58204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1565 | loss: 0.58204 - acc: 0.8306 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.57092\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1566 | loss: 0.57092 - acc: 0.8343 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.56092\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1567 | loss: 0.56092 - acc: 0.8377 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.55191\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1568 | loss: 0.55191 - acc: 0.8408 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.54380\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1569 | loss: 0.54380 - acc: 0.8449 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.53649\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1570 | loss: 0.53649 - acc: 0.8486 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.52990\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1571 | loss: 0.52990 - acc: 0.8519 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.52396\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1572 | loss: 0.52396 - acc: 0.8548 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.51860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1573 | loss: 0.51860 - acc: 0.8575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.51377\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1574 | loss: 0.51377 - acc: 0.8599 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.50940\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1575 | loss: 0.50940 - acc: 0.8621 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.63266\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1576 | loss: 0.63266 - acc: 0.8088 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.61640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1577 | loss: 0.61640 - acc: 0.8160 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.60178\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1578 | loss: 0.60178 - acc: 0.8226 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.58862\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1579 | loss: 0.58862 - acc: 0.8285 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.57677\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1580 | loss: 0.57677 - acc: 0.8338 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.56611\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1581 | loss: 0.56611 - acc: 0.8386 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.55651\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1582 | loss: 0.55651 - acc: 0.8429 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.54786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1583 | loss: 0.54786 - acc: 0.8468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.54007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1584 | loss: 0.54007 - acc: 0.8502 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.53305\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1585 | loss: 0.53305 - acc: 0.8534 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.52672\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1586 | loss: 0.52672 - acc: 0.8562 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.52101\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1587 | loss: 0.52101 - acc: 0.8587 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.51585\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1588 | loss: 0.51585 - acc: 0.8610 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.51120\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1589 | loss: 0.51120 - acc: 0.8631 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.50699\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1590 | loss: 0.50699 - acc: 0.8649 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.50319\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1591 | loss: 0.50319 - acc: 0.8666 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.49975\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1592 | loss: 0.49975 - acc: 0.8681 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.49664\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1593 | loss: 0.49664 - acc: 0.8694 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.49382\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1594 | loss: 0.49382 - acc: 0.8693 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.49126\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1595 | loss: 0.49126 - acc: 0.8692 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.48894\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1596 | loss: 0.48894 - acc: 0.8692 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.48684\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1597 | loss: 0.48684 - acc: 0.8691 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.48493\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1598 | loss: 0.48493 - acc: 0.8690 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.48319\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1599 | loss: 0.48319 - acc: 0.8690 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.48160\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1600 | loss: 0.48160 - acc: 0.8689 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.48016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1601 | loss: 0.48016 - acc: 0.8689 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.47884\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1602 | loss: 0.47884 - acc: 0.8688 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.47763\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1603 | loss: 0.47763 - acc: 0.8688 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.47653\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1604 | loss: 0.47653 - acc: 0.8687 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.47552\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1605 | loss: 0.47552 - acc: 0.8687 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.47459\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1606 | loss: 0.47459 - acc: 0.8687 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.47374\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1607 | loss: 0.47374 - acc: 0.8687 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.47295\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1608 | loss: 0.47295 - acc: 0.8686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.47222\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1609 | loss: 0.47222 - acc: 0.8686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.47155\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1610 | loss: 0.47155 - acc: 0.8686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m0.47093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1611 | loss: 0.47093 - acc: 0.8686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m0.47035\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1612 | loss: 0.47035 - acc: 0.8686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m0.46981\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1613 | loss: 0.46981 - acc: 0.8685 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m0.46930\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1614 | loss: 0.46930 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.46883\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1615 | loss: 0.46883 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.46839\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1616 | loss: 0.46839 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.46798\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1617 | loss: 0.46798 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.46759\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1618 | loss: 0.46759 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.46722\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1619 | loss: 0.46722 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m0.46688\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1620 | loss: 0.46688 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.46655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1621 | loss: 0.46655 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.46623\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1622 | loss: 0.46623 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.46594\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1623 | loss: 0.46594 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.46565\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1624 | loss: 0.46565 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.46538\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1625 | loss: 0.46538 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.46512\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1626 | loss: 0.46512 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.46487\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1627 | loss: 0.46487 - acc: 0.8684 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.59737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1628 | loss: 0.59737 - acc: 0.8145 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.58388\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1629 | loss: 0.58388 - acc: 0.8199 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m0.57174\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1630 | loss: 0.57174 - acc: 0.8261 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.56081\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1631 | loss: 0.56081 - acc: 0.8316 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m0.55097\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1632 | loss: 0.55097 - acc: 0.8366 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m0.54211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1633 | loss: 0.54211 - acc: 0.8411 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m0.53413\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1634 | loss: 0.53413 - acc: 0.8452 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m0.52695\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1635 | loss: 0.52695 - acc: 0.8488 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m0.52048\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1636 | loss: 0.52048 - acc: 0.8521 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m0.51464\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1637 | loss: 0.51464 - acc: 0.8550 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m0.50938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1638 | loss: 0.50938 - acc: 0.8577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m0.50463\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1639 | loss: 0.50463 - acc: 0.8601 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m0.50035\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1640 | loss: 0.50035 - acc: 0.8622 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m0.49649\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1641 | loss: 0.49649 - acc: 0.8642 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m0.49299\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1642 | loss: 0.49299 - acc: 0.8659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m0.48984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1643 | loss: 0.48984 - acc: 0.8675 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m0.48698\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1644 | loss: 0.48698 - acc: 0.8689 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m0.48440\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1645 | loss: 0.48440 - acc: 0.8701 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m0.64285\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1646 | loss: 0.64285 - acc: 0.8121 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m0.62467\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1647 | loss: 0.62467 - acc: 0.8190 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m0.60832\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1648 | loss: 0.60832 - acc: 0.8253 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.59361\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1649 | loss: 0.59361 - acc: 0.8309 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.58038\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1650 | loss: 0.58038 - acc: 0.8360 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m0.56846\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1651 | loss: 0.56846 - acc: 0.8419 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m0.55774\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1652 | loss: 0.55774 - acc: 0.8471 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m0.54809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1653 | loss: 0.54809 - acc: 0.8519 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m0.53939\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1654 | loss: 0.53939 - acc: 0.8562 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.53156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1655 | loss: 0.53156 - acc: 0.8600 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m0.66383\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1656 | loss: 0.66383 - acc: 0.8043 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m0.64356\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1657 | loss: 0.64356 - acc: 0.8133 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m0.62534\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1658 | loss: 0.62534 - acc: 0.8215 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m0.60894\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1659 | loss: 0.60894 - acc: 0.8288 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m0.59420\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1660 | loss: 0.59420 - acc: 0.8354 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.58093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1661 | loss: 0.58093 - acc: 0.8413 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.71768\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1662 | loss: 0.71768 - acc: 0.7861 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.69210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1663 | loss: 0.69210 - acc: 0.7970 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.66910\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1664 | loss: 0.66910 - acc: 0.8081 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.64843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1665 | loss: 0.64843 - acc: 0.8181 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.62984\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1666 | loss: 0.62984 - acc: 0.8271 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.61313\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1667 | loss: 0.61313 - acc: 0.8351 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.73140\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1668 | loss: 0.73140 - acc: 0.7806 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.70458\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1669 | loss: 0.70458 - acc: 0.7933 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.68047\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1670 | loss: 0.68047 - acc: 0.8048 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.65879\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1671 | loss: 0.65879 - acc: 0.8151 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.63931\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1672 | loss: 0.63931 - acc: 0.8244 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.62178\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1673 | loss: 0.62178 - acc: 0.8327 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.60602\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1674 | loss: 0.60602 - acc: 0.8402 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.59183\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1675 | loss: 0.59183 - acc: 0.8470 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.57907\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1676 | loss: 0.57907 - acc: 0.8531 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.56757\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1677 | loss: 0.56757 - acc: 0.8586 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.55722\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1678 | loss: 0.55722 - acc: 0.8635 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.54789\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1679 | loss: 0.54789 - acc: 0.8679 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m0.53948\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1680 | loss: 0.53948 - acc: 0.8719 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.53190\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1681 | loss: 0.53190 - acc: 0.8742 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.52506\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1682 | loss: 0.52506 - acc: 0.8763 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.51888\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1683 | loss: 0.51888 - acc: 0.8781 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.51330\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1684 | loss: 0.51330 - acc: 0.8798 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.50826\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1685 | loss: 0.50826 - acc: 0.8813 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.50371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1686 | loss: 0.50371 - acc: 0.8826 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.49958\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1687 | loss: 0.49958 - acc: 0.8825 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.49585\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1688 | loss: 0.49585 - acc: 0.8824 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.49247\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1689 | loss: 0.49247 - acc: 0.8823 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.62941\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1690 | loss: 0.62941 - acc: 0.8309 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.61266\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1691 | loss: 0.61266 - acc: 0.8360 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.59759\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1692 | loss: 0.59759 - acc: 0.8406 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.58402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1693 | loss: 0.58402 - acc: 0.8447 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.57182\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1694 | loss: 0.57182 - acc: 0.8484 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.56082\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1695 | loss: 0.56082 - acc: 0.8517 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.55092\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1696 | loss: 0.55092 - acc: 0.8547 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.54200\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1697 | loss: 0.54200 - acc: 0.8574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.53396\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1698 | loss: 0.53396 - acc: 0.8598 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.52672\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1699 | loss: 0.52672 - acc: 0.8620 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.52018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1700 | loss: 0.52018 - acc: 0.8639 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.51428\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1701 | loss: 0.51428 - acc: 0.8657 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.64582\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1702 | loss: 0.64582 - acc: 0.8133 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.62735\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1703 | loss: 0.62735 - acc: 0.8215 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.61074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1704 | loss: 0.61074 - acc: 0.8288 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.59580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1705 | loss: 0.59580 - acc: 0.8354 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.58235\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1706 | loss: 0.58235 - acc: 0.8413 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.57025\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1707 | loss: 0.57025 - acc: 0.8467 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.55935\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1708 | loss: 0.55935 - acc: 0.8515 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.54954\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1709 | loss: 0.54954 - acc: 0.8558 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.54070\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1710 | loss: 0.54070 - acc: 0.8597 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.53273\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1711 | loss: 0.53273 - acc: 0.8632 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.64452\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1712 | loss: 0.64452 - acc: 0.8124 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.62617\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1713 | loss: 0.62617 - acc: 0.8206 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.60968\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1714 | loss: 0.60968 - acc: 0.8294 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.59484\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1715 | loss: 0.59484 - acc: 0.8372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.58148\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1716 | loss: 0.58148 - acc: 0.8443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.56947\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1717 | loss: 0.56947 - acc: 0.8506 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.69535\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1718 | loss: 0.69535 - acc: 0.7985 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.67198\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1719 | loss: 0.67198 - acc: 0.8094 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.65097\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1720 | loss: 0.65097 - acc: 0.8193 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.63208\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1721 | loss: 0.63208 - acc: 0.8281 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.61509\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1722 | loss: 0.61509 - acc: 0.8361 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.59981\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1723 | loss: 0.59981 - acc: 0.8433 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.58606\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1724 | loss: 0.58606 - acc: 0.8497 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.57368\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1725 | loss: 0.57368 - acc: 0.8556 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.56253\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1726 | loss: 0.56253 - acc: 0.8608 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.55249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1727 | loss: 0.55249 - acc: 0.8655 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.54343\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1728 | loss: 0.54343 - acc: 0.8697 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.53525\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1729 | loss: 0.53525 - acc: 0.8736 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.67546\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1730 | loss: 0.67546 - acc: 0.8112 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.65409\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1731 | loss: 0.65409 - acc: 0.8209 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.63487\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1732 | loss: 0.63487 - acc: 0.8296 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.61758\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1733 | loss: 0.61758 - acc: 0.8374 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.60203\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1734 | loss: 0.60203 - acc: 0.8445 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.58803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1735 | loss: 0.58803 - acc: 0.8508 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.57543\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1736 | loss: 0.57543 - acc: 0.8565 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.56408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1737 | loss: 0.56408 - acc: 0.8616 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.67415\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1738 | loss: 0.67415 - acc: 0.8189 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.65293\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1739 | loss: 0.65293 - acc: 0.8278 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.77174\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1740 | loss: 0.77174 - acc: 0.7792 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.74084\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1741 | loss: 0.74084 - acc: 0.7921 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.71308\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1742 | loss: 0.71308 - acc: 0.8037 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.68814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1743 | loss: 0.68814 - acc: 0.8141 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.66572\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1744 | loss: 0.66572 - acc: 0.8235 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.64557\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1745 | loss: 0.64557 - acc: 0.8319 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.62744\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1746 | loss: 0.62744 - acc: 0.8395 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.61114\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1747 | loss: 0.61114 - acc: 0.8464 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.59646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1748 | loss: 0.59646 - acc: 0.8525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.58324\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1749 | loss: 0.58324 - acc: 0.8580 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.57133\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1750 | loss: 0.57133 - acc: 0.8630 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.56059\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1751 | loss: 0.56059 - acc: 0.8675 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.55090\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1752 | loss: 0.55090 - acc: 0.8716 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.54215\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1753 | loss: 0.54215 - acc: 0.8752 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.53425\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1754 | loss: 0.53425 - acc: 0.8785 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.52711\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1755 | loss: 0.52711 - acc: 0.8814 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.52064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1756 | loss: 0.52064 - acc: 0.8841 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.51479\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1757 | loss: 0.51479 - acc: 0.8864 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.64886\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1758 | loss: 0.64886 - acc: 0.8320 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.63016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1759 | loss: 0.63016 - acc: 0.8396 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.61333\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1760 | loss: 0.61333 - acc: 0.8464 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.59818\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1761 | loss: 0.59818 - acc: 0.8526 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.58454\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1762 | loss: 0.58454 - acc: 0.8568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.57225\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1763 | loss: 0.57225 - acc: 0.8606 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.56118\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1764 | loss: 0.56118 - acc: 0.8640 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.55119\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1765 | loss: 0.55119 - acc: 0.8671 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.54218\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1766 | loss: 0.54218 - acc: 0.8698 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.53405\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1767 | loss: 0.53405 - acc: 0.8723 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.52670\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1768 | loss: 0.52670 - acc: 0.8746 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.52007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1769 | loss: 0.52007 - acc: 0.8766 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.64908\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1770 | loss: 0.64908 - acc: 0.8152 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.63018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1771 | loss: 0.63018 - acc: 0.8232 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.73290\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1772 | loss: 0.73290 - acc: 0.7790 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.70567\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1773 | loss: 0.70567 - acc: 0.7906 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.79695\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1774 | loss: 0.79695 - acc: 0.7536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.76341\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1775 | loss: 0.76341 - acc: 0.7691 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.73329\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1776 | loss: 0.73329 - acc: 0.7830 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.70622\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1777 | loss: 0.70622 - acc: 0.7954 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.68191\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1778 | loss: 0.68191 - acc: 0.8067 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m0.66005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1779 | loss: 0.66005 - acc: 0.8168 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.64041\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1780 | loss: 0.64041 - acc: 0.8259 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.62274\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1781 | loss: 0.62274 - acc: 0.8341 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.60684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1782 | loss: 0.60684 - acc: 0.8415 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.59252\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1783 | loss: 0.59252 - acc: 0.8481 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.57963\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1784 | loss: 0.57963 - acc: 0.8541 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.56801\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1785 | loss: 0.56801 - acc: 0.8595 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.55752\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1786 | loss: 0.55752 - acc: 0.8643 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.54806\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1787 | loss: 0.54806 - acc: 0.8687 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.53951\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1788 | loss: 0.53951 - acc: 0.8726 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.53178\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1789 | loss: 0.53178 - acc: 0.8761 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.52478\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1790 | loss: 0.52478 - acc: 0.8793 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.51844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1791 | loss: 0.51844 - acc: 0.8822 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.51270\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1792 | loss: 0.51270 - acc: 0.8847 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.50749\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1793 | loss: 0.50749 - acc: 0.8871 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.64951\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1794 | loss: 0.64951 - acc: 0.8299 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.63058\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1795 | loss: 0.63058 - acc: 0.8377 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.61355\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1796 | loss: 0.61355 - acc: 0.8447 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.59821\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1797 | loss: 0.59821 - acc: 0.8511 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.58441\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1798 | loss: 0.58441 - acc: 0.8567 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.57196\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1799 | loss: 0.57196 - acc: 0.8619 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.69379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1800 | loss: 0.69379 - acc: 0.8073 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.67042\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1801 | loss: 0.67042 - acc: 0.8173 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.77713\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1802 | loss: 0.77713 - acc: 0.7672 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.74550\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1803 | loss: 0.74550 - acc: 0.7812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.71709\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1804 | loss: 0.71709 - acc: 0.7939 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.69155\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1805 | loss: 0.69155 - acc: 0.8053 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.66860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1806 | loss: 0.66860 - acc: 0.8156 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.64797\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1807 | loss: 0.64797 - acc: 0.8248 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.62940\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1808 | loss: 0.62940 - acc: 0.8331 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.61270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1809 | loss: 0.61270 - acc: 0.8406 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.59767\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1810 | loss: 0.59767 - acc: 0.8473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.58412\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1811 | loss: 0.58412 - acc: 0.8534 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.57192\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1812 | loss: 0.57192 - acc: 0.8588 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.56091\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1813 | loss: 0.56091 - acc: 0.8637 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.55097\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1814 | loss: 0.55097 - acc: 0.8681 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.54199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1815 | loss: 0.54199 - acc: 0.8721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.53388\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1816 | loss: 0.53388 - acc: 0.8757 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.52655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1817 | loss: 0.52655 - acc: 0.8789 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.51990\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1818 | loss: 0.51990 - acc: 0.8818 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.51388\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1819 | loss: 0.51388 - acc: 0.8844 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.50842\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1820 | loss: 0.50842 - acc: 0.8868 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.50347\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1821 | loss: 0.50347 - acc: 0.8889 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.49897\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1822 | loss: 0.49897 - acc: 0.8908 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.49488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1823 | loss: 0.49488 - acc: 0.8925 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.49116\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1824 | loss: 0.49116 - acc: 0.8940 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.48776\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1825 | loss: 0.48776 - acc: 0.8954 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.48467\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1826 | loss: 0.48467 - acc: 0.8954 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.48185\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1827 | loss: 0.48185 - acc: 0.8953 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.47927\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1828 | loss: 0.47927 - acc: 0.8952 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.47691\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1829 | loss: 0.47691 - acc: 0.8952 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.47474\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1830 | loss: 0.47474 - acc: 0.8951 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.47276\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1831 | loss: 0.47276 - acc: 0.8951 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.59090\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1832 | loss: 0.59090 - acc: 0.8372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.57725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1833 | loss: 0.57725 - acc: 0.8429 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.56496\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1834 | loss: 0.56496 - acc: 0.8481 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.55389\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1835 | loss: 0.55389 - acc: 0.8528 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.54390\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1836 | loss: 0.54390 - acc: 0.8570 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.53490\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1837 | loss: 0.53490 - acc: 0.8607 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.52678\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1838 | loss: 0.52678 - acc: 0.8655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.51945\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1839 | loss: 0.51945 - acc: 0.8697 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.51284\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1840 | loss: 0.51284 - acc: 0.8735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.50686\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1841 | loss: 0.50686 - acc: 0.8770 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.50145\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1842 | loss: 0.50145 - acc: 0.8801 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.49656\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1843 | loss: 0.49656 - acc: 0.8828 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.49214\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1844 | loss: 0.49214 - acc: 0.8853 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.48813\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1845 | loss: 0.48813 - acc: 0.8876 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.48449\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1846 | loss: 0.48449 - acc: 0.8896 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.48119\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1847 | loss: 0.48119 - acc: 0.8915 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.62661\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1848 | loss: 0.62661 - acc: 0.8326 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.60908\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1849 | loss: 0.60908 - acc: 0.8401 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.59331\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1850 | loss: 0.59331 - acc: 0.8469 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.57911\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1851 | loss: 0.57911 - acc: 0.8530 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.71452\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1852 | loss: 0.71452 - acc: 0.8006 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.68823\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1853 | loss: 0.68823 - acc: 0.8113 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.66460\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1854 | loss: 0.66460 - acc: 0.8210 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.64336\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1855 | loss: 0.64336 - acc: 0.8297 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.62425\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1856 | loss: 0.62425 - acc: 0.8375 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.60707\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1857 | loss: 0.60707 - acc: 0.8445 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.59161\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1858 | loss: 0.59161 - acc: 0.8509 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.57770\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1859 | loss: 0.57770 - acc: 0.8566 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.56517\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1860 | loss: 0.56517 - acc: 0.8617 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.55389\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1861 | loss: 0.55389 - acc: 0.8663 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.54373\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1862 | loss: 0.54373 - acc: 0.8705 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.53457\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1863 | loss: 0.53457 - acc: 0.8742 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m0.52631\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1864 | loss: 0.52631 - acc: 0.8776 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.51885\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1865 | loss: 0.51885 - acc: 0.8806 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.51212\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1866 | loss: 0.51212 - acc: 0.8833 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.50604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1867 | loss: 0.50604 - acc: 0.8858 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m0.50054\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1868 | loss: 0.50054 - acc: 0.8880 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.49557\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1869 | loss: 0.49557 - acc: 0.8900 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.49107\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1870 | loss: 0.49107 - acc: 0.8918 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.48699\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1871 | loss: 0.48699 - acc: 0.8934 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.48329\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1872 | loss: 0.48329 - acc: 0.8948 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.47993\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1873 | loss: 0.47993 - acc: 0.8962 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.47689\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1874 | loss: 0.47689 - acc: 0.8973 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.47411\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1875 | loss: 0.47411 - acc: 0.8984 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.47159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1876 | loss: 0.47159 - acc: 0.8993 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.46929\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1877 | loss: 0.46929 - acc: 0.9002 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.46720\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1878 | loss: 0.46720 - acc: 0.9010 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.46528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1879 | loss: 0.46528 - acc: 0.9017 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.46353\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1880 | loss: 0.46353 - acc: 0.9010 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.46193\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1881 | loss: 0.46193 - acc: 0.9003 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.46046\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1882 | loss: 0.46046 - acc: 0.8998 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.45911\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1883 | loss: 0.45911 - acc: 0.8993 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.45786\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1884 | loss: 0.45786 - acc: 0.8988 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.45672\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1885 | loss: 0.45672 - acc: 0.8984 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.45566\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1886 | loss: 0.45566 - acc: 0.8980 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.45468\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1887 | loss: 0.45468 - acc: 0.8977 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.45377\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1888 | loss: 0.45377 - acc: 0.8974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.45293\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1889 | loss: 0.45293 - acc: 0.8971 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.45215\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1890 | loss: 0.45215 - acc: 0.8969 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.45142\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1891 | loss: 0.45142 - acc: 0.8967 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m0.45073\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1892 | loss: 0.45073 - acc: 0.8965 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m0.45009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1893 | loss: 0.45009 - acc: 0.8963 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m0.44949\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1894 | loss: 0.44949 - acc: 0.8962 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.44893\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1895 | loss: 0.44893 - acc: 0.8960 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.44839\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1896 | loss: 0.44839 - acc: 0.8959 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.44789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1897 | loss: 0.44789 - acc: 0.8958 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.44741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1898 | loss: 0.44741 - acc: 0.8957 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.44696\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1899 | loss: 0.44696 - acc: 0.8969 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.44654\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1900 | loss: 0.44654 - acc: 0.8980 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.44613\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1901 | loss: 0.44613 - acc: 0.8990 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.44574\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1902 | loss: 0.44574 - acc: 0.8999 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.44537\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1903 | loss: 0.44537 - acc: 0.9007 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.44502\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1904 | loss: 0.44502 - acc: 0.9014 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.44468\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1905 | loss: 0.44468 - acc: 0.9020 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.62832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1906 | loss: 0.62832 - acc: 0.8395 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.60964\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1907 | loss: 0.60964 - acc: 0.8463 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.59283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1908 | loss: 0.59283 - acc: 0.8525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.57770\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1909 | loss: 0.57770 - acc: 0.8580 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.56409\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1910 | loss: 0.56409 - acc: 0.8630 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.55184\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1911 | loss: 0.55184 - acc: 0.8675 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.54082\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1912 | loss: 0.54082 - acc: 0.8715 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.53089\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1913 | loss: 0.53089 - acc: 0.8752 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.52195\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1914 | loss: 0.52195 - acc: 0.8784 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.51390\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1915 | loss: 0.51390 - acc: 0.8814 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.50665\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1916 | loss: 0.50665 - acc: 0.8840 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.50010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1917 | loss: 0.50010 - acc: 0.8864 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m0.49420\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1918 | loss: 0.49420 - acc: 0.8886 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m0.48888\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1919 | loss: 0.48888 - acc: 0.8905 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.48407\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1920 | loss: 0.48407 - acc: 0.8922 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.47973\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1921 | loss: 0.47973 - acc: 0.8938 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.47580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1922 | loss: 0.47580 - acc: 0.8952 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.47225\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1923 | loss: 0.47225 - acc: 0.8976 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.46903\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1924 | loss: 0.46903 - acc: 0.8976 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.46612\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1925 | loss: 0.46612 - acc: 0.8987 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.46348\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1926 | loss: 0.46348 - acc: 0.8996 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.46109\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1927 | loss: 0.46109 - acc: 0.9004 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.45892\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1928 | loss: 0.45892 - acc: 0.9012 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.45695\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1929 | loss: 0.45695 - acc: 0.9018 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m0.45516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1930 | loss: 0.45516 - acc: 0.9024 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m0.45353\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1931 | loss: 0.45353 - acc: 0.9030 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.61902\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1932 | loss: 0.61902 - acc: 0.8456 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.60099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1933 | loss: 0.60099 - acc: 0.8518 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.58477\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1934 | loss: 0.58477 - acc: 0.8574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.57017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1935 | loss: 0.57017 - acc: 0.8625 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.55703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1936 | loss: 0.55703 - acc: 0.8670 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.54521\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1937 | loss: 0.54521 - acc: 0.8711 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.53456\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1938 | loss: 0.53456 - acc: 0.8748 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.52497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1939 | loss: 0.52497 - acc: 0.8781 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.51634\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1940 | loss: 0.51634 - acc: 0.8811 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.50856\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1941 | loss: 0.50856 - acc: 0.8838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.65409\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1942 | loss: 0.65409 - acc: 0.8309 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.63254\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1943 | loss: 0.63254 - acc: 0.8386 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.61316\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1944 | loss: 0.61316 - acc: 0.8455 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.59572\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1945 | loss: 0.59572 - acc: 0.8518 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.58003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1946 | loss: 0.58003 - acc: 0.8574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.56592\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1947 | loss: 0.56592 - acc: 0.8624 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.55322\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1948 | loss: 0.55322 - acc: 0.8670 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.54178\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1949 | loss: 0.54178 - acc: 0.8711 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.53148\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1950 | loss: 0.53148 - acc: 0.8748 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.52220\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1951 | loss: 0.52220 - acc: 0.8781 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.51384\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1952 | loss: 0.51384 - acc: 0.8810 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.50630\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1953 | loss: 0.50630 - acc: 0.8837 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.49950\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1954 | loss: 0.49950 - acc: 0.8862 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.49337\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1955 | loss: 0.49337 - acc: 0.8883 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.48783\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1956 | loss: 0.48783 - acc: 0.8903 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.48283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1957 | loss: 0.48283 - acc: 0.8920 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.47832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1958 | loss: 0.47832 - acc: 0.8936 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.47423\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1959 | loss: 0.47423 - acc: 0.8951 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.62464\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1960 | loss: 0.62464 - acc: 0.8319 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.60591\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1961 | loss: 0.60591 - acc: 0.8395 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.58906\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1962 | loss: 0.58906 - acc: 0.8463 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.57391\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1963 | loss: 0.57391 - acc: 0.8525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.56026\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1964 | loss: 0.56026 - acc: 0.8580 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.54798\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1965 | loss: 0.54798 - acc: 0.8630 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.53693\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1966 | loss: 0.53693 - acc: 0.8675 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.52697\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1967 | loss: 0.52697 - acc: 0.8715 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.51800\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1968 | loss: 0.51800 - acc: 0.8752 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.50991\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1969 | loss: 0.50991 - acc: 0.8784 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.65663\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1970 | loss: 0.65663 - acc: 0.8248 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.63469\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1971 | loss: 0.63469 - acc: 0.8331 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.61495\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1972 | loss: 0.61495 - acc: 0.8406 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.59721\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1973 | loss: 0.59721 - acc: 0.8473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.58124\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1974 | loss: 0.58124 - acc: 0.8534 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.56687\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1975 | loss: 0.56687 - acc: 0.8588 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.55394\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1976 | loss: 0.55394 - acc: 0.8637 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.54230\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1977 | loss: 0.54230 - acc: 0.8682 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.53181\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1978 | loss: 0.53181 - acc: 0.8721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.52237\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1979 | loss: 0.52237 - acc: 0.8757 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.64502\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1980 | loss: 0.64502 - acc: 0.8263 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.62426\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1981 | loss: 0.62426 - acc: 0.8345 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.60559\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1982 | loss: 0.60559 - acc: 0.8418 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.58879\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1983 | loss: 0.58879 - acc: 0.8484 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.57368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1984 | loss: 0.57368 - acc: 0.8544 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.56008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1985 | loss: 0.56008 - acc: 0.8597 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.67700\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1986 | loss: 0.67700 - acc: 0.8066 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.65309\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1987 | loss: 0.65309 - acc: 0.8168 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.63159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1988 | loss: 0.63159 - acc: 0.8272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.61227\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1989 | loss: 0.61227 - acc: 0.8366 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.59488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1990 | loss: 0.59488 - acc: 0.8450 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.57924\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1991 | loss: 0.57924 - acc: 0.8526 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.56516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1992 | loss: 0.56516 - acc: 0.8595 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.55249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1993 | loss: 0.55249 - acc: 0.8656 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.67149\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1994 | loss: 0.67149 - acc: 0.8172 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.64820\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1995 | loss: 0.64820 - acc: 0.8276 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.62726\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1996 | loss: 0.62726 - acc: 0.8369 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.60843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1997 | loss: 0.60843 - acc: 0.8454 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.59149\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1998 | loss: 0.59149 - acc: 0.8529 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.57624\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1999 | loss: 0.57624 - acc: 0.8597 -- iter: 76/76\n",
      "--\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.56251\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2000 | loss: 0.56251 - acc: 0.8659 -- iter: 76/76\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, n_epoch=2000, batch_size=150, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test score [0.98648649454116821]\n"
     ]
    }
   ],
   "source": [
    "#pred = model.predict(test)\n",
    "#pred = pd.DataFrame(pred)\n",
    "score = model.evaluate(test_data, test_labels)\n",
    "print('Training test score', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred\n",
    "#result = []\n",
    "#for i in range(len(pred)):\n",
    "   # if 0[i] > 1[i] and 0[i] > 2[i]:\n",
    "   #     result.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
