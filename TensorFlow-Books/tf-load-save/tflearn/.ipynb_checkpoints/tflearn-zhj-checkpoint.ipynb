{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://tflearn.org/tutorials/quickstart.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.48967\u001b[0m\u001b[0m | time: 0.288s\n",
      "| Adam | epoch: 010 | loss: 0.48967 - acc: 0.7878 -- iter: 1296/1309\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.47422\u001b[0m\u001b[0m | time: 0.291s\n",
      "| Adam | epoch: 010 | loss: 0.47422 - acc: 0.8090 -- iter: 1309/1309\n",
      "--\n",
      "('DiCaprio Surviving Rate:', 0.13264095783233643)\n",
      "('Winslet Surviving Rate:', 0.9119688868522644)\n",
      "INFO:tensorflow:/home/haijunz/tensor-src/zhj/tf-model/zhjmodel_tflean is not in all_model_checkpoint_paths. Manually adding it.\n",
      "model  saving..ss33.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "\n",
    "# Download the Titanic dataset\n",
    "from tflearn.datasets import titanic\n",
    "titanic.download_dataset('titanic_dataset.csv')\n",
    "\n",
    "# Load CSV file, indicate that the first column represents labels\n",
    "from tflearn.data_utils import load_csv\n",
    "data, labels = load_csv('titanic_dataset.csv', target_column=0,\n",
    "                        categorical_labels=True, n_classes=2)\n",
    "# Preprocessing function\n",
    "def preprocess(data, columns_to_ignore):\n",
    "    # Sort by descending id and delete columns\n",
    "    for id in sorted(columns_to_ignore, reverse=True):\n",
    "        [r.pop(id) for r in data]\n",
    "    for i in range(len(data)):\n",
    "      # Converting 'sex' field to float (id is 1 after removing labels column)\n",
    "      data[i][1] = 1. if data[i][1] == 'female' else 0.\n",
    "    return np.array(data, dtype=np.float32)\n",
    "\n",
    "# Ignore 'name' and 'ticket' columns (id 1 & 6 of data array)\n",
    "to_ignore=[1, 6]\n",
    "\n",
    "\n",
    "data = preprocess(data, to_ignore)\n",
    "# Build neural network\n",
    "Adam = tflearn.Adam(learning_rate=0.00001, beta1=0.99,name=\"Adam\")\n",
    "net1 = tflearn.input_data(shape=[None, 6],name=\"v1\")\n",
    "net2 = tflearn.fully_connected(net1, 32,name=\"v2\")\n",
    "net3 = tflearn.fully_connected(net2, 32)\n",
    "net4 = tflearn.fully_connected(net3, 2, activation='softmax')\n",
    "net5 = tflearn.regression(net4)\n",
    "# Define model\n",
    "model = tflearn.DNN(net5)\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(data, labels, n_epoch=10, batch_size=16, show_metric=True)\n",
    "\n",
    "\n",
    "# Let's create some data for DiCaprio and Winslet\n",
    "dicaprio = [3, 'Jack Dawson', 'male', 19, 0, 0, 'N/A', 5.0000]\n",
    "winslet = [1, 'Rose DeWitt Bukater', 'female', 17, 1, 2, 'N/A', 100.0000]\n",
    "# Preprocess data\n",
    "dicaprio, winslet = preprocess([dicaprio, winslet], to_ignore)\n",
    "# Predict surviving chances (class 1 results)\n",
    "pred = model.predict([dicaprio, winslet])\n",
    "print(\"DiCaprio Surviving Rate:\", pred[0][1])\n",
    "print(\"Winslet Surviving Rate:\", pred[1][1])\n",
    "\n",
    "model.save (\"/home/haijunz/tensor-src/zhj/tf-model/zhjmodel_tflean\")\n",
    "\n",
    "print(\"model  saving..ss33.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tflearn/tflearn/blob/master/examples/basics/weights_persistence.py\n",
    "Retrieving a layer variables can either be done using the layer name, or directly by using 'W' or 'b' attributes that are supercharged to the layer's returned Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-561466adcea1>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-561466adcea1>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    model = tflearn.DNN(regression, checkpoint_path='model.tfl.ckpt',) tensorboard_verbose=3\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\" An example showing how to save/restore models and retrieve weights. \"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tflearn\n",
    "\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "# MNIST Data\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "# Model\n",
    "input_layer = tflearn.input_data(shape=[None, 784], name='input')\n",
    "dense1 = tflearn.fully_connected(input_layer, 128, name='dense1')\n",
    "dense2 = tflearn.fully_connected(dense1, 256, name='dense2')\n",
    "softmax = tflearn.fully_connected(dense2, 10, activation='softmax')\n",
    "regression = tflearn.regression(softmax, optimizer='adam',\n",
    "                                learning_rate=0.001,\n",
    "                                loss='categorical_crossentropy')\n",
    "\n",
    "# Define classifier, with model checkpoint (autosave)\n",
    "#model = tflearn.DNN(regression, checkpoint_path='model.tfl.ckpt')\n",
    "model = tflearn.DNN(regression, checkpoint_path='model.tfl.ckpt',tensorboard_verbose=3)\n",
    "\n",
    "# Train model, with model checkpoint every epoch and every 200 training steps.\n",
    "model.fit(X, Y, n_epoch=1,\n",
    "          validation_set=(testX, testY),\n",
    "          show_metric=True,\n",
    "          snapshot_epoch=True, # Snapshot (save & evaluate) model every epoch.\n",
    "          snapshot_step=500, # Snapshot (save & evalaute) model every 500 steps.\n",
    "          run_id='model_and_weights')\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# Save and load a model\n",
    "# ---------------------\n",
    "\n",
    "# Manually save model\n",
    "model.save(\"/tmp/tflearn_logs/model.tfl\")\n",
    "\n",
    "# Load a model\n",
    "model.load(\"/tmp/tflearn_logs/model.tfl\")\n",
    "\n",
    "# Or Load a model from auto-generated checkpoint\n",
    "# >> model.load(\"model.tfl.ckpt-500\")\n",
    "\n",
    "# Resume training\n",
    "model.fit(X, Y, n_epoch=1,\n",
    "          validation_set=(testX, testY),\n",
    "          show_metric=True,\n",
    "          snapshot_epoch=True,\n",
    "          run_id='model_and_weights')\n",
    "\n",
    "\n",
    "# ------------------\n",
    "# Retrieving weights\n",
    "# ------------------\n",
    "\n",
    "# Retrieve a layer weights, by layer name:\n",
    "dense1_vars = tflearn.variables.get_layer_variables_by_name('dense1')\n",
    "# Get a variable's value, using model `get_weights` method:\n",
    "print(\"Dense1 layer weights:\")\n",
    "print(model.get_weights(dense1_vars[0]))\n",
    "# Or using generic tflearn function:\n",
    "print(\"Dense1 layer biases:\")\n",
    "with model.session.as_default():\n",
    "    print(tflearn.variables.get_value(dense1_vars[1]))\n",
    "\n",
    "# It is also possible to retrieve a layer weights through its attributes `W`\n",
    "# and `b` (if available).\n",
    "# Get variable's value, using model `get_weights` method:\n",
    "print(\"Dense2 layer weights:\")\n",
    "print(model.get_weights(dense2.W))\n",
    "# Or using generic tflearn function:\n",
    "print(\"Dense2 layer biases:\")\n",
    "with model.session.as_default():\n",
    "    print(tflearn.variables.get_value(dense2.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py\n",
    "\n",
    "\n",
    "with name scope\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "haijunz@haijunz-ThinkPad-T420:/tmp$ tree tflearn_logs\n",
    "tflearn_logs\n",
    "├── 4GGNOL\n",
    "│   └── events.out.tfevents.1488592726.haijunz-ThinkPad-T420\n",
    "├── 8U95TL\n",
    "│   └── events.out.tfevents.1488588267.haijunz-ThinkPad-T420\n",
    "├── AU2V3U\n",
    "│   └── events.out.tfevents.1488590463.haijunz-ThinkPad-T420\n",
    "├── convnet_mnist\n",
    "│   ├── events.out.tfevents.1488592890.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488592939.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488594534.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488594575.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488594782.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488598277.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488601312.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488601368.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488601457.haijunz-ThinkPad-T420\n",
    "│   └── events.out.tfevents.1488601501.haijunz-ThinkPad-T420\n",
    "├── G01LA5\n",
    "│   └── events.out.tfevents.1488598238.haijunz-ThinkPad-T420\n",
    "├── H2MU13\n",
    "│   └── events.out.tfevents.1488592216.haijunz-ThinkPad-T420\n",
    "├── J1103M\n",
    "│   └── events.out.tfevents.1488594463.haijunz-ThinkPad-T420\n",
    "├── W2GXLT\n",
    "│   └── events.out.tfevents.1488590347.haijunz-ThinkPad-T420\n",
    "├── X4DGVP\n",
    "│   └── events.out.tfevents.1488598162.haijunz-ThinkPad-T420\n",
    "└── Z4980E\n",
    "    └── events.out.tfevents.1488590423.haijunz-ThinkPad-T420\n",
    "\n",
    "10 directories, 19 files\n",
    "haijunz@haijunz-ThinkPad-T420:/tmp$  tensorboard --logdir=/tmp/tflearn_logs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Starting TensorBoard 41 on port 6006\n",
    "(You can navigate to http://127.0.1.1:6006)\n",
    "\n",
    "\n",
    "\n",
    "http://tflearn.org/helpers/trainer/\n",
    "\n",
    "Trainer\n",
    "\n",
    "tflearn.helpers.trainer.Trainer (train_ops, graph=None, clip_gradients=5.0, tensorboard_dir='/tmp/tflearn_logs/', tensorboard_verbose=0, checkpoint_path=None, best_checkpoint_path=None, max_checkpoints=None, keep_checkpoint_every_n_hours=10000.0, random_seed=None, session=None, best_val_accuracy=0.0)\n",
    "\n",
    "http://tflearn.org/getting_started/#training-evaluating-predicting\n",
    "\n",
    "\n",
    "Training, Evaluating & Predicting\n",
    "\n",
    "Training functions are another core feature of TFLearn. In Tensorflow, there are no pre-built API to train a network, so TFLearn integrates a set of functions that can easily handle any neural network training, whatever the number of inputs, outputs and optimizers.\n",
    "\n",
    "While using TFlearn layers, many parameters are already self managed, so it is very easy to train a model, using DNN model class:\n",
    "\n",
    "network = ... (some layers) ...\n",
    "network = regression(network, optimizer='sgd', loss='categorical_crossentropy')\n",
    "\n",
    "model = DNN(network)\n",
    "model.fit(X, Y)\n",
    "It can also directly be called for prediction, or evaluation:\n",
    "\n",
    "network = ...\n",
    "\n",
    "model = DNN(network)\n",
    "model.load('model.tflearn')\n",
    "model.predict(X)\n",
    "To learn more about these wrappers, see: dnn and estimator.\n",
    "Visualization\n",
    "\n",
    "\n",
    "Visualization\n",
    "\n",
    "While writing a Tensorflow model and adding tensorboard summaries isn't very practical, TFLearn has the ability to self managed a lot of useful logs. Currently, TFLearn supports a verbose level to automatically manage summaries:\n",
    "\n",
    "0: Loss & Metric (Best speed).\n",
    "1: Loss, Metric & Gradients.\n",
    "2: Loss, Metric, Gradients & Weights.\n",
    "3: Loss, Metric, Gradients, Weights, Activations & Sparsity (Best Visualization).\n",
    "Using DNN model class, it just requires to specify the verbose argument:\n",
    "\n",
    "model = DNN(network, tensorboard_verbose=3)\n",
    "Then, Tensorboard can be run to visualize network and performance:\n",
    "\n",
    "$ tensorboard --logdir='/tmp/tflearn_logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m1.53392\u001b[0m\u001b[0m | time: 400.068s\n",
      "| Adam | epoch: 001 | loss: 1.53392 - acc: 0.6512 -- iter: 54976/55000\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m1.52995\u001b[0m\u001b[0m | time: 406.094s\n",
      "| Adam | epoch: 001 | loss: 1.52995 - acc: 0.6564 | val_loss: 1.48519 - val_acc: 0.6960 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Convolutional Neural Network for MNIST dataset classification task.\n",
    "References:\n",
    "    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based\n",
    "    learning applied to document recognition.\" Proceedings of the IEEE,\n",
    "    86(11):2278-2324, November 1998.\n",
    "Links:\n",
    "    [MNIST Dataset] http://yann.lecun.com/exdb/mnist/\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "# Data loading and preprocessing\n",
    "import tflearn.datasets.mnist as mnist\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "X = X.reshape([-1, 28, 28, 1])\n",
    "testX = testX.reshape([-1, 28, 28, 1])\n",
    "\n",
    "AdamX= tflearn.Adam(learning_rate=0.00001, beta1=0.99,name=\"AdamX\")\n",
    "# Building convolutional network\n",
    "\n",
    "network = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "network = max_pool_2d(network, 2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 64, 3, activation='relu', regularizer=\"L2\")\n",
    "network = max_pool_2d(network, 2)\n",
    "network = local_response_normalization(network)\n",
    "network = fully_connected(network, 128, activation='tanh')\n",
    "network = dropout(network, 0.8)\n",
    "network = fully_connected(network, 256, activation='tanh')\n",
    "network = dropout(network, 0.8)\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "network = regression(network, optimizer=AdamX, learning_rate=0.01,loss='categorical_crossentropy', name='target')\n",
    "#worked ...network = regression(network, optimizer='adam', learning_rate=0.01, loss='categorical_crossentropy', name='target')\n",
    "# Training\n",
    "model = tflearn.DNN(network, tensorboard_verbose=3)\n",
    "\n",
    "model.save (\"/home/haijunz/tensor-src/zhj/tf-model/zhjmodel_tflean1\")\n",
    "model.save (\"/tmp/tflearn_logs\")\n",
    "print(\"model  saving..ss33.\")\n",
    "\n",
    "model.fit({'input': X}, {'target': Y}, n_epoch=1,\n",
    "           validation_set=({'input': testX}, {'target': testY}),\n",
    "           snapshot_step=100, show_metric=True, run_id='convnet_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow.train.import_meta_graph does not work?\n",
    "\n",
    "http://stackoverflow.com/questions/38829641/tensorflow-train-import-meta-graph-does-not-work\n",
    "\n",
    "To reuse a MetaGraphDef, you will need to record the names of interesting tensors in your original graph. For example, in the first program, set an explicit name argument in the definition of v1, v2 and v4:\n",
    "\n",
    "v1 = tf.placeholder(tf.float32, name=\"v1\")\n",
    "v2 = tf.placeholder(tf.float32, name=\"v2\")\n",
    "# ...\n",
    "v4 = tf.add(v3, c1, name=\"v4\")\n",
    "Then, you can use the string names of the tensors in the original graph in your call to sess.run(). For example, the following snippet should work:\n",
    "\n",
    "import tensorflow as tf\n",
    "_ = tf.train.import_meta_graph(\"./file\")\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(\"v4:0\", feed_dict={\"v1:0\": 12.0, \"v2:0\": 3.3})\n",
    "Alternatively, you can use tf.get_default_graph().get_tensor_by_name() to get tf.Tensor objects for the tensors of interest, which you can then pass to sess.run():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-793f7cc8ca07>:14: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "59.6\n",
      "model_ex1================(method 1) ......============\n",
      "123.3\n",
      "121.0\n",
      "model_ex1==========(method 2)==================\n",
      "241.0\n",
      "model_ex1==========(method 3)==================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "v1 = tf.placeholder(tf.float32, name=\"v1\") \n",
    "v2 = tf.placeholder(tf.float32, name=\"v2\")\n",
    "v3 = tf.multiply(v1, v2)\n",
    "vx = tf.Variable(10.0, name=\"vx\")\n",
    "v4 = tf.add(v3, vx, name=\"v4\")\n",
    "saver = tf.train.Saver([vx])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(vx.assign(tf.add(vx, vx)))\n",
    "result = sess.run(v4, feed_dict={v1:12.0, v2:3.3})\n",
    "print(result)\n",
    "saver.save(sess, \"./model_ex1\")\n",
    "\n",
    "\n",
    "_ = tf.train.import_meta_graph(\"model_ex1.meta\")\n",
    "sess = tf.Session()\n",
    "result = sess.run(\"v4:0\", feed_dict={\"v1:0\": 12.0, \"v2:0\": 10,\"vx:0\": 3.3})\n",
    "print(\"model_ex1================(method 1) ......============\")\n",
    "print(result)\n",
    "\n",
    "_ = tf.train.import_meta_graph(\"./model_ex1.meta\")\n",
    "g = tf.get_default_graph()\n",
    "v1 = g.get_tensor_by_name(\"v1:0\")\n",
    "v2 = g.get_tensor_by_name(\"v2:0\")\n",
    "v4 = g.get_tensor_by_name(\"v4:0\")\n",
    "sess = tf.Session()\n",
    "result = sess.run(v4, feed_dict={v1: 12.0, v2: 10.0,vx:1})\n",
    "print(result)\n",
    "print(\"model_ex1==========(method 2)==================\")\n",
    "saver = tf.train.import_meta_graph(\"./model_ex1.meta\")\n",
    "\n",
    "v1_4 = tf.placeholder(tf.float32, name=\"v1\") \n",
    "sess = tf.Session()\n",
    "saver.restore(sess, \"./model_ex1\")\n",
    "result = sess.run(v4, feed_dict={v1: 24.0, v2: 10.0,vx:1})\n",
    "\n",
    "print(result)\n",
    "print(\"model_ex1==========(method 3)==================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://tflearn.org/getting_started/#extending-tensorflow\n",
    "\n",
    "TFLearn is a very flexible library designed to let you use any of its component independently. A model can be succinctly built using any combination of Tensorflow operations and TFLearn built-in layers and operations. The following instructions will show you the basics for extending Tensorflow with TFLearn\n",
    "\n",
    "https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1835c2a73da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Defining other ops using Tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model  saving...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1576\u001b[0m   \"\"\"\n\u001b[1;32m   1577\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[0;32m-> 1578\u001b[0;31m                     labels, logits)\n\u001b[0m\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m   \u001b[0;31m# TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1533\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1534\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This tutorial will introduce how to combine TFLearn and Tensorflow, using\n",
    "TFLearn trainer with regular Tensorflow graph.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# --------------------------------------\n",
    "# High-Level API: Using TFLearn wrappers\n",
    "# --------------------------------------\n",
    "\n",
    "# Using MNIST Dataset\n",
    "import tflearn.datasets.mnist as mnist\n",
    "mnist_data = mnist.read_data_sets(one_hot=True)\n",
    "\n",
    "# User defined placeholders\n",
    "with tf.Graph().as_default():\n",
    "    # Placeholders for data and labels\n",
    "    X = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=(None, 10), dtype=tf.float32)\n",
    "\n",
    "    net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "    # Using TFLearn wrappers for network building\n",
    "    net = tflearn.conv_2d(net, 32, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 128, activation='tanh')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 256, activation='tanh')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 10, activation='linear')\n",
    "\n",
    "    # Defining other ops using Tensorflow\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "    print(\"model  saving...\")\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        batch_size = 128\n",
    "        for epoch in range(2): # 2 epochs\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist_data.train.num_examples/batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist_data.train.next_batch(batch_size)\n",
    "                sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                cost = sess.run(loss, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                avg_cost += cost/total_batch\n",
    "                if i % 20 == 0:\n",
    "                    print(\"Epoch:\", '%03d' % (epoch+1), \"Step:\", '%03d' % i,\n",
    "                          \"Loss:\", str(cost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
