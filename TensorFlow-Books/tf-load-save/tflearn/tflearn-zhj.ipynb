{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://tflearn.org/tutorials/quickstart.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.48967\u001b[0m\u001b[0m | time: 0.288s\n",
      "| Adam | epoch: 010 | loss: 0.48967 - acc: 0.7878 -- iter: 1296/1309\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.47422\u001b[0m\u001b[0m | time: 0.291s\n",
      "| Adam | epoch: 010 | loss: 0.47422 - acc: 0.8090 -- iter: 1309/1309\n",
      "--\n",
      "('DiCaprio Surviving Rate:', 0.13264095783233643)\n",
      "('Winslet Surviving Rate:', 0.9119688868522644)\n",
      "INFO:tensorflow:/home/haijunz/tensor-src/zhj/tf-model/zhjmodel_tflean is not in all_model_checkpoint_paths. Manually adding it.\n",
      "model  saving..ss33.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "\n",
    "\n",
    "# Download the Titanic dataset\n",
    "from tflearn.datasets import titanic\n",
    "titanic.download_dataset('titanic_dataset.csv')\n",
    "\n",
    "# Load CSV file, indicate that the first column represents labels\n",
    "from tflearn.data_utils import load_csv\n",
    "data, labels = load_csv('titanic_dataset.csv', target_column=0,\n",
    "                        categorical_labels=True, n_classes=2)\n",
    "# Preprocessing function\n",
    "def preprocess(data, columns_to_ignore):\n",
    "    # Sort by descending id and delete columns\n",
    "    for id in sorted(columns_to_ignore, reverse=True):\n",
    "        [r.pop(id) for r in data]\n",
    "    for i in range(len(data)):\n",
    "      # Converting 'sex' field to float (id is 1 after removing labels column)\n",
    "      data[i][1] = 1. if data[i][1] == 'female' else 0.\n",
    "    return np.array(data, dtype=np.float32)\n",
    "\n",
    "# Ignore 'name' and 'ticket' columns (id 1 & 6 of data array)\n",
    "to_ignore=[1, 6]\n",
    "\n",
    "\n",
    "data = preprocess(data, to_ignore)\n",
    "# Build neural network\n",
    "Adam = tflearn.Adam(learning_rate=0.00001, beta1=0.99,name=\"Adam\")\n",
    "net1 = tflearn.input_data(shape=[None, 6],name=\"v1\")\n",
    "net2 = tflearn.fully_connected(net1, 32,name=\"v2\")\n",
    "net3 = tflearn.fully_connected(net2, 32)\n",
    "net4 = tflearn.fully_connected(net3, 2, activation='softmax')\n",
    "net5 = tflearn.regression(net4)\n",
    "# Define model\n",
    "model = tflearn.DNN(net5)\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(data, labels, n_epoch=10, batch_size=16, show_metric=True)\n",
    "\n",
    "\n",
    "# Let's create some data for DiCaprio and Winslet\n",
    "dicaprio = [3, 'Jack Dawson', 'male', 19, 0, 0, 'N/A', 5.0000]\n",
    "winslet = [1, 'Rose DeWitt Bukater', 'female', 17, 1, 2, 'N/A', 100.0000]\n",
    "# Preprocess data\n",
    "dicaprio, winslet = preprocess([dicaprio, winslet], to_ignore)\n",
    "# Predict surviving chances (class 1 results)\n",
    "pred = model.predict([dicaprio, winslet])\n",
    "print(\"DiCaprio Surviving Rate:\", pred[0][1])\n",
    "print(\"Winslet Surviving Rate:\", pred[1][1])\n",
    "\n",
    "model.save (\"/home/haijunz/tensor-src/zhj/tf-model/zhjmodel_tflean\")\n",
    "\n",
    "print(\"model  saving..ss33.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1  | time: 0.093s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 1/1\n",
      "INFO:tensorflow:/tmp/tflearn_logs/model1.tfl is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "input_layer = tflearn.input_data(shape=[None, 784], name='input')\n",
    "dense1 = tflearn.fully_connected(input_layer, 128, name='dense1')\n",
    "dense2 = tflearn.fully_connected(dense1, 256, name='dense2')\n",
    "softmax = tflearn.fully_connected(dense2, 10, activation='softmax',name='softmax')\n",
    "regression = tflearn.regression(softmax, optimizer='adam', learning_rate=0.001,\n",
    "                                loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(regression)\n",
    "X = np.zeros(shape=(1, 784))\n",
    "Y = np.zeros(shape=(1, 10))\n",
    "model.fit(X, Y, n_epoch=1, show_metric=True, snapshot_epoch=False)\n",
    "\n",
    "ops = tf.get_collection_ref(tf.GraphKeys.TRAIN_OPS)  # type: list\n",
    "for op in ops:\n",
    "    ops.remove(op)\n",
    "model.save('/tmp/tflearn_logs/model1.tfl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.36273\u001b[0m\u001b[0m | time: 55.597s\n",
      "| Adam | epoch: 002 | loss: 0.36273 - acc: 0.9039 -- iter: 54976/55000\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.35141\u001b[0m\u001b[0m | time: 56.887s\n",
      "| Adam | epoch: 002 | loss: 0.35141 - acc: 0.9057 | val_loss: 0.31456 - val_acc: 0.9142 -- iter: 55000/55000\n",
      "--\n",
      "INFO:tensorflow:/home/haijunz/tensor-src/zhj/model.tfl.ckpt-1720 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Dense1 layer weights:\n",
      "[[ 0.00260477  0.00589918 -0.00739277 ..., -0.00420203  0.00218264\n",
      "  -0.01234564]\n",
      " [-0.01247121 -0.01702414  0.01382977 ...,  0.0010445   0.00640426\n",
      "   0.00831161]\n",
      " [ 0.0126583   0.02141601 -0.02218018 ..., -0.01822089  0.01026019\n",
      "  -0.00774519]\n",
      " ..., \n",
      " [ 0.02864939  0.01492578  0.01030963 ..., -0.00540991  0.00403144\n",
      "   0.01024558]\n",
      " [-0.00432761  0.0073133  -0.00682526 ..., -0.00604162  0.00715768\n",
      "   0.0119549 ]\n",
      " [ 0.02559429  0.00131289 -0.01171902 ..., -0.00147637  0.01923358\n",
      "  -0.00856409]]\n",
      "Dense1 layer biases:\n",
      "[ -1.73819408e-01   1.22581773e-01   6.59960210e-02   1.02456480e-01\n",
      "   2.72886269e-02   3.26334909e-02  -7.06432909e-02  -3.92496958e-02\n",
      "   1.10869236e-01  -4.60590534e-02   4.13966626e-02  -3.48157063e-03\n",
      "  -1.02215722e-01   6.14373609e-02  -3.53416651e-02   2.64640879e-02\n",
      "   2.76347972e-03  -9.50194057e-03  -2.27159262e-02   8.60542729e-02\n",
      "  -1.14890955e-01  -5.17528690e-03   3.74402143e-02   5.05092144e-02\n",
      "  -9.05700177e-02   1.19666889e-01   3.61370221e-02  -1.34938732e-01\n",
      "   1.44004926e-01   3.64004374e-02  -1.42658085e-01  -4.17440347e-02\n",
      "  -4.82393689e-02   1.08391345e-01   8.30460042e-02   1.01826593e-01\n",
      "  -3.65149719e-03   5.85260130e-02   6.42531738e-02   6.52730763e-02\n",
      "  -9.92528871e-02  -1.95466462e-04  -6.16190173e-02  -1.60114333e-01\n",
      "   8.75839740e-02   7.50527829e-02  -1.00738674e-01  -1.06448770e-01\n",
      "   5.45779243e-02   6.47442937e-02   1.53549835e-01  -5.94414445e-03\n",
      "  -6.65667281e-02   5.81286959e-02   2.02609766e-02   2.00983156e-02\n",
      "   1.24356681e-02  -5.50560430e-02  -1.17384374e-01  -1.62858386e-02\n",
      "   1.02560923e-01   2.26991475e-02  -1.13971643e-01   6.55763149e-02\n",
      "  -1.03713438e-01  -9.71900970e-02   1.00822546e-01   3.22291106e-02\n",
      "   8.08899179e-02  -4.90753576e-02  -9.96953249e-02   9.18226019e-02\n",
      "   1.35505190e-02  -9.46685597e-02   6.49249926e-02   3.91334994e-03\n",
      "  -7.78825581e-02   1.63248293e-02   5.34542389e-02  -7.63652707e-03\n",
      "  -1.93600044e-01  -6.31698873e-04   1.61799993e-02   8.53544008e-03\n",
      "  -7.20819831e-02   6.07129931e-03  -3.61612928e-03  -8.87968466e-02\n",
      "   4.21306863e-02  -1.60977557e-01  -1.27806783e-01   2.18872738e-05\n",
      "   5.70088364e-02  -2.07045302e-01  -9.89502668e-02  -9.77824349e-03\n",
      "   9.94667504e-03  -9.26979631e-03   6.67352006e-02  -1.32477760e-01\n",
      "   2.60399487e-02   9.72798988e-02  -1.93345934e-01  -2.08200747e-03\n",
      "   7.30188042e-02  -1.02716520e-01   9.50485617e-02  -1.66926846e-01\n",
      "   4.98024486e-02   2.65638181e-03  -8.33524987e-02  -1.05541073e-01\n",
      "   8.39036107e-02  -1.06924236e-01   3.90314348e-02   3.03971004e-02\n",
      "   7.12913200e-02   4.23955396e-02   8.54396150e-02  -4.13394645e-02\n",
      "   4.33171280e-02   4.97389510e-02  -9.23052281e-02   6.54235622e-03\n",
      "  -4.54325154e-02  -1.09664455e-01   1.47432640e-01   2.17867360e-04]\n",
      "Dense2 layer weights:\n",
      "[[  1.07395183e-02  -3.24219726e-02  -2.25507803e-02 ...,   2.56219525e-02\n",
      "   -6.03396595e-02   1.72832962e-02]\n",
      " [  5.84010780e-03   5.26049174e-02   1.80347990e-02 ...,  -2.67586131e-02\n",
      "    7.48313516e-02  -9.85941291e-02]\n",
      " [  2.23585982e-02  -5.05856089e-02  -7.05547929e-02 ...,  -2.76055504e-02\n",
      "   -2.18627602e-02  -9.54960473e-03]\n",
      " ..., \n",
      " [ -1.80849116e-02   4.61346842e-02   4.75276560e-02 ...,   6.85200319e-02\n",
      "   -1.32638634e-05   3.15409526e-02]\n",
      " [  4.03395630e-02   2.31609829e-02   1.72615945e-02 ...,  -3.39114293e-02\n",
      "    2.07077004e-02  -2.02386118e-02]\n",
      " [  2.55078413e-02   1.59047712e-02   5.44197969e-02 ...,   2.02532224e-02\n",
      "    5.81481755e-02  -4.15539294e-02]]\n",
      "Dense2 layer biases:\n",
      "[-0.0002007   0.06280165 -0.04851242  0.06066518  0.08050092 -0.15934682\n",
      "  0.02849785  0.04345194  0.11546025  0.00639303 -0.13526401 -0.07749617\n",
      " -0.09883957  0.01647855 -0.04047817 -0.09949315 -0.0516211   0.10847908\n",
      "  0.01514685 -0.02760754  0.00517433 -0.12615116 -0.12779197  0.0607442\n",
      " -0.13008617 -0.0209207  -0.07149322  0.11558574 -0.00136631 -0.09673402\n",
      "  0.10482711  0.0430733  -0.14770721 -0.09305394 -0.06435224  0.0371596\n",
      " -0.00640574 -0.05264146  0.01254688  0.08577085  0.01101691 -0.14896774\n",
      " -0.02487117 -0.01441111 -0.06415509  0.12398223  0.02910708 -0.1082523\n",
      " -0.09417546 -0.0148373   0.05270642 -0.03359489 -0.15553604  0.04583989\n",
      "  0.07336975 -0.09011569  0.03998764  0.01747212  0.04222385 -0.03729252\n",
      "  0.065304   -0.02635865  0.06416478  0.02520413 -0.01745772  0.06169457\n",
      " -0.1235026  -0.02173452 -0.01290203  0.05620508 -0.05259641  0.09187302\n",
      " -0.04815967 -0.00155632  0.09850296 -0.04363873 -0.0392747  -0.03585199\n",
      " -0.00509932  0.02407484 -0.15822077  0.14147405 -0.09600522 -0.12561077\n",
      "  0.01612101  0.10054232 -0.08686981 -0.02810197 -0.06767785 -0.06611697\n",
      "  0.12832746 -0.00770847  0.05650343 -0.13455653 -0.01098701  0.11069937\n",
      " -0.06012163  0.00639592  0.03383162  0.04420165 -0.02823515 -0.03975683\n",
      "  0.04279963  0.06620537 -0.02811157  0.09333748  0.04724719 -0.08322628\n",
      "  0.02546249 -0.12423323 -0.14500587 -0.11098263 -0.02230629 -0.18621151\n",
      "  0.04595372 -0.04574364  0.0374244   0.02957573  0.08080948  0.07002035\n",
      " -0.01901996 -0.04225941  0.17483363 -0.18231177 -0.0910124   0.11181395\n",
      "  0.05993243  0.0224224   0.04699817  0.0054821   0.05631071 -0.1192594\n",
      " -0.04661467  0.13317627  0.05701903 -0.02838997  0.13870263 -0.06404119\n",
      "  0.04398053 -0.05211924 -0.05244466 -0.1228915  -0.07417437 -0.01473878\n",
      "  0.10446741 -0.00794759 -0.00665482 -0.08039565 -0.10529328  0.06023184\n",
      " -0.05655758  0.04546679 -0.06097468 -0.0181249  -0.00105165  0.00609408\n",
      " -0.09706595  0.00210579  0.03114178  0.05310233 -0.15311314  0.06630752\n",
      " -0.02762676  0.02960283 -0.03470477  0.04272551  0.00412395 -0.07696679\n",
      " -0.05202487  0.12205126 -0.03039339 -0.03549955  0.10447395 -0.08374378\n",
      "  0.06118226  0.12882082 -0.01952665  0.01951087 -0.06484289 -0.11804891\n",
      "  0.03615822 -0.00349296 -0.10308988  0.0749336   0.02981136 -0.09589282\n",
      "  0.01914669 -0.00761076  0.1230444   0.05867056  0.07223302 -0.14849851\n",
      " -0.17250918 -0.0541222  -0.06740507  0.09185014 -0.0155649   0.10195819\n",
      " -0.02210399 -0.09311595 -0.0456811   0.17751482 -0.08798438  0.08059599\n",
      "  0.09634968 -0.04669292 -0.10605896  0.0173353  -0.0965186  -0.07677795\n",
      "  0.07051033  0.06112859  0.00807946  0.0296541   0.06761622 -0.12715235\n",
      " -0.09302141 -0.07073991  0.04378112 -0.12705117  0.11169105  0.00588306\n",
      "  0.01796003  0.06137914 -0.01844424  0.04473692 -0.0461211   0.10987238\n",
      " -0.15715775  0.05435991 -0.01787731  0.01730877  0.1667338   0.07009371\n",
      " -0.09939811 -0.12513822 -0.05020103  0.02402473 -0.06446017 -0.17960583\n",
      "  0.05610861  0.06404071  0.01119135  0.1108542  -0.04216907  0.02461663\n",
      "  0.07532883  0.04788625 -0.17733456 -0.07559474  0.06529931  0.02992324\n",
      " -0.0266142  -0.1144816   0.10570075 -0.1339507 ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" An example showing how to save/restore models and retrieve weights. \"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "# MNIST Data\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "input_layer = tflearn.input_data(shape=[None, 784], name='input')\n",
    "dense1 = tflearn.fully_connected(input_layer, 128, name='dense1')\n",
    "dense2 = tflearn.fully_connected(dense1, 256, name='dense2')\n",
    "softmax = tflearn.fully_connected(dense2, 10, activation='softmax',name='softmax')\n",
    "regression = tflearn.regression(softmax, optimizer='adam',\n",
    "                                learning_rate=0.001,\n",
    "                                loss='categorical_crossentropy')\n",
    "\n",
    "# Define classifier, with model checkpoint (autosave)\n",
    "#model = tflearn.DNN(regression, checkpoint_path='model.tfl.ckpt')\n",
    "model = tflearn.DNN(regression, checkpoint_path='model.tfl.ckpt',tensorboard_verbose=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train model, with model checkpoint every epoch and every 200 training steps.\n",
    "model.fit(X, Y, n_epoch=1,\n",
    "          validation_set=(testX, testY),\n",
    "          show_metric=True,\n",
    "          snapshot_epoch=True, # Snapshot (save & evaluate) model every epoch.\n",
    "          snapshot_step=500, # Snapshot (save & evalaute) model every 500 steps.\n",
    "          run_id='model_and_weights')\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# Save and load a model\n",
    "# ---------------------\n",
    "ops = tf.get_collection_ref(tf.GraphKeys.TRAIN_OPS)  # type: list\n",
    "for op in ops:\n",
    "    ops.remove(op)\n",
    "# Manually save model\n",
    "model.save(\"/tmp/tflearn_logs/model.tfl\")\n",
    "\n",
    "# Load a model\n",
    "model.load(\"/tmp/tflearn_logs/model.tfl\")\n",
    "\n",
    "# Or Load a model from auto-generated checkpoint\n",
    "# >> model.load(\"model.tfl.ckpt-500\")\n",
    "\n",
    "# Resume training\n",
    "model.fit(X, Y, n_epoch=1,\n",
    "          validation_set=(testX, testY),\n",
    "          show_metric=True,\n",
    "          snapshot_epoch=True,\n",
    "          run_id='model_and_weights')\n",
    "\n",
    "\n",
    "# ------------------\n",
    "# Retrieving weights\n",
    "# ------------------\n",
    "\n",
    "# Retrieve a layer weights, by layer name:\n",
    "dense1_vars = tflearn.variables.get_layer_variables_by_name('dense1')\n",
    "# Get a variable's value, using model `get_weights` method:\n",
    "print(\"Dense1 layer weights:\")\n",
    "print(model.get_weights(dense1_vars[0]))\n",
    "# Or using generic tflearn function:\n",
    "print(\"Dense1 layer biases:\")\n",
    "with model.session.as_default():\n",
    "    print(tflearn.variables.get_value(dense1_vars[1]))\n",
    "\n",
    "# It is also possible to retrieve a layer weights through its attributes `W`\n",
    "# and `b` (if available).\n",
    "# Get variable's value, using model `get_weights` method:\n",
    "print(\"Dense2 layer weights:\")\n",
    "print(model.get_weights(dense2.W))\n",
    "# Or using generic tflearn function:\n",
    "print(\"Dense2 layer biases:\")\n",
    "with model.session.as_default():\n",
    "    print(tflearn.variables.get_value(dense2.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tflearn/tflearn/blob/master/examples/basics/weights_persistence.py\n",
    "Retrieving a layer variables can either be done using the layer name, or directly by using 'W' or 'b' attributes that are supercharged to the layer's returned Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py\n",
    "\n",
    "\n",
    "with name scope\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "haijunz@haijunz-ThinkPad-T420:/tmp$ tree tflearn_logs\n",
    "tflearn_logs\n",
    "├── 4GGNOL\n",
    "│   └── events.out.tfevents.1488592726.haijunz-ThinkPad-T420\n",
    "├── 8U95TL\n",
    "│   └── events.out.tfevents.1488588267.haijunz-ThinkPad-T420\n",
    "├── AU2V3U\n",
    "│   └── events.out.tfevents.1488590463.haijunz-ThinkPad-T420\n",
    "├── convnet_mnist\n",
    "│   ├── events.out.tfevents.1488592890.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488592939.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488594534.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488594575.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488594782.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488598277.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488601312.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488601368.haijunz-ThinkPad-T420\n",
    "│   ├── events.out.tfevents.1488601457.haijunz-ThinkPad-T420\n",
    "│   └── events.out.tfevents.1488601501.haijunz-ThinkPad-T420\n",
    "├── G01LA5\n",
    "│   └── events.out.tfevents.1488598238.haijunz-ThinkPad-T420\n",
    "├── H2MU13\n",
    "│   └── events.out.tfevents.1488592216.haijunz-ThinkPad-T420\n",
    "├── J1103M\n",
    "│   └── events.out.tfevents.1488594463.haijunz-ThinkPad-T420\n",
    "├── W2GXLT\n",
    "│   └── events.out.tfevents.1488590347.haijunz-ThinkPad-T420\n",
    "├── X4DGVP\n",
    "│   └── events.out.tfevents.1488598162.haijunz-ThinkPad-T420\n",
    "└── Z4980E\n",
    "    └── events.out.tfevents.1488590423.haijunz-ThinkPad-T420\n",
    "\n",
    "10 directories, 19 files\n",
    "haijunz@haijunz-ThinkPad-T420:/tmp$  tensorboard --logdir=/tmp/tflearn_logs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Starting TensorBoard 41 on port 6006\n",
    "(You can navigate to http://127.0.1.1:6006)\n",
    "\n",
    "\n",
    "\n",
    "http://tflearn.org/helpers/trainer/\n",
    "\n",
    "Trainer\n",
    "\n",
    "tflearn.helpers.trainer.Trainer (train_ops, graph=None, clip_gradients=5.0, tensorboard_dir='/tmp/tflearn_logs/', tensorboard_verbose=0, checkpoint_path=None, best_checkpoint_path=None, max_checkpoints=None, keep_checkpoint_every_n_hours=10000.0, random_seed=None, session=None, best_val_accuracy=0.0)\n",
    "\n",
    "http://tflearn.org/getting_started/#training-evaluating-predicting\n",
    "\n",
    "\n",
    "Training, Evaluating & Predicting\n",
    "\n",
    "Training functions are another core feature of TFLearn. In Tensorflow, there are no pre-built API to train a network, so TFLearn integrates a set of functions that can easily handle any neural network training, whatever the number of inputs, outputs and optimizers.\n",
    "\n",
    "While using TFlearn layers, many parameters are already self managed, so it is very easy to train a model, using DNN model class:\n",
    "\n",
    "network = ... (some layers) ...\n",
    "network = regression(network, optimizer='sgd', loss='categorical_crossentropy')\n",
    "\n",
    "model = DNN(network)\n",
    "model.fit(X, Y)\n",
    "It can also directly be called for prediction, or evaluation:\n",
    "\n",
    "network = ...\n",
    "\n",
    "model = DNN(network)\n",
    "model.load('model.tflearn')\n",
    "model.predict(X)\n",
    "To learn more about these wrappers, see: dnn and estimator.\n",
    "Visualization\n",
    "\n",
    "\n",
    "Visualization\n",
    "\n",
    "While writing a Tensorflow model and adding tensorboard summaries isn't very practical, TFLearn has the ability to self managed a lot of useful logs. Currently, TFLearn supports a verbose level to automatically manage summaries:\n",
    "\n",
    "0: Loss & Metric (Best speed).\n",
    "1: Loss, Metric & Gradients.\n",
    "2: Loss, Metric, Gradients & Weights.\n",
    "3: Loss, Metric, Gradients, Weights, Activations & Sparsity (Best Visualization).\n",
    "Using DNN model class, it just requires to specify the verbose argument:\n",
    "\n",
    "model = DNN(network, tensorboard_verbose=3)\n",
    "Then, Tensorboard can be run to visualize network and performance:\n",
    "\n",
    "$ tensorboard --logdir='/tmp/tflearn_logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m1.53392\u001b[0m\u001b[0m | time: 400.068s\n",
      "| Adam | epoch: 001 | loss: 1.53392 - acc: 0.6512 -- iter: 54976/55000\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m1.52995\u001b[0m\u001b[0m | time: 406.094s\n",
      "| Adam | epoch: 001 | loss: 1.52995 - acc: 0.6564 | val_loss: 1.48519 - val_acc: 0.6960 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Convolutional Neural Network for MNIST dataset classification task.\n",
    "References:\n",
    "    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based\n",
    "    learning applied to document recognition.\" Proceedings of the IEEE,\n",
    "    86(11):2278-2324, November 1998.\n",
    "Links:\n",
    "    [MNIST Dataset] http://yann.lecun.com/exdb/mnist/\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "# Data loading and preprocessing\n",
    "import tflearn.datasets.mnist as mnist\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "X = X.reshape([-1, 28, 28, 1])\n",
    "testX = testX.reshape([-1, 28, 28, 1])\n",
    "\n",
    "AdamX= tflearn.Adam(learning_rate=0.00001, beta1=0.99,name=\"AdamX\")\n",
    "# Building convolutional network\n",
    "\n",
    "network = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "network = max_pool_2d(network, 2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 64, 3, activation='relu', regularizer=\"L2\")\n",
    "network = max_pool_2d(network, 2)\n",
    "network = local_response_normalization(network)\n",
    "network = fully_connected(network, 128, activation='tanh')\n",
    "network = dropout(network, 0.8)\n",
    "network = fully_connected(network, 256, activation='tanh')\n",
    "network = dropout(network, 0.8)\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "network = regression(network, optimizer=AdamX, learning_rate=0.01,loss='categorical_crossentropy', name='target')\n",
    "#worked ...network = regression(network, optimizer='adam', learning_rate=0.01, loss='categorical_crossentropy', name='target')\n",
    "# Training\n",
    "model = tflearn.DNN(network, tensorboard_verbose=3)\n",
    "\n",
    "model.save (\"/home/haijunz/tensor-src/zhj/tf-model/zhjmodel_tflean1\")\n",
    "model.save (\"/tmp/tflearn_logs\")\n",
    "print(\"model  saving..ss33.\")\n",
    "\n",
    "model.fit({'input': X}, {'target': Y}, n_epoch=1,\n",
    "           validation_set=({'input': testX}, {'target': testY}),\n",
    "           snapshot_step=100, show_metric=True, run_id='convnet_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow.train.import_meta_graph does not work?\n",
    "\n",
    "http://stackoverflow.com/questions/38829641/tensorflow-train-import-meta-graph-does-not-work\n",
    "\n",
    "To reuse a MetaGraphDef, you will need to record the names of interesting tensors in your original graph. For example, in the first program, set an explicit name argument in the definition of v1, v2 and v4:\n",
    "\n",
    "v1 = tf.placeholder(tf.float32, name=\"v1\")\n",
    "v2 = tf.placeholder(tf.float32, name=\"v2\")\n",
    "# ...\n",
    "v4 = tf.add(v3, c1, name=\"v4\")\n",
    "Then, you can use the string names of the tensors in the original graph in your call to sess.run(). For example, the following snippet should work:\n",
    "\n",
    "import tensorflow as tf\n",
    "_ = tf.train.import_meta_graph(\"./file\")\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(\"v4:0\", feed_dict={\"v1:0\": 12.0, \"v2:0\": 3.3})\n",
    "Alternatively, you can use tf.get_default_graph().get_tensor_by_name() to get tf.Tensor objects for the tensors of interest, which you can then pass to sess.run():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-793f7cc8ca07>:14: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "59.6\n",
      "model_ex1================(method 1) ......============\n",
      "123.3\n",
      "121.0\n",
      "model_ex1==========(method 2)==================\n",
      "241.0\n",
      "model_ex1==========(method 3)==================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "v1 = tf.placeholder(tf.float32, name=\"v1\") \n",
    "v2 = tf.placeholder(tf.float32, name=\"v2\")\n",
    "v3 = tf.multiply(v1, v2)\n",
    "vx = tf.Variable(10.0, name=\"vx\")\n",
    "v4 = tf.add(v3, vx, name=\"v4\")\n",
    "saver = tf.train.Saver([vx])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(vx.assign(tf.add(vx, vx)))\n",
    "result = sess.run(v4, feed_dict={v1:12.0, v2:3.3})\n",
    "print(result)\n",
    "saver.save(sess, \"./model_ex1\")\n",
    "\n",
    "\n",
    "_ = tf.train.import_meta_graph(\"model_ex1.meta\")\n",
    "sess = tf.Session()\n",
    "result = sess.run(\"v4:0\", feed_dict={\"v1:0\": 12.0, \"v2:0\": 10,\"vx:0\": 3.3})\n",
    "print(\"model_ex1================(method 1) ......============\")\n",
    "print(result)\n",
    "\n",
    "_ = tf.train.import_meta_graph(\"./model_ex1.meta\")\n",
    "g = tf.get_default_graph()\n",
    "v1 = g.get_tensor_by_name(\"v1:0\")\n",
    "v2 = g.get_tensor_by_name(\"v2:0\")\n",
    "v4 = g.get_tensor_by_name(\"v4:0\")\n",
    "sess = tf.Session()\n",
    "result = sess.run(v4, feed_dict={v1: 12.0, v2: 10.0,vx:1})\n",
    "print(result)\n",
    "print(\"model_ex1==========(method 2)==================\")\n",
    "saver = tf.train.import_meta_graph(\"./model_ex1.meta\")\n",
    "\n",
    "v1_4 = tf.placeholder(tf.float32, name=\"v1\") \n",
    "sess = tf.Session()\n",
    "saver.restore(sess, \"./model_ex1\")\n",
    "result = sess.run(v4, feed_dict={v1: 24.0, v2: 10.0,vx:1})\n",
    "\n",
    "print(result)\n",
    "print(\"model_ex1==========(method 3)==================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending TensorFlow\n",
    "\n",
    "http://tflearn.org/examples/\n",
    "Layers. Use TFLearn layers along with TensorFlow.\n",
    "Trainer. Use TFLearn trainer class to train any TensorFlow graph.\n",
    "Built-in Ops. Use TFLearn built-in operations along with TensorFlow.\n",
    "Summaries. Use TFLearn summarizers along with TensorFlow.\n",
    "Variables. Use TFLearn variables along with TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2fa2216ef7a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Defining other ops using Tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1576\u001b[0m   \"\"\"\n\u001b[1;32m   1577\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[0;32m-> 1578\u001b[0;31m                     labels, logits)\n\u001b[0m\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m   \u001b[0;31m# TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1533\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1534\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "#https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/layers.py\n",
    "\"\"\"\n",
    "This tutorial will introduce how to combine TFLearn and Tensorflow, using\n",
    "TFLearn trainer with regular Tensorflow graph.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# --------------------------------------\n",
    "# High-Level API: Using TFLearn wrappers\n",
    "# --------------------------------------\n",
    "\n",
    "# Using MNIST Dataset\n",
    "import tflearn.datasets.mnist as mnist\n",
    "mnist_data = mnist.read_data_sets(one_hot=True)\n",
    "\n",
    "# User defined placeholders\n",
    "with tf.Graph().as_default():\n",
    "    # Placeholders for data and labels\n",
    "    X = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=(None, 10), dtype=tf.float32)\n",
    "\n",
    "    net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "    # Using TFLearn wrappers for network building\n",
    "    net = tflearn.conv_2d(net, 32, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 128, activation='tanh')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 256, activation='tanh')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 10, activation='linear')\n",
    "\n",
    "    # Defining other ops using Tensorflow\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        batch_size = 128\n",
    "        for epoch in range(2): # 2 epochs\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist_data.train.num_examples/batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist_data.train.next_batch(batch_size)\n",
    "                sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                cost = sess.run(loss, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                avg_cost += cost/total_batch\n",
    "                if i % 20 == 0:\n",
    "                    print(\"Epoch:\", '%03d' % (epoch+1), \"Step:\", '%03d' % i,\n",
    "                          \"Loss:\", str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec2c3bd46718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     accuracy = tf.reduce_mean(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1576\u001b[0m   \"\"\"\n\u001b[1;32m   1577\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[0;32m-> 1578\u001b[0;31m                     labels, logits)\n\u001b[0m\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m   \u001b[0;31m# TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1533\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1534\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "#https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/trainer.py\n",
    "\n",
    "\"\"\"\n",
    "This tutorial will introduce how to combine TFLearn and Tensorflow, using\n",
    "TFLearn wrappers regular Tensorflow expressions.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# ----------------------------\n",
    "# Utils: Using TFLearn Trainer\n",
    "# ----------------------------\n",
    "\n",
    "# Loading MNIST complete dataset\n",
    "import tflearn.datasets.mnist as mnist\n",
    "trainX, trainY, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "# Define a dnn using Tensorflow\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Model variables\n",
    "    X = tf.placeholder(\"float\", [None, 784])\n",
    "    Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "    W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "    W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "    W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "    b1 = tf.Variable(tf.random_normal([256]))\n",
    "    b2 = tf.Variable(tf.random_normal([256]))\n",
    "    b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "    # Multilayer perceptron\n",
    "    def dnn(x):\n",
    "        x = tf.nn.tanh(tf.add(tf.matmul(x, W1), b1))\n",
    "        x = tf.nn.tanh(tf.add(tf.matmul(x, W2), b2))\n",
    "        x = tf.add(tf.matmul(x, W3), b3)\n",
    "        return x\n",
    "\n",
    "    net = dnn(X)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1)), tf.float32),\n",
    "        name='acc')\n",
    "\n",
    "    # Using TFLearn Trainer\n",
    "    # Define a training op (op for backprop, only need 1 in this model)\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                              metric=accuracy, batch_size=128)\n",
    "\n",
    "    # Create Trainer, providing all training ops. Tensorboard logs stored\n",
    "    # in /tmp/tflearn_logs/. It is possible to change verbose level for more\n",
    "    # details logs about gradients, variables etc...\n",
    "    trainer = tflearn.Trainer(train_ops=trainop, tensorboard_verbose=0)\n",
    "    # Training for 10 epochs.\n",
    "    trainer.fit({X: trainX, Y: trainY}, val_feed_dicts={X: testX, Y: testY},\n",
    "                n_epoch=10, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4299  | total loss: \u001b[1m\u001b[32m10.23656\u001b[0m\u001b[0m | time: 10.437s\n",
      "| Optimizer | epoch: 010 | loss: 10.23656 - Accuracy/Mean: 0.5553 -- iter: 54912/55000\n",
      "Training Step: 4300  | total loss: \u001b[1m\u001b[32m10.36420\u001b[0m\u001b[0m | time: 11.621s\n",
      "| Optimizer | epoch: 010 | loss: 10.36420 - Accuracy/Mean: 0.5498 | val_loss: 10.20576 - val_acc: 0.5558 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/builtin_ops.py\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "\"\"\"\n",
    "This tutorial will introduce how to combine TFLearn built-in ops with any\n",
    "Tensorflow graph.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# ----------------------------------\n",
    "# Using TFLearn built-in ops example\n",
    "# ----------------------------------\n",
    "\n",
    "# Using MNIST Dataset\n",
    "import tflearn.datasets.mnist as mnist\n",
    "trainX, trainY, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "# User defined placeholders\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Model variables\n",
    "    X = tf.placeholder(\"float\", [None, 784])\n",
    "    Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "    W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "    W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "    W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "    b1 = tf.Variable(tf.random_normal([256]))\n",
    "    b2 = tf.Variable(tf.random_normal([256]))\n",
    "    b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "    # Multilayer perceptron\n",
    "    def dnn(x):\n",
    "        # Using TFLearn PReLU activations ops\n",
    "        x = tflearn.prelu(tf.add(tf.matmul(x, W1), b1))\n",
    "        tflearn.summaries.monitor_activation(x) # Monitor activation\n",
    "        x = tflearn.prelu(tf.add(tf.matmul(x, W2), b2))\n",
    "        tflearn.summaries.monitor_activation(x) # Monitor activation\n",
    "        x = tf.nn.softmax(tf.add(tf.matmul(x, W3), b3))\n",
    "        return x\n",
    "\n",
    "    net = dnn(X)\n",
    "\n",
    "    # Using objective ops from TFLearn to compute crossentropy\n",
    "    loss = tflearn.categorical_crossentropy(net, Y)\n",
    "\n",
    "    # Using metric ops from TFLearn to compute accuracy\n",
    "    acc = tflearn.metrics.accuracy_op(net, Y)\n",
    "\n",
    "    # Using TFLearn SGD Optimizer class\n",
    "    optimizer = tflearn.SGD(learning_rate=0.1, lr_decay=0.96, decay_step=200)\n",
    "    # Because of lr decay, it is required to first build the Optimizer with\n",
    "    # the step tensor that will monitor training step.\n",
    "    # (Note: When using TFLearn estimators wrapper, build is self managed,\n",
    "    # so only using above `Optimizer` class as `DNN` optimizer arg is enough).\n",
    "    step = tflearn.variable(\"step\", initializer='zeros', shape=[])\n",
    "    optimizer.build(step_tensor=step)\n",
    "    optim_tensor = optimizer.get_tensor()\n",
    "\n",
    "    # Using TFLearn Trainer\n",
    "    # Define a training op (op for backprop, only need 1 in this model)\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optim_tensor,\n",
    "                              metric=acc, batch_size=128,\n",
    "                              step_tensor=step)\n",
    "\n",
    "    # Create Trainer, providing all training ops. Tensorboard logs stored\n",
    "    # in /tmp/tflearn_logs/. It is possible to change verbose level for more\n",
    "    # details logs about gradients, variables etc...\n",
    "    trainer = tflearn.Trainer(train_ops=trainop, tensorboard_verbose=0)\n",
    "    # Training for 10 epochs.\n",
    "    trainer.fit({X: trainX, Y: trainY}, val_feed_dicts={X: testX, Y: testY},\n",
    "                n_epoch=10, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7f4d20a69f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     accuracy = tf.reduce_mean(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1576\u001b[0m   \"\"\"\n\u001b[1;32m   1577\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[0;32m-> 1578\u001b[0;31m                     labels, logits)\n\u001b[0m\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m   \u001b[0;31m# TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1533\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1534\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "#https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/summaries.py\n",
    "\n",
    "\"\"\"\n",
    "This example introduces the use of TFLearn functions to easily summarize\n",
    "variables into tensorboard.\n",
    "TFLearn can summarize:\n",
    "- Loss / Accuracy: The model loss and accuracy over training steps.\n",
    "- Activations: Histogram of operation output values.(Requires to add each\n",
    "    activation to monitor into tf.Graphkeys.ACTIVATIONS collection).\n",
    "- Gradients: Histogram of trainable variables gradient.\n",
    "- Weights: Histogram of trainable variables weights.\n",
    "- Weight Decay: Decay of trainable variables with regularizer. (Requires\n",
    "    to add each decay into tf.Graphkeys.REGULARIZATION_LOSSES collection)\n",
    "- Sparsity: Sparsity of trainable variables.\n",
    "It is useful to also be able to periodically monitor various variables\n",
    "during training, e.g. confusion matrix entries or AUC metrics. This\n",
    "can be done using \"validation_monitors\", an argument to regression or\n",
    "TrainOp; this argument takes a list of Tensor variables, and passes\n",
    "them to the trainer, where they are evaluated each time a validation\n",
    "step happens. The evaluation results are then summarized, and saved\n",
    "for tensorboard visualization.\n",
    "Summaries are monitored according to the following verbose levels:\n",
    "- 0: Loss & Metric (Best speed).\n",
    "- 1: Loss, Metric & Gradients.\n",
    "- 2: Loss, Metric, Gradients & Weights.\n",
    "- 3: Loss, Metric, Gradients, Weights, Activations & Sparsity (Best\n",
    "     Visualization).\n",
    "Note: If you are using TFLearn layers, summaries are automatically handled,\n",
    "so you do not need to manually add them.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# Loading MNIST dataset\n",
    "import tflearn.datasets.mnist as mnist\n",
    "trainX, trainY, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "# Define a dnn using Tensorflow\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Model variables\n",
    "    X = tf.placeholder(\"float\", [None, 784])\n",
    "    Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "    # Multilayer perceptron, with `tanh` functions activation monitor\n",
    "    def dnn(x):\n",
    "        with tf.name_scope('Layer1'):\n",
    "            W1 = tf.Variable(tf.random_normal([784, 256]), name=\"W1\")\n",
    "            b1 = tf.Variable(tf.random_normal([256]), name=\"b1\")\n",
    "            x = tf.nn.tanh(tf.add(tf.matmul(x, W1), b1))\n",
    "            # Add this `tanh` op to activations collection or monitoring\n",
    "            tf.add_to_collection(tf.GraphKeys.ACTIVATIONS, x)\n",
    "            # Add weights regularizer (Regul. summary automatically added)\n",
    "            tflearn.add_weights_regularizer(W1, 'L2', weight_decay=0.001)\n",
    "\n",
    "        with tf.name_scope('Layer2'):\n",
    "            W2 = tf.Variable(tf.random_normal([256, 256]), name=\"W2\")\n",
    "            b2 = tf.Variable(tf.random_normal([256]), name=\"b2\")\n",
    "            x = tf.nn.tanh(tf.add(tf.matmul(x, W2), b2))\n",
    "            # Add this `tanh` op to activations collection or monitoring\n",
    "            tf.add_to_collection(tf.GraphKeys.ACTIVATIONS, x)\n",
    "            # Add weights regularizer (Regul. summary automatically added)\n",
    "            tflearn.add_weights_regularizer(W2, 'L2', weight_decay=0.001)\n",
    "\n",
    "        with tf.name_scope('Layer3'):\n",
    "            W3 = tf.Variable(tf.random_normal([256, 10]), name=\"W3\")\n",
    "            b3 = tf.Variable(tf.random_normal([10]), name=\"b3\")\n",
    "            x = tf.add(tf.matmul(x, W3), b3)\n",
    "\n",
    "        return x\n",
    "\n",
    "    net = dnn(X)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1)), tf.float32),\n",
    "        name=\"acc\")\n",
    "\n",
    "    # construct two varaibles to add as additional \"valiation monitors\"\n",
    "    # these varaibles are evaluated each time validation happens (eg at a snapshot)\n",
    "    # and the results are summarized and output to the tensorboard events file,\n",
    "    # together with the accuracy and loss plots.\n",
    "    #\n",
    "    # Here, we generate a dummy variable given by the sum over the current\n",
    "    # network tensor, and a constant variable.  In practice, the validation\n",
    "    # monitor may present useful information, like confusion matrix\n",
    "    # entries, or an AUC metric.\n",
    "    with tf.name_scope('CustomMonitor'):\n",
    "        test_var = tf.reduce_sum(tf.cast(net, tf.float32), name=\"test_var\")\n",
    "        test_const = tf.constant(32.0, name=\"custom_constant\")\n",
    "\n",
    "    # Define a train op\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                              validation_monitors=[test_var, test_const],\n",
    "                              metric=accuracy, batch_size=128)\n",
    "\n",
    "    # Tensorboard logs stored in /tmp/tflearn_logs/. Using verbose level 2.\n",
    "    trainer = tflearn.Trainer(train_ops=trainop,\n",
    "                              tensorboard_dir='/tmp/tflearn_logs/',\n",
    "                              tensorboard_verbose=2)\n",
    "    # Training for 10 epochs.\n",
    "    trainer.fit({X: trainX, Y: trainY}, val_feed_dicts={X: testX, Y: testY},\n",
    "                n_epoch=10, show_metric=True, run_id='Summaries_example')\n",
    "\n",
    "    # Run the following command to start tensorboard:\n",
    "    # >> tensorboard /tmp/tflearn_logs/\n",
    "    # Navigate with your web browser to http://0.0.0.0:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-075af1a1ecf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     accuracy = tf.reduce_mean(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1576\u001b[0m   \"\"\"\n\u001b[1;32m   1577\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[0;32m-> 1578\u001b[0;31m                     labels, logits)\n\u001b[0m\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m   \u001b[0;31m# TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1533\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1534\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "#https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/variables.py\n",
    "\n",
    "\"\"\"\n",
    "This example introduces the use of TFLearn variables to easily implement\n",
    "Tensorflow variables with custom initialization and regularization.\n",
    "Note: If you are using TFLearn layers, inititalization and regularization\n",
    "are directly defined at the layer definition level and applied to inner\n",
    "variables.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import tflearn.variables as va\n",
    "\n",
    "# Loading MNIST dataset\n",
    "import tflearn.datasets.mnist as mnist\n",
    "trainX, trainY, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "# Define a dnn using Tensorflow\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Model variables\n",
    "    X = tf.placeholder(\"float\", [None, 784])\n",
    "    Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "    # Multilayer perceptron\n",
    "    def dnn(x):\n",
    "        with tf.variable_scope('Layer1'):\n",
    "            # Creating variable using TFLearn\n",
    "            W1 = va.variable(name='W', shape=[784, 256],\n",
    "                             initializer='uniform_scaling',\n",
    "                             regularizer='L2')\n",
    "            b1 = va.variable(name='b', shape=[256])\n",
    "            x = tf.nn.tanh(tf.add(tf.matmul(x, W1), b1))\n",
    "\n",
    "        with tf.variable_scope('Layer2'):\n",
    "            W2 = va.variable(name='W', shape=[256, 256],\n",
    "                             initializer='uniform_scaling',\n",
    "                             regularizer='L2')\n",
    "            b2 = va.variable(name='b', shape=[256])\n",
    "            x = tf.nn.tanh(tf.add(tf.matmul(x, W2), b2))\n",
    "\n",
    "        with tf.variable_scope('Layer3'):\n",
    "            W3 = va.variable(name='W', shape=[256, 10],\n",
    "                             initializer='uniform_scaling')\n",
    "            b3 = va.variable(name='b', shape=[10])\n",
    "            x = tf.add(tf.matmul(x, W3), b3)\n",
    "\n",
    "        return x\n",
    "\n",
    "    net = dnn(X)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1)), tf.float32),\n",
    "        name='acc')\n",
    "\n",
    "    # Define a train op\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                              metric=accuracy, batch_size=128)\n",
    "\n",
    "    trainer = tflearn.Trainer(train_ops=trainop, tensorboard_verbose=3,\n",
    "                              tensorboard_dir='/tmp/tflearn_logs/')\n",
    "    # Training for 10 epochs.\n",
    "    trainer.fit({X: trainX, Y: trainY}, val_feed_dicts={X: testX, Y: testY},\n",
    "                n_epoch=10, show_metric=True, run_id='Variables_example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 17199  | total loss: \u001b[1m\u001b[32m0.15596\u001b[0m\u001b[0m | time: 5.341s\n",
      "| SGD | epoch: 020 | loss: 0.15596 - top3: 0.9936 -- iter: 54976/55000\n",
      "Training Step: 17200  | total loss: \u001b[1m\u001b[32m0.15071\u001b[0m\u001b[0m | time: 6.483s\n",
      "| SGD | epoch: 020 | loss: 0.15071 - top3: 0.9936 | val_loss: 0.11902 - val_acc: 0.9942 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/tflearn/tflearn/blob/master/examples/images/dnn.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Deep Neural Network for MNIST dataset classification task.\n",
    "References:\n",
    "    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based\n",
    "    learning applied to document recognition.\" Proceedings of the IEEE,\n",
    "    86(11):2278-2324, November 1998.\n",
    "Links:\n",
    "    [MNIST Dataset] http://yann.lecun.com/exdb/mnist/\n",
    "\"\"\"\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "\n",
    "# Data loading and preprocessing\n",
    "import tflearn.datasets.mnist as mnist\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "# Building deep neural network\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "dense1 = tflearn.fully_connected(input_layer, 64, activation='tanh',\n",
    "                                 regularizer='L2', weight_decay=0.001)\n",
    "dropout1 = tflearn.dropout(dense1, 0.8)\n",
    "dense2 = tflearn.fully_connected(dropout1, 64, activation='tanh',\n",
    "                                 regularizer='L2', weight_decay=0.001)\n",
    "dropout2 = tflearn.dropout(dense2, 0.8)\n",
    "softmax = tflearn.fully_connected(dropout2, 10, activation='softmax')\n",
    "\n",
    "# Regression using SGD with learning rate decay and Top-3 accuracy\n",
    "sgd = tflearn.SGD(learning_rate=0.1, lr_decay=0.96, decay_step=1000)\n",
    "top_k = tflearn.metrics.Top_k(3)\n",
    "net = tflearn.regression(softmax, optimizer=sgd, metric=top_k,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=20, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"dense_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Finetuning Example. Using weights from model trained in\n",
    "convnet_cifar10.py to retrain network for a new task (your own dataset).\n",
    "All weights are restored except last layer (softmax) that will be retrained\n",
    "to match the new task (finetuning).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "# Data loading\n",
    "# Note: You input here any dataset you would like to finetune\n",
    "X, Y = your_dataset()\n",
    "num_classes = 10\n",
    "\n",
    "# Redefinition of convnet_cifar10 network\n",
    "network = input_data(shape=[None, 32, 32, 3])\n",
    "network = conv_2d(network, 32, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = dropout(network, 0.75)\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = dropout(network, 0.5)\n",
    "network = fully_connected(network, 512, activation='relu')\n",
    "network = dropout(network, 0.5)\n",
    "# Finetuning Softmax layer (Setting restore=False to not restore its weights)\n",
    "softmax = fully_connected(network, num_classes, activation='softmax', restore=False)\n",
    "regression = regression(softmax, optimizer='adam',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        learning_rate=0.001)\n",
    "\n",
    "model = tflearn.DNN(regression, checkpoint_path='model_finetuning',\n",
    "                    max_checkpoints=3, tensorboard_verbose=0)\n",
    "# Load pre-existing model, restoring all weights, except softmax layer ones\n",
    "model.load('cifar10_cnn')\n",
    "\n",
    "# Start finetuning\n",
    "model.fit(X, Y, n_epoch=10, validation_set=0.1, shuffle=True,\n",
    "          show_metric=True, batch_size=64, snapshot_step=200,\n",
    "          snapshot_epoch=False, run_id='model_finetuning')\n",
    "\n",
    "model.save('model_finetuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tflearn/tflearn/blob/master/examples/basics/linear_regression.py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.15719\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1000 | loss: 0.15719 - R2: 0.9803 -- iter: 17/17\n",
      "\n",
      "Regression result:\n",
      "Y = [ 0.22911617]*X + [ 0.95844913]\n",
      "\n",
      "Test prediction for x = 3.2, 3.3, 3.4:\n",
      "[1.6916208267211914, 1.714532494544983, 1.7374441623687744]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Linear Regression Example \"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tflearn\n",
    "\n",
    "# Regression data\n",
    "X = [3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1]\n",
    "Y = [1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3]\n",
    "\n",
    "# Linear Regression graph\n",
    "input_ = tflearn.input_data(shape=[None])\n",
    "linear = tflearn.single_unit(input_)\n",
    "regression = tflearn.regression(linear, optimizer='sgd', loss='mean_square',\n",
    "                                metric='R2', learning_rate=0.01)\n",
    "m = tflearn.DNN(regression)\n",
    "m.fit(X, Y, n_epoch=1000, show_metric=True, snapshot_epoch=False)\n",
    "\n",
    "print(\"\\nRegression result:\")\n",
    "print(\"Y = \" + str(m.get_weights(linear.W)) +\n",
    "      \"*X + \" + str(m.get_weights(linear.b)))\n",
    "\n",
    "print(\"\\nTest prediction for x = 3.2, 3.3, 3.4:\")\n",
    "print(m.predict([3.2, 3.3, 3.4]))\n",
    "# should output (close, not exact) y = [1.5315033197402954, 1.5585315227508545, 1.5855598449707031]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tflearn/tflearn/blob/master/examples/basics/logical.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.90185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD_0 | epoch: 400 | loss: 0.44085 -- iter: 4/4\n",
      "| SGD_1 | epoch: 400 | loss: 0.46100 -- iter: 4/4\n",
      "Testing XOR operator\n",
      "0 xor 0: [[0.0011839382350444794]]\n",
      "0 xor 1: [[0.9974536895751953]]\n",
      "1 xor 0: [[0.9972860813140869]]\n",
      "1 xor 1: [[0.0013692948268726468]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Simple Example to train logical operators\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# Logical NOT operator\n",
    "X = [[0.], [1.]]\n",
    "Y = [[1.], [0.]]\n",
    "\n",
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    g = tflearn.input_data(shape=[None, 1])\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 1, activation='sigmoid')\n",
    "    g = tflearn.regression(g, optimizer='sgd', learning_rate=2.,\n",
    "                           loss='mean_square')\n",
    "\n",
    "    # Model training\n",
    "    m = tflearn.DNN(g)\n",
    "    m.fit(X, Y, n_epoch=100, snapshot_epoch=False)\n",
    "\n",
    "    # Test model\n",
    "    print(\"Testing NOT operator\")\n",
    "    print(\"NOT 0:\", m.predict([[0.]]))\n",
    "    print(\"NOT 1:\", m.predict([[1.]]))\n",
    "\n",
    "# Logical OR operator\n",
    "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "Y = [[0.], [1.], [1.], [1.]]\n",
    "\n",
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    g = tflearn.input_data(shape=[None, 2])\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 1, activation='sigmoid')\n",
    "    g = tflearn.regression(g, optimizer='sgd', learning_rate=2.,\n",
    "                           loss='mean_square')\n",
    "\n",
    "    # Model training\n",
    "    m = tflearn.DNN(g)\n",
    "    m.fit(X, Y, n_epoch=100, snapshot_epoch=False)\n",
    "\n",
    "    # Test model\n",
    "    print(\"Testing OR operator\")\n",
    "    print(\"0 or 0:\", m.predict([[0., 0.]]))\n",
    "    print(\"0 or 1:\", m.predict([[0., 1.]]))\n",
    "    print(\"1 or 0:\", m.predict([[1., 0.]]))\n",
    "    print(\"1 or 1:\", m.predict([[1., 1.]]))\n",
    "\n",
    "# Logical AND operator\n",
    "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "Y = [[0.], [0.], [0.], [1.]]\n",
    "\n",
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    g = tflearn.input_data(shape=[None, 2])\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 1, activation='sigmoid')\n",
    "    g = tflearn.regression(g, optimizer='sgd', learning_rate=2.,\n",
    "                           loss='mean_square')\n",
    "\n",
    "    # Model training\n",
    "    m = tflearn.DNN(g)\n",
    "    m.fit(X, Y, n_epoch=100, snapshot_epoch=False)\n",
    "\n",
    "    # Test model\n",
    "    print(\"Testing AND operator\")\n",
    "    print(\"0 and 0:\", m.predict([[0., 0.]]))\n",
    "    print(\"0 and 1:\", m.predict([[0., 1.]]))\n",
    "    print(\"1 and 0:\", m.predict([[1., 0.]]))\n",
    "    print(\"1 and 1:\", m.predict([[1., 1.]]))\n",
    "\n",
    "'''\n",
    "Going further: Graph combination with multiple optimizers\n",
    "Create a XOR operator using product of NAND and OR operators\n",
    "'''\n",
    "# Data\n",
    "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "Y_nand = [[1.], [1.], [1.], [0.]]\n",
    "Y_or = [[0.], [1.], [1.], [1.]]\n",
    "\n",
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    # Building a network with 2 optimizers\n",
    "    g = tflearn.input_data(shape=[None, 2])\n",
    "    # Nand operator definition\n",
    "    g_nand = tflearn.fully_connected(g, 32, activation='linear')\n",
    "    g_nand = tflearn.fully_connected(g_nand, 32, activation='linear')\n",
    "    g_nand = tflearn.fully_connected(g_nand, 1, activation='sigmoid')\n",
    "    g_nand = tflearn.regression(g_nand, optimizer='sgd',\n",
    "                                learning_rate=2.,\n",
    "                                loss='binary_crossentropy')\n",
    "    # Or operator definition\n",
    "    g_or = tflearn.fully_connected(g, 32, activation='linear')\n",
    "    g_or = tflearn.fully_connected(g_or, 32, activation='linear')\n",
    "    g_or = tflearn.fully_connected(g_or, 1, activation='sigmoid')\n",
    "    g_or = tflearn.regression(g_or, optimizer='sgd',\n",
    "                              learning_rate=2.,\n",
    "                              loss='binary_crossentropy')\n",
    "    # XOR merging Nand and Or operators\n",
    "    g_xor = tflearn.merge([g_nand, g_or], mode='elemwise_mul')\n",
    "\n",
    "    # Training\n",
    "    m = tflearn.DNN(g_xor)\n",
    "    m.fit(X, [Y_nand, Y_or], n_epoch=400, snapshot_epoch=False)\n",
    "\n",
    "    # Testing\n",
    "    print(\"Testing XOR operator\")\n",
    "    print(\"0 xor 0:\", m.predict([[0., 0.]]))\n",
    "    print(\"0 xor 1:\", m.predict([[0., 1.]]))\n",
    "    print(\"1 xor 0:\", m.predict([[1., 0.]]))\n",
    "    print(\"1 xor 1:\", m.predict([[1., 1.]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
