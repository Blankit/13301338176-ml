{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.infoq.com/cn/articles/deeplearning-tensorflow-actual-combat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建模型的权重及偏置\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35), name=\"weights\")\n",
    "\n",
    "biases = tf.Variable(tf.zeros([200]), name=\"biases\")\n",
    "\n",
    "\n",
    "\n",
    "#指定变量所在设备为CPU:0\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "\n",
    "  v = tf.Variable(...)\n",
    "\n",
    "\n",
    "\n",
    "#初始化模型变量\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess=tf.Session()\n",
    "\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "\n",
    "#保存模型变量，由三个文件组成model.data，model.index，model.meta\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(sess, \"/tmp/model\")\n",
    "\n",
    "\n",
    "\n",
    "#恢复模型变量\n",
    "\n",
    "saver.restore(sess, \"/tmp/model\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在复杂的深度学习模型中，存在大量的模型变量，并且期望能够一次性地初始化这些变量。TensorFlow提供了tf.variable_scope和tf.get_variable两个API，实现了共享模型变量。tf.get_variable(<name>, <shape>, <initializer>)：表示创建或返回指定名称的模型变量，其中name表示变量名称，shape表示变量的维度信息，initializer表示变量的初始化方法。tf.variable_scope(<scope_name>)：表示变量所在的命名空间，其中scope_name表示命名空间的名称。共享模型变量使用示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义卷积神经网络运算规则，其中weights和biases为共享变量\n",
    "\n",
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "\n",
    "    # 创建变量\"weights\".\n",
    "\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape, initializer=tf.random_normal_initializer())\n",
    "\n",
    "    # 创建变量 \"biases\".\n",
    "\n",
    "    biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    conv = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "\n",
    "\n",
    "#定义卷积层，conv1和conv2为变量命名空间\n",
    "\n",
    "with tf.variable_scope(\"conv1\"):\n",
    "\n",
    "    # 创建变量 \"conv1/weights\", \"conv1/biases\".\n",
    "\n",
    "    relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n",
    "\n",
    "with tf.variable_scope(\"conv2\"):\n",
    "\n",
    "    # 创建变量 \"conv2/weights\", \"conv2/biases\".\n",
    "\n",
    "    relu1 = conv_relu(relu1, [5, 5, 32, 32], [32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#两个隐藏层，一个logits输出层\n",
    "\n",
    "hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "\n",
    "\n",
    "#损失方程，采用softmax交叉熵算法\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits( logits, labels, name='xentropy')\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "\n",
    "\n",
    "#选定优化算法及定义训练操作\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "模型训练及模型验证示例如下：\n",
    "\n",
    "#加载训练数据，并执行网络训练\n",
    "\n",
    "for step in xrange(FLAGS.max_steps):\n",
    "\n",
    "    feed_dict = fill_feed_dict(data_sets.train, images_placeholder, labels_placeholder)\n",
    "\n",
    "    _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "\n",
    "#加载测试数据，计算模型精确度\n",
    "\n",
    "for step in xrange(steps_per_epoch):\n",
    "\n",
    "    feed_dict = fill_feed_dict(data_set, images_placeholder, labels_placeholder)\n",
    "\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow可视化技术主要分为两部分：TensorFlow摘要模型及TensorBoard可视化组件。在摘要模型中，需要把模型变量或样本数据转换为TensorFlow summary操作，然后合并summary操作，最后通过Summary Writer操作写入TensorFlow的事件日志。TensorBoard通过读取事件日志，进行相关摘要信息的可视化展示，主要包括：Scalar图、图片数据可视化、声音数据展示、图模型可视化，以及变量数据的直方图和概率分布图。TensorFlow可视化技术的关键流程如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义变量及训练数据的摘要操作\n",
    "\n",
    "tf.summary.scalar('max', tf.reduce_max(var))\n",
    "\n",
    "tf.summary.histogram('histogram', var)\n",
    "\n",
    "tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "\n",
    "\n",
    "#定义合并变量操作，一次性生成所有摘要数据\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "\n",
    "#定义写入摘要数据到事件日志的操作\n",
    "\n",
    "train_writer = tf.train.SummaryWriter(FLAGS.log_dir + '/train', sess.graph)\n",
    "\n",
    "test_writer = tf.train.SummaryWriter(FLAGS.log_dir + '/test')\n",
    "\n",
    "\n",
    "\n",
    "#执行训练操作，并把摘要信息写入到事件日志\n",
    "\n",
    "summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "\n",
    "train_writer.add_summary(summary, i)\n",
    "\n",
    "\n",
    "\n",
    "  #下载示例code，并执行模型训练\n",
    "\n",
    "  python mnist_with_summaries.py\n",
    "\n",
    "\n",
    "\n",
    "#启动TensorBoard，TensorBoard的UI地址为http://ip_address:6006\n",
    "\n",
    "tensorboard --logdir=/path/to/log-directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample\n",
    "\n",
    "https://github.com/MindorksOpenSource\n",
    "\n",
    "https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample\n",
    "\n",
    "\n",
    "https://github.com/locuslab/icnn\n",
    "\n",
    "https://github.com/locuslab\n",
    "\n",
    "http://python-for-android.readthedocs.io/en/latest/\n",
    "\n",
    "https://github.com/nok/sklearn-porter\n",
    "\n",
    "http://cs.smith.edu/dftwiki/index.php/Tutorial:_TensorFlow_References#Soon_Hin_Khor.27s_Gentlest_Introduction_to_TensorFlow\n",
    "\n",
    "https://jalammar.github.io/Supercharging-android-apps-using-tensorflow/\n",
    "\n",
    "App Structure Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.csdn.net/freedom098/article/details/52117330\n",
    "\n",
    " tensorflow实现KNN识别MNIST KNN算法算是最简单的机器学习算法之一了，这个算法最大的特点是没有训练过程，是一种懒惰学习，这种结构也可以在tensorflow实现。\n",
    "\n",
    "KNN的最核心就是距离度量方式，官方例程给出的是L1范数的例子，我这里改成了L2范数，也就是我们常说的欧几里得距离度量，另外，虽然是叫KNN，意思是选取k个最接近的元素来投票产生分类，但是这里只是用了最近的那个数据的标签作为预测值了。\n",
    "\n",
    "距离的定义:      距离衡量包括欧式距离、夹角余弦等。对于文本分类来说，使用余弦(cosine)来计算相似度就比欧式(Euclidean)距离更合适\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-8-924a546f2d61>:18: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  3\n",
      "true value is  8\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  1\n",
      "true value is  8\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  2\n",
      "true value is  9\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  5\n",
      "true value is  6\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  4\n",
      "true value is  8\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  4\n",
      "true value is  9\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  7\n",
      "true value is  3\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  6\n",
      "true value is  8\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  9\n",
      "true value is  4\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  3\n",
      "true value is  3\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  1\n",
      "true value is  1\n",
      "prediction is  7\n",
      "true value is  7\n",
      "prediction is  0\n",
      "true value is  5\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  6\n",
      "true value is  6\n",
      "prediction is  9\n",
      "true value is  4\n",
      "prediction is  5\n",
      "true value is  5\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  2\n",
      "true value is  2\n",
      "prediction is  0\n",
      "true value is  0\n",
      "prediction is  4\n",
      "true value is  4\n",
      "prediction is  8\n",
      "true value is  8\n",
      "prediction is  9\n",
      "true value is  9\n",
      "prediction is  9\n",
      "true value is  9\n",
      "0.945\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "  \n",
    "def loadMNIST():  \n",
    "    from tensorflow.examples.tutorials.mnist import input_data  \n",
    "    mnist = input_data.read_data_sets('MNIST_data',one_hot=True)  \n",
    "    return mnist  \n",
    "def KNN(mnist):  \n",
    "    train_x,train_y = mnist.train.next_batch(5000)  \n",
    "    test_x,test_y = mnist.train.next_batch(200)  \n",
    "  \n",
    "    xtr = tf.placeholder(tf.float32,[None,784])  \n",
    "    xte = tf.placeholder(tf.float32,[784])  \n",
    "    distance = tf.sqrt(tf.reduce_sum(tf.pow(tf.add(xtr,tf.negative(xte)),2),reduction_indices=1))  \n",
    "  \n",
    "    pred = tf.argmin(distance,0)  \n",
    "  \n",
    "    init = tf.initialize_all_variables()  \n",
    "  \n",
    "    sess = tf.Session()  \n",
    "    sess.run(init)  \n",
    "  \n",
    "    right = 0  \n",
    "    for i in range(200):  \n",
    "        ansIndex = sess.run(pred,{xtr:train_x,xte:test_x[i,:]})  \n",
    "        print 'prediction is ',np.argmax(train_y[ansIndex])  \n",
    "        print 'true value is ',np.argmax(test_y[i])  \n",
    "        if np.argmax(test_y[i]) == np.argmax(train_y[ansIndex]):  \n",
    "            right += 1.0  \n",
    "    accracy = right/200.0  \n",
    "    print accracy  \n",
    "  \n",
    "if __name__ == \"__main__\":  \n",
    "    #mnist = loadMNIST()  \n",
    "    KNN(mnist)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://my.oschina.net/u/3281376/blog/847370\n",
    "\n",
    "kNN分类算法的评价\n",
    "优点\n",
    "\n",
    "    1.简单，易于理解，易于实现，无需估计参数，无需训练；\n",
    "        适合对稀有事件进行分类；\n",
    "    3.特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好。\n",
    "\n",
    "缺点\n",
    "\n",
    "    1.样本不平衡时，会使结果出现偏差\n",
    "    2.计算量庞大\n",
    "    3.由于不需要训练，所以算法的可控性比较差\n",
    "\n",
    "改进策略\n",
    "\n",
    "分类效率：事先对样本属性进行约简，删除对分类结果影响较小的属性，快速的得出待分类样本的类别。该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。\n",
    "\n",
    "分类效果：采用权值的方法（和该样本距离小的邻居权值大）来改进，Han等人于2002年尝试利用贪心法，针对文件分类实做可调整权重的k最近邻居法WAkNN (weighted adjusted k nearest neighbor)，以促进分类效果；而Li等人于2004年提出由于不同分类的文件本身有数量上有差异，因此也应该依照训练集合中各种分类的文件数量，选取不同数目的最近邻居，来参与分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-dd7aef54b5b8>:23: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "('Test', 0, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 1, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 2, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 3, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 4, 'Prediction:', 9, 'True Class:', 9)\n",
      "('Test', 5, 'Prediction:', 9, 'True Class:', 9)\n",
      "('Test', 6, 'Prediction:', 2, 'True Class:', 2)\n",
      "('Test', 7, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 8, 'Prediction:', 9, 'True Class:', 9)\n",
      "('Test', 9, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 10, 'Prediction:', 8, 'True Class:', 8)\n",
      "('Test', 11, 'Prediction:', 1, 'True Class:', 7)\n",
      "('Test', 12, 'Prediction:', 3, 'True Class:', 3)\n",
      "('Test', 13, 'Prediction:', 9, 'True Class:', 9)\n",
      "('Test', 14, 'Prediction:', 7, 'True Class:', 7)\n",
      "('Test', 15, 'Prediction:', 9, 'True Class:', 4)\n",
      "('Test', 16, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 17, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 18, 'Prediction:', 4, 'True Class:', 9)\n",
      "('Test', 19, 'Prediction:', 2, 'True Class:', 2)\n",
      "('Test', 20, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 21, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 22, 'Prediction:', 7, 'True Class:', 7)\n",
      "('Test', 23, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 24, 'Prediction:', 7, 'True Class:', 7)\n",
      "('Test', 25, 'Prediction:', 9, 'True Class:', 9)\n",
      "('Test', 26, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 27, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 28, 'Prediction:', 8, 'True Class:', 8)\n",
      "('Test', 29, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 30, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 31, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 32, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 33, 'Prediction:', 7, 'True Class:', 7)\n",
      "('Test', 34, 'Prediction:', 8, 'True Class:', 8)\n",
      "('Test', 35, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 36, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 37, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 38, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 39, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 40, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 41, 'Prediction:', 7, 'True Class:', 7)\n",
      "('Test', 42, 'Prediction:', 3, 'True Class:', 3)\n",
      "('Test', 43, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 44, 'Prediction:', 7, 'True Class:', 7)\n",
      "('Test', 45, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 46, 'Prediction:', 8, 'True Class:', 8)\n",
      "('Test', 47, 'Prediction:', 2, 'True Class:', 2)\n",
      "('Test', 48, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 49, 'Prediction:', 2, 'True Class:', 2)\n",
      "('Test', 50, 'Prediction:', 9, 'True Class:', 9)\n",
      "('Test', 51, 'Prediction:', 8, 'True Class:', 9)\n",
      "('Test', 52, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 53, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 54, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 55, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 56, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 57, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 58, 'Prediction:', 5, 'True Class:', 3)\n",
      "('Test', 59, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 60, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 61, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 62, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 63, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 64, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 65, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 66, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 67, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 68, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 69, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 70, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 71, 'Prediction:', 7, 'True Class:', 7)\n",
      "('Test', 72, 'Prediction:', 2, 'True Class:', 2)\n",
      "('Test', 73, 'Prediction:', 3, 'True Class:', 3)\n",
      "('Test', 74, 'Prediction:', 2, 'True Class:', 2)\n",
      "('Test', 75, 'Prediction:', 1, 'True Class:', 7)\n",
      "('Test', 76, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 77, 'Prediction:', 8, 'True Class:', 8)\n",
      "('Test', 78, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 79, 'Prediction:', 8, 'True Class:', 8)\n",
      "('Test', 80, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 81, 'Prediction:', 8, 'True Class:', 8)\n",
      "('Test', 82, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 83, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 84, 'Prediction:', 8, 'True Class:', 8)\n",
      "('Test', 85, 'Prediction:', 9, 'True Class:', 9)\n",
      "('Test', 86, 'Prediction:', 2, 'True Class:', 2)\n",
      "('Test', 87, 'Prediction:', 5, 'True Class:', 5)\n",
      "('Test', 88, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 89, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 90, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 91, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 92, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 93, 'Prediction:', 4, 'True Class:', 9)\n",
      "('Test', 94, 'Prediction:', 0, 'True Class:', 0)\n",
      "('Test', 95, 'Prediction:', 1, 'True Class:', 3)\n",
      "('Test', 96, 'Prediction:', 1, 'True Class:', 1)\n",
      "('Test', 97, 'Prediction:', 6, 'True Class:', 6)\n",
      "('Test', 98, 'Prediction:', 4, 'True Class:', 4)\n",
      "('Test', 99, 'Prediction:', 8, 'True Class:', 2)\n",
      "Done!\n",
      "('Accuracy:', 0.9100000000000006)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "#import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)     #下载并加载mnist数据\n",
    "\n",
    "train_X, train_Y = mnist.train.next_batch(5000) # 5000 for training (nn candidates)\n",
    "test_X, test_Y = mnist.test.next_batch(100)   # 200 for testing\n",
    "\n",
    "\n",
    "tra_X = tf.placeholder(\"float\", [None, 784])\n",
    "te_X = tf.placeholder(\"float\", [784])\n",
    "\n",
    "# Nearest Neighbor calculation using L1 Distance\n",
    "# Calculate L1 Distance\n",
    "distance = tf.reduce_sum(tf.abs(tf.add(tra_X,tf.negative(te_X))), reduction_indices=1)\n",
    "# Prediction: Get min distance index (Nearest neighbor)\n",
    "pred = tf.arg_min(distance, 0)\n",
    "\n",
    "accuracy = 0.\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # loop over test data\n",
    "    for i in range(len(test_X)):\n",
    "        # Get nearest neighbor\n",
    "        nn_index = sess.run(pred, feed_dict={tra_X: train_X, te_X: test_X[i, :]})\n",
    "        # Get nearest neighbor class label and compare it to its true label\n",
    "        print(\"Test\", i, \"Prediction:\", np.argmax(train_Y[nn_index]), \\\n",
    "            \"True Class:\", np.argmax(test_Y[i]))\n",
    "        # Calculate accuracy\n",
    "        if np.argmax(train_Y[nn_index]) == np.argmax(test_Y[i]):\n",
    "            accuracy += 1./len(test_X)\n",
    "    print(\"Done!\")\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.26274511  0.26274511  0.43529415  0.27843139  0.54509807  0.5411765\n",
      "   0.26274511  0.26274511  0.07058824  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.18823531  0.3921569\n",
      "   0.52156866  0.82352948  1.          0.99215692  0.99215692  0.99215692\n",
      "   0.99215692  0.99215692  0.99215692  0.99215692  0.86666673  0.27843139\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.04705883  0.37254903\n",
      "   0.86274517  0.97254908  0.99215692  0.96862751  0.90588242  0.90980399\n",
      "   0.79215693  0.47450984  0.24705884  0.03921569  0.03921569  0.05882353\n",
      "   0.56470591  0.99215692  0.59607846  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.21960786  0.06666667\n",
      "   0.35294119  0.83137262  0.99215692  0.99215692  0.89019614  0.7019608\n",
      "   0.2392157   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.0627451   0.82745105  0.99215692  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.84705889  0.84313732  0.95686281  0.99215692  0.91764712  0.64705884\n",
      "   0.14901961  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.13725491  0.93725497\n",
      "   0.65098041  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.10588236  0.85098046  0.99215692  0.95686281\n",
      "   0.25098041  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.00392157\n",
      "   0.40000004  0.99215692  0.5411765   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.61960787\n",
      "   0.99215692  0.99215692  0.81568635  0.03921569  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.12156864  0.99215692  0.94117653  0.10588236  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.07058824  0.73333335  0.99215692  0.99215692  0.34901962  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.60784316  0.94117653  0.29019609  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.22352943  0.97647065  0.99215692  0.7960785   0.03921569\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.30588236  0.96470594  0.27450982  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.27843139  0.99607849  0.59215689\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.37254903  0.90588242  0.89803928  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.02745098  0.3019608\n",
      "   0.06666667  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.02352941  0.74117649  0.90196085  0.19215688  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.80784321  0.90980399  0.20392159  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.41568631  0.99215692  0.36078432  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.17254902  0.9333334   0.84313732  0.12156864  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.26274511  0.99215692  0.13333334  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.28235295  0.80392164  0.02352941  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.61960787  0.34117648  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.18431373  0.12156864  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]]\n",
      "====================\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y = mnist.train.next_batch(1)\n",
    "print train_x\n",
    "print \"====================\"\n",
    "print train_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " pandas使用get_dummies进行one-hot编码    http://blog.csdn.net/lujiandong1/article/details/52836051\n",
    " \n",
    " 离散特征的编码分为两种情况：\n",
    "\n",
    "1、离散特征的取值之间没有大小的意义，比如color：[red,blue],那么就使用one-hot编码\n",
    "\n",
    "2、离散特征的取值有大小的意义，比如size:[X,XL,XXL],那么就使用数值的映射{X:1,XL:2,XXL:3\n",
    "\n",
    "Using the get_dummies will create a new column for every unique string in a certain column:使用get_dummies进行one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   size  prize  class label  color_blue  color_green  color_red\n",
      "0     1   10.1            1           0            1          0\n",
      "1     2   13.5            0           0            0          1\n",
      "2     3   15.3            1           1            0          0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "df = pd.DataFrame([  \n",
    "            ['green', 'M', 10.1, 'class1'],   \n",
    "            ['red', 'L', 13.5, 'class2'],   \n",
    "            ['blue', 'XL', 15.3, 'class1']])  \n",
    "  \n",
    "df.columns = ['color', 'size', 'prize', 'class label']  \n",
    "  \n",
    "size_mapping = {  \n",
    "           'XL': 3,  \n",
    "           'L': 2,  \n",
    "           'M': 1}  \n",
    "df['size'] = df['size'].map(size_mapping)  \n",
    "  \n",
    "class_mapping = {label:idx for idx,label in enumerate(set(df['class label']))}  \n",
    "df['class label'] = df['class label'].map(class_mapping)  \n",
    "\n",
    "r=pd.get_dummies(df)  \n",
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 可以利用numpy的array来创建tensor,需要使用的函数是tensorflow里的convert_to_tensor函数。也可将tensor转为numpy数据用eval函数, Session.run(). t.eval() is a shortcut for calling tf.get_default_session().run(t).\n",
    "\n",
    "2. reshape() 所以-1，就是缺省值，就是先以你们合适，到时总数除以你们几个的乘积，我该是几就是几\n",
    "\n",
    "3. http://www.tensorfly.cn/tfdoc/api_docs/python/array_ops.html\n",
    "\n",
    "4. http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.as_default\n",
    "Tensor Transformations\n",
    "\n",
    "Casting\n",
    "tf.string_to_number(string_tensor, out_type=None, name=None)\n",
    "tf.to_double(x, name='ToDouble')\n",
    "tf.to_float(x, name='ToFloat')\n",
    "tf.to_bfloat16(x, name='ToBFloat16')\n",
    "tf.to_int32(x, name='ToInt32')\n",
    "tf.to_int64(x, name='ToInt64')\n",
    "tf.cast(x, dtype, name=None)\n",
    "Shapes and Shaping\n",
    "tf.shape(input, name=None)\n",
    "tf.size(input, name=None)\n",
    "tf.rank(input, name=None)\n",
    "tf.reshape(tensor, shape, name=None)\n",
    "tf.squeeze(input, squeeze_dims=None, name=None)\n",
    "tf.expand_dims(input, dim, name=None)\n",
    "Slicing and Joining\n",
    "tf.slice(input_, begin, size, name=None)\n",
    "tf.split(split_dim, num_split, value, name='split')\n",
    "tf.tile(input, multiples, name=None)\n",
    "tf.pad(input, paddings, name=None)\n",
    "tf.concat(concat_dim, values, name='concat')\n",
    "tf.pack(values, name='pack')\n",
    "tf.unpack(value, num=None, name='unpack')\n",
    "tf.reverse_sequence(input, seq_lengths, seq_dim, name=None)\n",
    "tf.reverse(tensor, dims, name=None)\n",
    "tf.transpose(a, perm=None, name='transpose')\n",
    "tf.gather(params, indices, name=None)\n",
    "tf.dynamic_partition(data, partitions, num_partitions, name=None)\n",
    "tf.dynamic_stitch(indices, data, name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1001 1002 1003]\n",
      " [   3    4    5]]\n",
      "Tensor(\"Const_162:0\", shape=(2, 3), dtype=float32)\n",
      "=======1======\n",
      "Tensor(\"Reshape_74:0\", shape=(2, 3), dtype=int64)\n",
      "=======2======\n",
      "Tensor(\"Reshape_75:0\", shape=(2, 3), dtype=int32)\n",
      "=======3======\n",
      "[[ 1.  3.]\n",
      " [ 3.  7.]]\n",
      "(2, 3)\n",
      "(4, 2)\n",
      "Tensor(\"concat_37:0\", shape=(4, 3), dtype=int32)\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "Tensor(\"concat_38:0\", shape=(2, 6), dtype=int32)\n",
      "[[ 1  2  3  7  8  9]\n",
      " [ 4  5  6 10 11 12]]\n",
      "=======4======\n",
      "Tensor(\"Reshape_76:0\", shape=(2, 4), dtype=int32)\n",
      "Tensor(\"Reshape_77:0\", shape=(2, 2, 2), dtype=int32)\n",
      "=======5======\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np\n",
    "\n",
    "x=np.array([[1001,1002,1003],[3,4,5]])\n",
    "print x\n",
    "tx = tf.convert_to_tensor(x ,dtype=tf.float32)\n",
    "print  tx\n",
    "print \"=======1======\"\n",
    "\n",
    "\n",
    "x=np.array([[1001,1002,1003],[3,4,5]])\n",
    "tx =tf.reshape(x,[2,3]) \n",
    "print tx\n",
    "print \"=======2======\"   \n",
    "    \n",
    "    \n",
    "    \n",
    "t1 = [[1,2,3], [4,5,6]]  \n",
    "t2 = [[7,8,9], [10,11,12]]  \n",
    "x =tf.reshape(t1,[2,3])\n",
    "print x\n",
    "print \"=======3======\"\n",
    "    \n",
    "    \n",
    "\n",
    "  # Build a dataflow graph.\n",
    "c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "e = tf.matmul(c, d)\n",
    "# Construct a `Session` to execut the graph.\n",
    "sess = tf.Session()\n",
    "# Execute the graph and store the value that `e` represents in `result`.\n",
    "result = sess.run(e)\n",
    "print result    \n",
    "    \n",
    "\n",
    "\n",
    "c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "print c.get_shape()\n",
    "d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\n",
    "print d.get_shape()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t1 = [[1, 2, 3], [4, 5, 6]]\n",
    "t2 = [[7, 8, 9], [10, 11, 12]]\n",
    "t=tf.concat([t1, t2],0)\n",
    "print t\n",
    "with sess.as_default():\n",
    "    print(t.eval())\n",
    "#==> [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "t=tf.concat([t1, t2],1)\n",
    "print t\n",
    "with sess.as_default():\n",
    "    print(t.eval())\n",
    "\n",
    "\n",
    "print \"=======4======\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "tensor = tf.constant([1, 2, 3, 4, 5, 6, 7,8])  \n",
    "tensorReshape = tf.reshape(tensor,[2,4])  \n",
    "print(tensorReshape)  \n",
    "tensorReshape = tf.reshape(tensor,[-1,2,2])  \n",
    "print(tensorReshape)  \n",
    "\n",
    "print \"=======5======\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-dd31a7b4c7b9>:8: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "v1 =  11\n",
      "v2 =  22\n",
      "Model saved in file:  ./model/model3.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Graph().as_default() as net2_graph:\n",
    "# Create some variables.\n",
    "  v1 = tf.Variable(11, name=\"v1\")\n",
    "  v2 = tf.Variable(22, name=\"v2\")\n",
    "\n",
    "# Add an op to initialize the variables.\n",
    "  init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, save the\n",
    "# variables to disk.\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print \"v1 = \", v1.eval()\n",
    "    print \"v2 = \", v2.eval()\n",
    "  # Save the variables to disk.\n",
    "    save_path = saver.save(sess, \"./model/model3.ckpt\")\n",
    "    print \"Model saved in file: \", save_path\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "! rm ./model/*\n",
    "\n",
    "!  tree \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "v1 =  11\n",
      "v2 =  22\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default() as net1_graph:\n",
    "# Create some variables.\n",
    "  v1 = tf.Variable(0, name=\"v1\")\n",
    "  v2 = tf.Variable(0, name=\"v2\")\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "  with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "    saver.restore(sess, \"./model/model3.ckpt\")\n",
    "    print \"Model restored.\"\n",
    "    print \"v1 = \", v1.eval()\n",
    "    print \"v2 = \", v2.eval()\n",
    "    v_ = sess.run(v1)\n",
    "    print(v_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-646299c9db8a>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "1.2\n",
      "Model stored....\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "v1 = tf.Variable(1.1, name=\"v1\")\n",
    "v2 = tf.Variable(1.2, name=\"v2\")\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print v2.eval(sess)\n",
    "    save_path=\"/tmp/zhjmodel-1.ckpt\"\n",
    "    saver.save(sess,save_path)\n",
    "    print \"Model stored....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print v1.eval(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-757189015d8b>:3: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "1.0\n",
      "Model restored.\n",
      "1.1\n",
      "1.1\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "v1 = tf.Variable(1.0, name=\"v1\")\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print v1.eval(sess)\n",
    "    save_path=\"/tmp/zhjmodel-1.ckpt\"\n",
    "    saver.restore(sess, save_path)\n",
    "    print(\"Model restored.\")\n",
    "    print sess.run(v1)\n",
    "    print sess.run(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-85-cd3af2050e27>:8: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "('save to path:', 'my_net/save_net.ckpt')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "  \n",
    "# save to file  \n",
    "W = tf.Variable([[1,2,3],[4,5,6]],dtype = tf.float32,name='weight')  \n",
    "b = tf.Variable([[1,2,3]],dtype = tf.float32,name='biases')  \n",
    "  \n",
    "init = tf.initialize_all_variables()  \n",
    "saver = tf.train.Saver()  \n",
    "with tf.Session() as sess:  \n",
    "        sess.run(init)  \n",
    "        save_path = saver.save(sess,\"my_net/save_net.ckpt\")  \n",
    "        print (\"save to path:\",save_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mmodel\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model3.ckpt.data-00000-of-00001\r\n",
      "│   ├── model3.ckpt.index\r\n",
      "│   └── model3.ckpt.meta\r\n",
      "├── \u001b[01;34mmy_net\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── save_net.ckpt.data-00000-of-00001\r\n",
      "│   ├── save_net.ckpt.index\r\n",
      "│   └── save_net.ckpt.meta\r\n",
      "└── TF_zhj.ipynb\r\n",
      "\r\n",
      "2 directories, 9 files\r\n"
     ]
    }
   ],
   "source": [
    "! tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weights:', array([[ 1.,  2.,  3.],\n",
      "       [ 4.,  5.,  6.]], dtype=float32))\n",
      "('biases:', array([[ 1.,  2.,  3.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "tf.reset_default_graph()\n",
    "W = tf.Variable(np.arange(6).reshape((2,3)),dtype = tf.float32,name='weight')  \n",
    "b = tf.Variable(np.arange(3).reshape((1,3)),dtype = tf.float32,name='biases')  \n",
    "  \n",
    "saver = tf.train.Saver()  \n",
    "with tf.Session() as sess:  \n",
    "        saver.restore(sess,\"my_net/save_net.ckpt\")  \n",
    "        print (\"weights:\",sess.run(W))  \n",
    "        print (\"biases:\",sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.csdn.net/lujiandong1/article/details/53301994\n",
    "\n",
    "训练的过程：\n",
    "\n",
    "1、先设置isTrain=True,然后会保存模型,设置isTrain=False会将训练好的模型加载进来进行测试\n",
    "\n",
    "2、train_steps：表示训练的次数，例子中使用100\n",
    "3、checkpoint_steps：表示训练多少次保存一下checkpoints，例子中使用50\n",
    "4、checkpoint_dir：表示checkpoints文件的保存路径，例子中使用当前路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-98-42bdf6e030b9>:27: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n",
      "+++++++\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "  \n",
    "isTrain = True  \n",
    "train_steps = 100  \n",
    "checkpoint_steps = 50  \n",
    "checkpoint_dir = 'continus/'  \n",
    "  \n",
    "x = tf.placeholder(tf.float32, shape=[None, 1])  \n",
    "y = 4 * x + 4  \n",
    "  \n",
    "w = tf.Variable(tf.random_normal([1], -1, 1))  \n",
    "b = tf.Variable(tf.zeros([1]))  \n",
    "y_predict = w * x + b  \n",
    "  \n",
    "  \n",
    "loss = tf.reduce_mean(tf.square(y - y_predict))  \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)  \n",
    "train = optimizer.minimize(loss)  \n",
    "  \n",
    "\n",
    "  \n",
    "saver = tf.train.Saver()  # defaults to saving all variables - in this case w and b  \n",
    "x_data = np.reshape(np.random.rand(10).astype(np.float32), (10, 1))  \n",
    "  \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.initialize_all_variables())  \n",
    "    if isTrain:  \n",
    "        for i in xrange(train_steps):  \n",
    "            sess.run(train, feed_dict={x: x_data})  \n",
    "            print ('+++++++')  \n",
    "            if (i + 1) % checkpoint_steps == 0:  \n",
    "                saver.save(sess, checkpoint_dir + 'model.ckpt', global_step=i+1)  \n",
    "                \n",
    "    else:  \n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)  \n",
    "        print ('-------') \n",
    "        if ckpt and ckpt.model_checkpoint_path:  \n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)  \n",
    "        else:  \n",
    "            pass  \n",
    "        print(sess.run(w))  \n",
    "        print(sess.run(b))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mcontinus\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model.ckpt-100.data-00000-of-00001\r\n",
      "│   ├── model.ckpt-100.index\r\n",
      "│   ├── model.ckpt-100.meta\r\n",
      "│   ├── model.ckpt-50.data-00000-of-00001\r\n",
      "│   ├── model.ckpt-50.index\r\n",
      "│   └── model.ckpt-50.meta\r\n",
      "├── \u001b[01;34mmodel\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model3.ckpt.data-00000-of-00001\r\n",
      "│   ├── model3.ckpt.index\r\n",
      "│   └── model3.ckpt.meta\r\n",
      "├── \u001b[01;34mmy_net\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── save_net.ckpt.data-00000-of-00001\r\n",
      "│   ├── save_net.ckpt.index\r\n",
      "│   └── save_net.ckpt.meta\r\n",
      "└── TF_zhj.ipynb\r\n",
      "\r\n",
      "3 directories, 16 files\r\n"
     ]
    }
   ],
   "source": [
    "! tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.csdn.net/lujiandong1/article/details/53385092\n",
    " tensorflow将训练好的模型freeze,即将权重固化到图里面,并使用该模型进行预测\n",
    " \n",
    " 说明：对于freeze操作,我们需要定义输出结点的名字.因为网络其实是比较复杂的,定义了输出结点的名字,那么freeze的时候就只把输出该结点所需要的子图都固化下来,其他无关的就舍弃掉.因为我们freeze模型的目的是接下来做预测.所以,一般情况下,output_node_names就是我们预测的目标.\n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs.shape:', (10000, 10))\n",
      "('labels.shape:', (10000, 1))\n",
      "('test_inputs.shape:', (100, 10))\n",
      "('test_labels.shape:', (100, 1))\n",
      "10000 items in batch of 32 gives us 312 full batches and -280 batches of 16 items\n",
      "Number of batches: 313\n",
      "Size of full batch: 2\n",
      "Size if final batch: 2\n",
      "WARNING:tensorflow:From <ipython-input-1-9513045c2b99>:71: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "accuracy: 0.530000\n",
      "final accuracy: 0.530000\n",
      "Placeholder/inputs_placeholder\n",
      "Placeholder/labels_placeholder\n",
      "NN/W1/Initializer/random_normal/shape\n",
      "NN/W1/Initializer/random_normal/mean\n",
      "NN/W1/Initializer/random_normal/stddev\n",
      "NN/W1/Initializer/random_normal/RandomStandardNormal\n",
      "NN/W1/Initializer/random_normal/mul\n",
      "NN/W1/Initializer/random_normal\n",
      "NN/W1\n",
      "NN/W1/Assign\n",
      "NN/W1/read\n",
      "NN/b1/Initializer/Const\n",
      "NN/b1\n",
      "NN/b1/Assign\n",
      "NN/b1/read\n",
      "NN/W2/Initializer/random_normal/shape\n",
      "NN/W2/Initializer/random_normal/mean\n",
      "NN/W2/Initializer/random_normal/stddev\n",
      "NN/W2/Initializer/random_normal/RandomStandardNormal\n",
      "NN/W2/Initializer/random_normal/mul\n",
      "NN/W2/Initializer/random_normal\n",
      "NN/W2\n",
      "NN/W2/Assign\n",
      "NN/W2/read\n",
      "NN/b2/Initializer/Const\n",
      "NN/b2\n",
      "NN/b2/Assign\n",
      "NN/b2/read\n",
      "NN/MatMul\n",
      "NN/add\n",
      "NN/Relu\n",
      "NN/MatMul_1\n",
      "NN/add_1\n",
      "NN/Relu_1\n",
      "NN/Add\n",
      "NN/div/y\n",
      "NN/div\n",
      "Loss/sub\n",
      "Loss/Square\n",
      "Loss/div/y\n",
      "Loss/div\n",
      "Loss/Const\n",
      "Loss/Sum\n",
      "Accuracy/predictions/y\n",
      "Accuracy/predictions\n",
      "Accuracy/Cast\n",
      "Accuracy/correct_predictions\n",
      "Accuracy/Cast_1\n",
      "Accuracy/Const\n",
      "Accuracy/Mean\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/Loss/Sum_grad/Reshape/shape\n",
      "gradients/Loss/Sum_grad/Reshape\n",
      "gradients/Loss/Sum_grad/Shape\n",
      "gradients/Loss/Sum_grad/Tile\n",
      "gradients/Loss/div_grad/Shape\n",
      "gradients/Loss/div_grad/Shape_1\n",
      "gradients/Loss/div_grad/BroadcastGradientArgs\n",
      "gradients/Loss/div_grad/RealDiv\n",
      "gradients/Loss/div_grad/Sum\n",
      "gradients/Loss/div_grad/Reshape\n",
      "gradients/Loss/div_grad/Neg\n",
      "gradients/Loss/div_grad/RealDiv_1\n",
      "gradients/Loss/div_grad/RealDiv_2\n",
      "gradients/Loss/div_grad/mul\n",
      "gradients/Loss/div_grad/Sum_1\n",
      "gradients/Loss/div_grad/Reshape_1\n",
      "gradients/Loss/div_grad/tuple/group_deps\n",
      "gradients/Loss/div_grad/tuple/control_dependency\n",
      "gradients/Loss/div_grad/tuple/control_dependency_1\n",
      "gradients/Loss/Square_grad/mul/x\n",
      "gradients/Loss/Square_grad/mul\n",
      "gradients/Loss/Square_grad/mul_1\n",
      "gradients/Loss/sub_grad/Shape\n",
      "gradients/Loss/sub_grad/Shape_1\n",
      "gradients/Loss/sub_grad/BroadcastGradientArgs\n",
      "gradients/Loss/sub_grad/Sum\n",
      "gradients/Loss/sub_grad/Reshape\n",
      "gradients/Loss/sub_grad/Sum_1\n",
      "gradients/Loss/sub_grad/Neg\n",
      "gradients/Loss/sub_grad/Reshape_1\n",
      "gradients/Loss/sub_grad/tuple/group_deps\n",
      "gradients/Loss/sub_grad/tuple/control_dependency\n",
      "gradients/Loss/sub_grad/tuple/control_dependency_1\n",
      "gradients/NN/div_grad/Shape\n",
      "gradients/NN/div_grad/Shape_1\n",
      "gradients/NN/div_grad/BroadcastGradientArgs\n",
      "gradients/NN/div_grad/RealDiv\n",
      "gradients/NN/div_grad/Sum\n",
      "gradients/NN/div_grad/Reshape\n",
      "gradients/NN/div_grad/Neg\n",
      "gradients/NN/div_grad/RealDiv_1\n",
      "gradients/NN/div_grad/RealDiv_2\n",
      "gradients/NN/div_grad/mul\n",
      "gradients/NN/div_grad/Sum_1\n",
      "gradients/NN/div_grad/Reshape_1\n",
      "gradients/NN/div_grad/tuple/group_deps\n",
      "gradients/NN/div_grad/tuple/control_dependency\n",
      "gradients/NN/div_grad/tuple/control_dependency_1\n",
      "gradients/NN/Add_grad/Shape\n",
      "gradients/NN/Add_grad/Shape_1\n",
      "gradients/NN/Add_grad/BroadcastGradientArgs\n",
      "gradients/NN/Add_grad/Sum\n",
      "gradients/NN/Add_grad/Reshape\n",
      "gradients/NN/Add_grad/Sum_1\n",
      "gradients/NN/Add_grad/Reshape_1\n",
      "gradients/NN/Add_grad/tuple/group_deps\n",
      "gradients/NN/Add_grad/tuple/control_dependency\n",
      "gradients/NN/Add_grad/tuple/control_dependency_1\n",
      "gradients/NN/Relu_grad/ReluGrad\n",
      "gradients/NN/Relu_1_grad/ReluGrad\n",
      "gradients/NN/add_grad/Shape\n",
      "gradients/NN/add_grad/Shape_1\n",
      "gradients/NN/add_grad/BroadcastGradientArgs\n",
      "gradients/NN/add_grad/Sum\n",
      "gradients/NN/add_grad/Reshape\n",
      "gradients/NN/add_grad/Sum_1\n",
      "gradients/NN/add_grad/Reshape_1\n",
      "gradients/NN/add_grad/tuple/group_deps\n",
      "gradients/NN/add_grad/tuple/control_dependency\n",
      "gradients/NN/add_grad/tuple/control_dependency_1\n",
      "gradients/NN/add_1_grad/Shape\n",
      "gradients/NN/add_1_grad/Shape_1\n",
      "gradients/NN/add_1_grad/BroadcastGradientArgs\n",
      "gradients/NN/add_1_grad/Sum\n",
      "gradients/NN/add_1_grad/Reshape\n",
      "gradients/NN/add_1_grad/Sum_1\n",
      "gradients/NN/add_1_grad/Reshape_1\n",
      "gradients/NN/add_1_grad/tuple/group_deps\n",
      "gradients/NN/add_1_grad/tuple/control_dependency\n",
      "gradients/NN/add_1_grad/tuple/control_dependency_1\n",
      "gradients/NN/MatMul_grad/MatMul\n",
      "gradients/NN/MatMul_grad/MatMul_1\n",
      "gradients/NN/MatMul_grad/tuple/group_deps\n",
      "gradients/NN/MatMul_grad/tuple/control_dependency\n",
      "gradients/NN/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/NN/MatMul_1_grad/MatMul\n",
      "gradients/NN/MatMul_1_grad/MatMul_1\n",
      "gradients/NN/MatMul_1_grad/tuple/group_deps\n",
      "gradients/NN/MatMul_1_grad/tuple/control_dependency\n",
      "gradients/NN/MatMul_1_grad/tuple/control_dependency_1\n",
      "beta1_power/initial_value\n",
      "beta1_power\n",
      "beta1_power/Assign\n",
      "beta1_power/read\n",
      "beta2_power/initial_value\n",
      "beta2_power\n",
      "beta2_power/Assign\n",
      "beta2_power/read\n",
      "zeros\n",
      "NN/W1/Adam\n",
      "NN/W1/Adam/Assign\n",
      "NN/W1/Adam/read\n",
      "zeros_1\n",
      "NN/W1/Adam_1\n",
      "NN/W1/Adam_1/Assign\n",
      "NN/W1/Adam_1/read\n",
      "zeros_2\n",
      "NN/b1/Adam\n",
      "NN/b1/Adam/Assign\n",
      "NN/b1/Adam/read\n",
      "zeros_3\n",
      "NN/b1/Adam_1\n",
      "NN/b1/Adam_1/Assign\n",
      "NN/b1/Adam_1/read\n",
      "zeros_4\n",
      "NN/W2/Adam\n",
      "NN/W2/Adam/Assign\n",
      "NN/W2/Adam/read\n",
      "zeros_5\n",
      "NN/W2/Adam_1\n",
      "NN/W2/Adam_1/Assign\n",
      "NN/W2/Adam_1/read\n",
      "zeros_6\n",
      "NN/b2/Adam\n",
      "NN/b2/Adam/Assign\n",
      "NN/b2/Adam/read\n",
      "zeros_7\n",
      "NN/b2/Adam_1\n",
      "NN/b2/Adam_1/Assign\n",
      "NN/b2/Adam_1/read\n",
      "Adam/learning_rate\n",
      "Adam/beta1\n",
      "Adam/beta2\n",
      "Adam/epsilon\n",
      "Adam/update_NN/W1/ApplyAdam\n",
      "Adam/update_NN/b1/ApplyAdam\n",
      "Adam/update_NN/W2/ApplyAdam\n",
      "Adam/update_NN/b2/ApplyAdam\n",
      "Adam/mul\n",
      "Adam/Assign\n",
      "Adam/mul_1\n",
      "Adam/Assign_1\n",
      "Adam\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/RestoreV2_12/tensor_names\n",
      "save/RestoreV2_12/shape_and_slices\n",
      "save/RestoreV2_12\n",
      "save/Assign_12\n",
      "save/RestoreV2_13/tensor_names\n",
      "save/RestoreV2_13/shape_and_slices\n",
      "save/RestoreV2_13\n",
      "save/Assign_13\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-  \n",
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "  \n",
    "  \n",
    "with tf.variable_scope('Placeholder'):  \n",
    "    inputs_placeholder = tf.placeholder(tf.float32, name='inputs_placeholder', shape=[None, 10])  \n",
    "    labels_placeholder = tf.placeholder(tf.float32, name='labels_placeholder', shape=[None, 1])  \n",
    "  \n",
    "with tf.variable_scope('NN'):  \n",
    "    W1 = tf.get_variable('W1', shape=[10, 1], initializer=tf.random_normal_initializer(stddev=1e-1))  \n",
    "    b1 = tf.get_variable('b1', shape=[1], initializer=tf.constant_initializer(0.1))  \n",
    "    W2 = tf.get_variable('W2', shape=[10, 1], initializer=tf.random_normal_initializer(stddev=1e-1))  \n",
    "    b2 = tf.get_variable('b2', shape=[1], initializer=tf.constant_initializer(0.1))  \n",
    "  \n",
    "    a = tf.nn.relu(tf.matmul(inputs_placeholder, W1) + b1)  \n",
    "    a2 = tf.nn.relu(tf.matmul(inputs_placeholder, W2) + b2)  \n",
    "  \n",
    "    y = tf.div(tf.add(a, a2), 2)  \n",
    "  \n",
    "with tf.variable_scope('Loss'):  \n",
    "    loss = tf.reduce_sum(tf.square(y - labels_placeholder) / 2)  \n",
    "  \n",
    "with tf.variable_scope('Accuracy'):  \n",
    "    predictions = tf.greater(y, 0.5, name=\"predictions\")  \n",
    "    correct_predictions = tf.equal(predictions, tf.cast(labels_placeholder, tf.bool), name=\"correct_predictions\")  \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))  \n",
    "  \n",
    "  \n",
    "adam = tf.train.AdamOptimizer(learning_rate=1e-3)  \n",
    "train_op = adam.minimize(loss)  \n",
    "  \n",
    "# generate_data  \n",
    "inputs = np.random.choice(10, size=[10000, 10])  \n",
    "labels = (np.sum(inputs, axis=1) > 45).reshape(-1, 1).astype(np.float32)  \n",
    "print('inputs.shape:', inputs.shape)  \n",
    "print('labels.shape:', labels.shape)  \n",
    "  \n",
    "  \n",
    "test_inputs = np.random.choice(10, size=[100, 10])  \n",
    "test_labels = (np.sum(test_inputs, axis=1) > 45).reshape(-1, 1).astype(np.float32)  \n",
    "print('test_inputs.shape:', test_inputs.shape)  \n",
    "print('test_labels.shape:', test_labels.shape)  \n",
    "  \n",
    "batch_size = 32  \n",
    "epochs = 10  \n",
    "  \n",
    "batches = []  \n",
    "print(\"%d items in batch of %d gives us %d full batches and %d batches of %d items\" % (  \n",
    "    len(inputs),  \n",
    "    batch_size,  \n",
    "    len(inputs) // batch_size,  \n",
    "    batch_size - len(inputs) // batch_size,  \n",
    "    len(inputs) - (len(inputs) // batch_size) * 32)  \n",
    ")  \n",
    "for i in range(len(inputs) // batch_size):  \n",
    "    batch = [ inputs[batch_size*i:batch_size*i+batch_size], labels[batch_size*i:batch_size*i+batch_size] ]  \n",
    "    batches.append(list(batch))  \n",
    "if (i + 1) * batch_size < len(inputs):  \n",
    "    batch = [ inputs[batch_size*(i + 1):],labels[batch_size*(i + 1):] ]  \n",
    "    batches.append(list(batch))  \n",
    "print(\"Number of batches: %d\" % len(batches))  \n",
    "print(\"Size of full batch: %d\" % len(batches[0]))  \n",
    "print(\"Size if final batch: %d\" % len(batches[-1]))  \n",
    "  \n",
    "global_count = 0  \n",
    "  \n",
    "with tf.Session() as sess:  \n",
    "#sv = tf.train.Supervisor()  \n",
    "#with sv.managed_session() as sess:  \n",
    "    sess.run(tf.initialize_all_variables())  \n",
    "    for i in range(epochs):  \n",
    "        for batch in batches:  \n",
    "            # print(batch[0].shape, batch[1].shape)  \n",
    "            train_loss , _= sess.run([loss, train_op], feed_dict={  \n",
    "                inputs_placeholder: batch[0],  \n",
    "                labels_placeholder: batch[1]  \n",
    "            })  \n",
    "            # print('train_loss: %d' % train_loss)  \n",
    "  \n",
    "            if global_count % 100 == 0:  \n",
    "                acc = sess.run(accuracy, feed_dict={  \n",
    "                    inputs_placeholder: test_inputs,  \n",
    "                    labels_placeholder: test_labels  \n",
    "                })  \n",
    "                print('accuracy: %f' % acc)  \n",
    "            global_count += 1  \n",
    "  \n",
    "    acc = sess.run(accuracy, feed_dict={  \n",
    "        inputs_placeholder: test_inputs,  \n",
    "        labels_placeholder: test_labels  \n",
    "    })  \n",
    "    print(\"final accuracy: %f\" % acc)  \n",
    "    #在session当中就要将模型进行保存  \n",
    "    saver = tf.train.Saver()  \n",
    "    last_chkp = saver.save(sess, 'freeze/graph.chkp')  \n",
    "    #sv.saver.save(sess, 'results/graph.chkp')  \n",
    "  \n",
    "for op in tf.get_default_graph().get_operations():  \n",
    "    print(op.name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mcontinus\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model.ckpt-100.data-00000-of-00001\r\n",
      "│   ├── model.ckpt-100.index\r\n",
      "│   ├── model.ckpt-100.meta\r\n",
      "│   ├── model.ckpt-50.data-00000-of-00001\r\n",
      "│   ├── model.ckpt-50.index\r\n",
      "│   └── model.ckpt-50.meta\r\n",
      "├── \u001b[01;34mfreeze\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── graph.chkp.data-00000-of-00001\r\n",
      "│   ├── graph.chkp.index\r\n",
      "│   └── graph.chkp.meta\r\n",
      "├── \u001b[01;34mmodel\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model3.ckpt.data-00000-of-00001\r\n",
      "│   ├── model3.ckpt.index\r\n",
      "│   └── model3.ckpt.meta\r\n",
      "├── \u001b[01;34mmy_net\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── save_net.ckpt.data-00000-of-00001\r\n",
      "│   ├── save_net.ckpt.index\r\n",
      "│   └── save_net.ckpt.meta\r\n",
      "└── TF_zhj.ipynb\r\n",
      "\r\n",
      "4 directories, 20 files\r\n"
     ]
    }
   ],
   "source": [
    "! tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：\n",
    ".data:存放的是权重参数\n",
    ".meta:存放的是图和metadata,metadata是其他配置的数据\n",
    "如果想将我们的模型固化，让别人能够使用，我们仅仅需要的是图和参数，metadata是不需要的\n",
    "二、综合上述几个文件,生成可以使用的模型的步骤如下：\n",
    "\n",
    "1、恢复我们保存的图\n",
    "2、开启一个Session，然后载入该图要求的权重\n",
    "3、删除对预测无关的metadata\n",
    "4、将处理好的模型序列化之后保存\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n",
      "Converted 4 variables to const ops.\n",
      "20 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, argparse  \n",
    "import tensorflow as tf  \n",
    "from tensorflow.python.framework import graph_util  \n",
    "  \n",
    "#dir = os.path.dirname(os.path.realpath(__file__))  \n",
    "  \n",
    "def freeze_graph(model_folder):  \n",
    "    # We retrieve our checkpoint fullpath  \n",
    "    checkpoint = tf.train.get_checkpoint_state(model_folder)  \n",
    "    input_checkpoint = checkpoint.model_checkpoint_path  \n",
    "      \n",
    "    # We precise the file fullname of our freezed graph  \n",
    "    absolute_model_folder = \"/\".join(input_checkpoint.split('/')[:-1])  \n",
    "    output_graph = absolute_model_folder + \"/frozen_model.pb\"  \n",
    "  \n",
    "    # Before exporting our graph, we need to precise what is our output node  \n",
    "    # this variables is plural, because you can have multiple output nodes  \n",
    "    #freeze之前必须明确哪个是输出结点,也就是我们要得到推论结果的结点  \n",
    "    #输出结点可以看我们模型的定义  \n",
    "    #只有定义了输出结点,freeze才会把得到输出结点所必要的结点都保存下来,或者哪些结点可以丢弃  \n",
    "    #所以,output_node_names必须根据不同的网络进行修改  \n",
    "    output_node_names = \"Accuracy/predictions\"  \n",
    "  \n",
    "    # We clear the devices, to allow TensorFlow to control on the loading where it wants operations to be calculated  \n",
    "    clear_devices = True  \n",
    "      \n",
    "    # We import the meta graph and retrive a Saver  \n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)  \n",
    "  \n",
    "    # We retrieve the protobuf graph definition  \n",
    "    graph = tf.get_default_graph()  \n",
    "    input_graph_def = graph.as_graph_def()  \n",
    "  \n",
    "    #We start a session and restore the graph weights  \n",
    "    #这边已经将训练好的参数加载进来,也即最后保存的模型是有图,并且图里面已经有参数了,所以才叫做是frozen  \n",
    "    #相当于将参数已经固化在了图当中   \n",
    "    with tf.Session() as sess:  \n",
    "        saver.restore(sess, input_checkpoint)  \n",
    "  \n",
    "        # We use a built-in TF helper to export variables to constant  \n",
    "        output_graph_def = graph_util.convert_variables_to_constants(  \n",
    "            sess,   \n",
    "            input_graph_def,   \n",
    "            output_node_names.split(\",\") # We split on comma for convenience  \n",
    "        )   \n",
    "  \n",
    "        # Finally we serialize and dump the output graph to the filesystem  \n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:  \n",
    "            f.write(output_graph_def.SerializeToString())  \n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))  \n",
    "  \n",
    "  \n",
    "if __name__ == '__main__':  \n",
    "    #parser = argparse.ArgumentParser()  \n",
    "    #parser.add_argument(\"--model_folder\", type=str, help=\"Model folder to export\")  \n",
    "    #args = parser.parse_args()  \n",
    "    #freeze_graph(args.model_folder)\n",
    "    freeze_graph( 'freeze/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mcontinus\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model.ckpt-100.data-00000-of-00001\r\n",
      "│   ├── model.ckpt-100.index\r\n",
      "│   ├── model.ckpt-100.meta\r\n",
      "│   ├── model.ckpt-50.data-00000-of-00001\r\n",
      "│   ├── model.ckpt-50.index\r\n",
      "│   └── model.ckpt-50.meta\r\n",
      "├── \u001b[01;34mfreeze\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── frozen_model.pb\r\n",
      "│   ├── graph.chkp.data-00000-of-00001\r\n",
      "│   ├── graph.chkp.index\r\n",
      "│   └── graph.chkp.meta\r\n",
      "├── \u001b[01;34mmodel\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model3.ckpt.data-00000-of-00001\r\n",
      "│   ├── model3.ckpt.index\r\n",
      "│   └── model3.ckpt.meta\r\n",
      "├── \u001b[01;34mmy_net\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── save_net.ckpt.data-00000-of-00001\r\n",
      "│   ├── save_net.ckpt.index\r\n",
      "│   └── save_net.ckpt.meta\r\n",
      "└── TF_zhj.ipynb\r\n",
      "\r\n",
      "4 directories, 21 files\r\n"
     ]
    }
   ],
   "source": [
    "! tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载freeze后的模型,注意该模型已经是包含图和相应的参数了.所以,我们不需要再加载参数进来.也即该模型加载进来已经是可以使用了.\n",
    "说明：\n",
    "1、在预测的过程中,当把freeze后的模型加载进来后,我们只需要定义好输入的tensor和目标tensor即可\n",
    "\n",
    "2、在这里要注意一下tensor_name和ops_name,\n",
    "\n",
    "注意prefix/Placeholder/inputs_placeholder仅仅是操作的名字,prefix/Placeholder/inputs_placeholder:0才是tensor的名字\n",
    "\n",
    "x = graph.get_tensor_by_name('prefix/Placeholder/inputs_placeholder:0')一定要使用tensor的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'prefix/Placeholder/inputs_placeholder', (<tf.Tensor 'prefix/Placeholder/inputs_placeholder:0' shape=<unknown> dtype=float32>,))\n",
      "(u'prefix/NN/W1', (<tf.Tensor 'prefix/NN/W1:0' shape=(10, 1) dtype=float32>,))\n",
      "(u'prefix/NN/W1/read', (<tf.Tensor 'prefix/NN/W1/read:0' shape=(10, 1) dtype=float32>,))\n",
      "(u'prefix/NN/b1', (<tf.Tensor 'prefix/NN/b1:0' shape=(1,) dtype=float32>,))\n",
      "(u'prefix/NN/b1/read', (<tf.Tensor 'prefix/NN/b1/read:0' shape=(1,) dtype=float32>,))\n",
      "(u'prefix/NN/W2', (<tf.Tensor 'prefix/NN/W2:0' shape=(10, 1) dtype=float32>,))\n",
      "(u'prefix/NN/W2/read', (<tf.Tensor 'prefix/NN/W2/read:0' shape=(10, 1) dtype=float32>,))\n",
      "(u'prefix/NN/b2', (<tf.Tensor 'prefix/NN/b2:0' shape=(1,) dtype=float32>,))\n",
      "(u'prefix/NN/b2/read', (<tf.Tensor 'prefix/NN/b2/read:0' shape=(1,) dtype=float32>,))\n",
      "(u'prefix/NN/MatMul', (<tf.Tensor 'prefix/NN/MatMul:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/add', (<tf.Tensor 'prefix/NN/add:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/Relu', (<tf.Tensor 'prefix/NN/Relu:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/MatMul_1', (<tf.Tensor 'prefix/NN/MatMul_1:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/add_1', (<tf.Tensor 'prefix/NN/add_1:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/Relu_1', (<tf.Tensor 'prefix/NN/Relu_1:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/Add', (<tf.Tensor 'prefix/NN/Add:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/div/y', (<tf.Tensor 'prefix/NN/div/y:0' shape=() dtype=float32>,))\n",
      "(u'prefix/NN/div', (<tf.Tensor 'prefix/NN/div:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/Accuracy/predictions/y', (<tf.Tensor 'prefix/Accuracy/predictions/y:0' shape=() dtype=float32>,))\n",
      "(u'prefix/Accuracy/predictions', (<tf.Tensor 'prefix/Accuracy/predictions:0' shape=(?, 1) dtype=bool>,))\n",
      "[[False]]\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-  \n",
    "import argparse   \n",
    "import tensorflow as tf  \n",
    "  \n",
    "def load_graph(frozen_graph_filename):  \n",
    "    # We parse the graph_def file  \n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:  \n",
    "        graph_def = tf.GraphDef()  \n",
    "        graph_def.ParseFromString(f.read())  \n",
    "  \n",
    "    # We load the graph_def in the default graph  \n",
    "    with tf.Graph().as_default() as graph:  \n",
    "        tf.import_graph_def(  \n",
    "            graph_def,   \n",
    "            input_map=None,   \n",
    "            return_elements=None,   \n",
    "            name=\"prefix\",   \n",
    "            op_dict=None,   \n",
    "            producer_op_list=None  \n",
    "        )  \n",
    "    return graph  \n",
    "  \n",
    "if __name__ == '__main__':  \n",
    "    #parser = argparse.ArgumentParser()  \n",
    "    #parser.add_argument(\"--frozen_model_filename\", default=\"freeze/frozen_model.pb\", type=str, help=\"Frozen model file to import\")  \n",
    "    #args = parser.parse_args()  \n",
    "    #加载已经将参数固化后的图  \n",
    "    #graph = load_graph(args.frozen_model_filename)  \n",
    "     graph = load_graph(\"freeze/frozen_model.pb\") \n",
    "    # We can list operations  \n",
    "    #op.values() gives you a list of tensors it produces  \n",
    "    #op.name gives you the name  \n",
    "    #输入,输出结点也是operation,所以,我们可以得到operation的名字  \n",
    "     for op in graph.get_operations():  \n",
    "        print(op.name,op.values())  \n",
    "        # prefix/Placeholder/inputs_placeholder  \n",
    "        # ...  \n",
    "        # prefix/Accuracy/predictions  \n",
    "    #操作有:prefix/Placeholder/inputs_placeholder  \n",
    "    #操作有:prefix/Accuracy/predictions  \n",
    "    #为了预测,我们需要找到我们需要feed的tensor,那么就需要该tensor的名字  \n",
    "    #注意prefix/Placeholder/inputs_placeholder仅仅是操作的名字,prefix/Placeholder/inputs_placeholder:0才是tensor的名字  \n",
    "     x = graph.get_tensor_by_name('prefix/Placeholder/inputs_placeholder:0')  \n",
    "     y = graph.get_tensor_by_name('prefix/Accuracy/predictions:0')  \n",
    "          \n",
    "     with tf.Session(graph=graph) as sess:  \n",
    "        y_out = sess.run(y, feed_dict={  \n",
    "            x: [[3, 5, 7, 4, 5, 1, 1, 1, 1, 1]] # < 45  \n",
    "        })  \n",
    "        print(y_out) # [[ 0.]] Yay!  \n",
    "     print (\"finish\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要获取图中ops的名字和对应的tensor的名字,可用如下的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'prefix/Placeholder/inputs_placeholder', (<tf.Tensor 'prefix/Placeholder/inputs_placeholder:0' shape=<unknown> dtype=float32>,))\n",
      "(u'prefix/NN/W1', (<tf.Tensor 'prefix/NN/W1:0' shape=(10, 1) dtype=float32>,))\n",
      "(u'prefix/NN/W1/read', (<tf.Tensor 'prefix/NN/W1/read:0' shape=(10, 1) dtype=float32>,))\n",
      "(u'prefix/NN/b1', (<tf.Tensor 'prefix/NN/b1:0' shape=(1,) dtype=float32>,))\n",
      "(u'prefix/NN/b1/read', (<tf.Tensor 'prefix/NN/b1/read:0' shape=(1,) dtype=float32>,))\n",
      "(u'prefix/NN/W2', (<tf.Tensor 'prefix/NN/W2:0' shape=(10, 1) dtype=float32>,))\n",
      "(u'prefix/NN/W2/read', (<tf.Tensor 'prefix/NN/W2/read:0' shape=(10, 1) dtype=float32>,))\n",
      "(u'prefix/NN/b2', (<tf.Tensor 'prefix/NN/b2:0' shape=(1,) dtype=float32>,))\n",
      "(u'prefix/NN/b2/read', (<tf.Tensor 'prefix/NN/b2/read:0' shape=(1,) dtype=float32>,))\n",
      "(u'prefix/NN/MatMul', (<tf.Tensor 'prefix/NN/MatMul:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/add', (<tf.Tensor 'prefix/NN/add:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/Relu', (<tf.Tensor 'prefix/NN/Relu:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/MatMul_1', (<tf.Tensor 'prefix/NN/MatMul_1:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/add_1', (<tf.Tensor 'prefix/NN/add_1:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/Relu_1', (<tf.Tensor 'prefix/NN/Relu_1:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/Add', (<tf.Tensor 'prefix/NN/Add:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/NN/div/y', (<tf.Tensor 'prefix/NN/div/y:0' shape=() dtype=float32>,))\n",
      "(u'prefix/NN/div', (<tf.Tensor 'prefix/NN/div:0' shape=(?, 1) dtype=float32>,))\n",
      "(u'prefix/Accuracy/predictions/y', (<tf.Tensor 'prefix/Accuracy/predictions/y:0' shape=() dtype=float32>,))\n",
      "(u'prefix/Accuracy/predictions', (<tf.Tensor 'prefix/Accuracy/predictions:0' shape=(?, 1) dtype=bool>,))\n"
     ]
    }
   ],
   "source": [
    "# We can list operations  \n",
    "#op.values() gives you a list of tensors it produces  \n",
    "#op.name gives you the name  \n",
    "#输入,输出结点也是operation,所以,我们可以得到operation的名字  \n",
    "for op in graph.get_operations():  \n",
    "    print(op.name,op.values())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training utilities\n",
    "http://www.tensorfly.cn/tfdoc/api_docs/python/train.html#global_step\n",
    "\n",
    "tf.train.write_graph(graph_def, logdir, name, as_text=True)\n",
    "\n",
    "Writes a graph proto on disk.\n",
    "\n",
    "The graph is written as a binary proto unless as_text is True.\n",
    "\n",
    "v = tf.Variable(0, name='my_variable')\n",
    "sess = tf.Session()\n",
    "tf.train.write_graph(sess.graph_def, './write_graph', 'train.pbtxt')\n",
    "\n",
    "Args:\n",
    "graph_def: A GraphDef protocol buffer.\n",
    "logdir: Directory where to write the graph.\n",
    "name: Filename for the graph.\n",
    "as_text: If True, writes the graph as an ASCII proto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./write_graph/train.pb'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable(0, name='my_variable')\n",
    "sess = tf.Session()\n",
    "tf.train.write_graph(sess.graph_def, './write_graph', 'train.pb',as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mcontinus\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model.ckpt-100.data-00000-of-00001\r\n",
      "│   ├── model.ckpt-100.index\r\n",
      "│   ├── model.ckpt-100.meta\r\n",
      "│   ├── model.ckpt-50.data-00000-of-00001\r\n",
      "│   ├── model.ckpt-50.index\r\n",
      "│   └── model.ckpt-50.meta\r\n",
      "├── \u001b[01;34mfreeze\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── frozen_model.pb\r\n",
      "│   ├── graph.chkp.data-00000-of-00001\r\n",
      "│   ├── graph.chkp.index\r\n",
      "│   └── graph.chkp.meta\r\n",
      "├── \u001b[01;34mlogs\u001b[00m\r\n",
      "│   └── events.out.tfevents.1491728464.haijunz-ThinkPad-T420\r\n",
      "├── \u001b[01;34mmodel\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── model3.ckpt.data-00000-of-00001\r\n",
      "│   ├── model3.ckpt.index\r\n",
      "│   └── model3.ckpt.meta\r\n",
      "├── \u001b[01;34mmy_net\u001b[00m\r\n",
      "│   ├── checkpoint\r\n",
      "│   ├── save_net.ckpt.data-00000-of-00001\r\n",
      "│   ├── save_net.ckpt.index\r\n",
      "│   └── save_net.ckpt.meta\r\n",
      "├── TF_zhj.ipynb\r\n",
      "└── \u001b[01;34mwrite_graph\u001b[00m\r\n",
      "    ├── train.pb\r\n",
      "    └── train.pbtxt\r\n",
      "\r\n",
      "6 directories, 24 files\r\n"
     ]
    }
   ],
   "source": [
    "! tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = tf.Variable(0, name='my_variable')\n",
    "sess = tf.Session()\n",
    "tf.train.write_graph(sess.graph_def, './write_graph', 'train.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "meta_graph\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/meta_graph.md\n",
    "\n",
    "1.first build an inference graph, export it as a meta graph\n",
    "2. Then later import it and extend it to a training graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np  \n",
    "import math\n",
    "  \n",
    "images = tf.constant(1.2, tf.float32, shape=[100, 28])\n",
    "with tf.name_scope(\"hidden1\"):\n",
    "  weights = tf.Variable(\n",
    "      tf.truncated_normal([28, 128],\n",
    "                          stddev=1.0 / math.sqrt(float(28))),\n",
    "      name=\"weights\")\n",
    "  biases = tf.Variable(tf.zeros([128]),\n",
    "                       name=\"biases\")\n",
    "  hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "# Hidden 2\n",
    "with tf.name_scope(\"hidden2\"):\n",
    "  weights = tf.Variable(\n",
    "      tf.truncated_normal([128, 32],\n",
    "                          stddev=1.0 / math.sqrt(float(128))),\n",
    "      name=\"weights\")\n",
    "  biases = tf.Variable(tf.zeros([32]),\n",
    "                       name=\"biases\")\n",
    "  hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "# Linear\n",
    "with tf.name_scope(\"softmax_linear\"):\n",
    "  weights = tf.Variable(\n",
    "      tf.truncated_normal([32, 10],\n",
    "                          stddev=1.0 / math.sqrt(float(32))),\n",
    "      name=\"weights\")\n",
    "  biases = tf.Variable(tf.zeros([10]),\n",
    "                       name=\"biases\")\n",
    "  logits = tf.matmul(hidden2, weights) + biases\n",
    "  tf.add_to_collection(\"logits\", logits)\n",
    "\n",
    "  init_all_op = tf.global_variables_initializer()\n",
    "  #init_all_op =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Initializes all the variables.\n",
    "  sess.run(init_all_op)\n",
    "  # Runs to logit.\n",
    "  sess.run(logits)\n",
    "  # Creates a saver.\n",
    "  saver0 = tf.train.Saver()\n",
    "  saver0.save(sess, 'meta_graph/my-model-10000')\n",
    "  # Generates MetaGraphDef.\n",
    "  saver0.export_meta_graph('meta_graph/my-model-10000.meta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  new_saver = tf.train.import_meta_graph('meta_graph/my-model-10000.meta')\n",
    "  new_saver.restore(sess, 'meta_graph/my-model-10000')\n",
    "  # Addes loss and train.\n",
    "  labels = tf.constant(0, tf.int32, shape=[100], name=\"labels\")\n",
    "  batch_size = tf.size(labels)\n",
    "  labels = tf.expand_dims(labels, 1)\n",
    "  indices = tf.expand_dims(tf.range(0, batch_size), 1)\n",
    "  concated = tf.concat([indices, labels], 1)\n",
    "  onehot_labels = tf.sparse_to_dense(\n",
    "      concated, tf.stack([batch_size, 10]), 1.0, 0.0)\n",
    "  logits = tf.get_collection(\"logits\")[0]\n",
    "  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "      labels=onehot_labels, logits=logits, name=\"xentropy\")\n",
    "  loss = tf.reduce_mean(cross_entropy, name=\"xentropy_mean\")\n",
    "\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  # Creates the gradient descent optimizer with the given learning rate.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "  # Runs train_op.\n",
    "  train_op = optimizer.minimize(loss)\n",
    "  sess.run(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.csdn.net/qiqiaiairen/article/details/53184216\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# build graph and run\n",
      "WARNING:tensorflow:From <ipython-input-1-1ddf21043fa7>:21: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "load graph\n",
      "map variables\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:From <ipython-input-1-1ddf21043fa7>:35: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "load data\n",
      "[100 101 102 103 104 105 106 107 108 109]\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "data = np.arange(10,dtype=np.int32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(\"# build graph and run\")\n",
    "  input1= tf.placeholder(tf.int32, [10], name=\"input\")\n",
    "  output1= tf.add(input1, tf.constant(100,dtype=tf.int32), name=\"output\") #  data depends on the input data\n",
    "  saved_result= tf.Variable(data, name=\"saved_result\")\n",
    "  do_save=tf.assign(saved_result,output1)\n",
    "  \n",
    "  os.system(\"rm -rf ./graph_data/\")\n",
    "  tf.train.write_graph(sess.graph_def, \"./graph_data/\", \"test.pb\", False) #proto\n",
    "  #tf.initialize_all_variables()\n",
    "  #r=tf.global_variables_initializer()\n",
    "  # now set the data:\n",
    "  result,_=sess.run([output1,do_save], {input1: data}) # calculate output1 and assign to 'saved_result'\n",
    "  saver = tf.train.Saver(tf.all_variables())\n",
    "  saver.save(sess,\"./graph_data/checkpoint.data\")\n",
    "\n",
    "with tf.Session() as persisted_sess:\n",
    "  print(\"load graph\")\n",
    "  with gfile.FastGFile(\"./graph_data/test.pb\",'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    persisted_sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "  print(\"map variables\")\n",
    "  persisted_result = persisted_sess.graph.get_tensor_by_name(\"saved_result:0\")\n",
    "  tf.add_to_collection(tf.GraphKeys.VARIABLES,persisted_result)\n",
    "  try:\n",
    "    saver = tf.train.Saver(tf.all_variables()) # 'Saver' misnomer! Better: Persister!\n",
    "  except:pass\n",
    "  print(\"load data\")\n",
    "  saver.restore(persisted_sess, \"./graph_data/checkpoint.data\")  # now OK\n",
    "  print(persisted_result.eval())\n",
    "  print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/34112202/tensorflow-checkpoint-save-and-read\n",
    "https://sec.xiaomi.com/article/13\n",
    "\n",
    "如何保存模型并在模型训练完后查看模型的训练参数？ \n",
    "TensorFlow的checkpoint机制使得其能够同时支持Online Learning和Continuous Learning，首先，通过tf.train.Saver()将训练好的或者训练过程中的模型保存成checkpoint:\n",
    "\n",
    "_, loss_value, step = sess.run([train_op, loss, global_step])\n",
    "saver.save(sess,\"./checkpoint/checkpoint.ckpt\", global_step=step)\n",
    "然后通过restore()函数从本地的checkpoint文件中恢复模型，当然也可以从该点开始继续运行，也就是所谓的Continuous Learning：\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(\"./checkpoint/\")\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    print(\"Continue training from the model {}\".format(ckpt.model_checkpoint_path))\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    _, loss_value, step = sess.run([train_op, loss, global_step])\n",
    "\n",
    "最后通过tf.trainable_variables()获取返回模型中所训练的参数：\n",
    "\n",
    "for var in tf.trainable_varisbles():\n",
    "    print var.name\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "http://blog.csdn.net/lujiandong1/article/details/53222651\n",
    "tensorflow中可视化loss,weight,bias,提供算法调试的信息\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/tf_upgrade.py#L116\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "unbound method to_proto() must be called with Variable instance as first argument (got Tensor instance instead)\n",
      "WARNING:tensorflow:From <ipython-input-3-7fa5baa20001>:59: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7fa5baa20001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# important step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m           \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m           func.__module__, date, instructions)\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     new_func.__doc__ = _add_deprecated_function_notice_to_docstring(\n\u001b[1;32m    119\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36minitialize_all_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m   \u001b[0;34m\"\"\"See `tf.global_variables_initializer`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36mglobal_variables_initializer\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0mAn\u001b[0m \u001b[0mOp\u001b[0m \u001b[0mthat\u001b[0m \u001b[0minitializes\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m   \"\"\"\n\u001b[0;32m-> 1203\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36mvariables_initializer\u001b[0;34m(var_list, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m   \"\"\"\n\u001b[1;32m   1184\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'initializer'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function  \n",
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "  \n",
    "  \n",
    "def add_layer(inputs, in_size, out_size, n_layer, activation_function=None):  \n",
    "    # add one more layer and return the output of this layer  \n",
    "    layer_name = 'layer%s' % n_layer  \n",
    "    with tf.name_scope(layer_name):  \n",
    "        with tf.name_scope('weights'):  \n",
    "            Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')  \n",
    "            #tf.histogram_summary(layer_name + '/weights', Weights)  \n",
    "            tf.summary.histogram(layer_name + '/weights', Weights) \n",
    "        with tf.name_scope('biases'):  \n",
    "            biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')  \n",
    "            #tf.histogram_summary(layer_name + '/biases', biases)  \n",
    "            tf.summary.histogram(layer_name + '/biases', biases)\n",
    "        with tf.name_scope('Wx_plus_b'):  \n",
    "            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)  \n",
    "        if activation_function is None:  \n",
    "            outputs = Wx_plus_b  \n",
    "        else:  \n",
    "            outputs = activation_function(Wx_plus_b, )  \n",
    "        #tf.histogram_summary(layer_name + '/outputs', outputs)  \n",
    "        tf.summary.histogram(layer_name + '/outputs', outputs)  \n",
    "        return outputs  \n",
    "  \n",
    "  \n",
    "# Make up some real data  \n",
    "x_data = np.linspace(-1, 1, 300)[:, np.newaxis]  \n",
    "noise = np.random.normal(0, 0.05, x_data.shape)  \n",
    "y_data = np.square(x_data) - 0.5 + noise  \n",
    "  \n",
    "# define placeholder for inputs to network  \n",
    "with tf.name_scope('inputs'):  \n",
    "    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')  \n",
    "    ys = tf.placeholder(tf.float32, [None, 1], name='y_input')  \n",
    "  \n",
    "# add hidden layer  \n",
    "l1 = add_layer(xs, 1, 10, n_layer=1, activation_function=tf.nn.relu)  \n",
    "# add output layer  \n",
    "prediction = add_layer(l1, 10, 1, n_layer=2, activation_function=None)  \n",
    "  \n",
    "# the error between prediciton and real data  \n",
    "with tf.name_scope('loss'):  \n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),  \n",
    "                                        reduction_indices=[1]))  \n",
    "    #tf.scalar_summary('loss', loss)  \n",
    "    tf.summary.scalar('loss', loss) \n",
    "with tf.name_scope('train'):  \n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)  \n",
    "  \n",
    "sess = tf.Session()  \n",
    "#merged = tf.merge_all_summaries() \n",
    "merged=tf.summary.merge_all()\n",
    "#writer = tf.train.SummaryWriter(\"logs/\", sess.graph)  \n",
    "writer =tf.summary.FileWriter(\"logs/\", sess.graph) \n",
    "# important step  \n",
    "sess.run(tf.initialize_all_variables())  \n",
    "  \n",
    "for i in range(1000):  \n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})  \n",
    "    if i % 50 == 0:  \n",
    "        result = sess.run(merged,  \n",
    "                          feed_dict={xs: x_data, ys: y_data})  \n",
    "        writer.add_summary(result, i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " tensorflow构建网络模型\n",
    " http://blog.csdn.net/lujiandong1/article/details/53217248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-98022f6452b5>:36: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0.177221\n",
      "0.00867744\n",
      "0.00643269\n",
      "0.00523516\n",
      "0.00458893\n",
      "0.00429269\n",
      "0.00414355\n",
      "0.00402482\n",
      "0.00392423\n",
      "0.00383714\n",
      "0.00376122\n",
      "0.00369587\n",
      "0.00363715\n",
      "0.00358822\n",
      "0.00354582\n",
      "0.0035086\n",
      "0.0034749\n",
      "0.0034466\n",
      "0.0034179\n",
      "0.00339362\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function  \n",
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "  \n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):  \n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))  \n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)  \n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases  \n",
    "    if activation_function is None:  \n",
    "        outputs = Wx_plus_b  \n",
    "    else:  \n",
    "        outputs = activation_function(Wx_plus_b)  \n",
    "    return outputs  \n",
    "  \n",
    "# Make up some real data  \n",
    "x_data = np.linspace(-1, 1, 300)[:, np.newaxis]  \n",
    "noise = np.random.normal(0, 0.05, x_data.shape)  \n",
    "y_data = np.square(x_data) - 0.5 + noise  \n",
    "  \n",
    "##plt.scatter(x_data, y_data)  \n",
    "##plt.show()  \n",
    "  \n",
    "# define placeholder for inputs to network  \n",
    "xs = tf.placeholder(tf.float32, [None, 1])  \n",
    "ys = tf.placeholder(tf.float32, [None, 1])  \n",
    "# add hidden layer  \n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)  \n",
    "# add output layer  \n",
    "prediction = add_layer(l1, 10, 1, activation_function=None)  \n",
    "  \n",
    "# the error between prediciton and real data  \n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[1]))  \n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)  \n",
    "# important step  \n",
    "init = tf.initialize_all_variables()  \n",
    "sess= tf.Session()  \n",
    "sess.run(init)  \n",
    "  \n",
    "for i in range(1000):  \n",
    "    # training  \n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})  \n",
    "    if i % 50 == 0:  \n",
    "        # to see the step improvement  \n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-08-RNN2/\n",
    "RNN LSTM 循环神经网络 (分类例子)\n",
    "\n",
    "次我们会使用 RNN 来进行分类的训练 (Classification). 会继续使用到手写数字 MNIST 数据集. 让 RNN 从每张图片的第一行像素读到最后一行, 然后再进行分类判断. 接下来我们导入 MNIST 数据并确定 RNN 的各种参数(hyper-parameters):\n",
    "\n",
    "接着是 cell 中的计算, 有两种途径:\n",
    "\n",
    "使用 tf.nn.rnn(cell, inputs) (不推荐原因http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/). 但是如果使用这种方法, 可以参考这个代码(https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py);\n",
    "使用 tf.nn.dynamic_rnn(cell, inputs) (推荐). 这次的练习将使用这种方式.\n",
    "因 Tensorflow 版本升级原因, state_is_tuple=True 将在之后的版本中变为默认. 对于 lstm 来说, state可被分为(c_state, h_state).\n",
    "\n",
    "如果使用tf.nn.dynamic_rnn(cell, inputs), 我们要确定 inputs 的格式. tf.nn.dynamic_rnn 中的 time_major 参数会针对不同 inputs 格式有不同的值.\n",
    "1.如果 inputs 为 (batches, steps, inputs) ==> time_major=False;\n",
    "2.如果 inputs 为 (steps, batches, inputs) ==> time_major=True;\n",
    "\n",
    "最后是 output_layer 和 return 的值. 因为这个例子的特殊性, 有两种方法可以求得 results.\n",
    "\n",
    "方式一: 直接调用final_state 中的 h_state (final_state[1]) 来进行运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.117188\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.65625\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.734375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.671875\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.84375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.789062\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.90625\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.929688\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.914062\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.929688\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.859375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.929688\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.945312\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.890625\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.953125\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.929688\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.96875\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.945312\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.898438\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.945312\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.929688\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.96875\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.945312\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.984375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.945312\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.9375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.984375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.9375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.992188\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.984375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.945312\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.976562\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.9375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.960938\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.9375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.96875\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.96875\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.96875\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.945312\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "0.984375\n",
      "Model saved in file:  ./model/model3.ckpt\n",
      "Model saved in file:  ./model/model3.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(1)   # set random seed\n",
    "\n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# hyperparameters\n",
    "lr = 0.001                  # learning rate\n",
    "training_iters = 100000     # train step 上限\n",
    "batch_size = 128            \n",
    "n_inputs = 28               # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28                # time steps\n",
    "n_hidden_units = 128        # neurons in hidden layer\n",
    "n_classes = 10              # MNIST classes (0-9 digits)\n",
    "# x y placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# 对 weights biases 初始值的定义\n",
    "weights = {\n",
    "    # shape (28, 128)\n",
    "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n",
    "    # shape (128, 10)\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    # shape (128, )\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
    "    # shape (10, )\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}\n",
    "\n",
    "def RNN(X, weights, biases):\n",
    "    # 原始的 X 是 3 维数据, 我们需要把它变成 2 维数据才能使用 weights 的矩阵乘法\n",
    "    # X ==> (128 batches * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "\n",
    "    # X_in = W*X + b\n",
    "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
    "    # X_in ==> (128 batches, 28 steps, 128 hidden) 换回3维\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "    # 使用 basic LSTM Cell.\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32) # 初始化全零 state\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "    results = tf.matmul(final_state[1], weights['out']) + biases['out']\n",
    "    return results\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# init= tf.initialize_all_variables() # tf 马上就要废弃这种写法\n",
    "# 替换成下面的写法:\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n",
    "        sess.run([train_op], feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        })\n",
    "        if step % 20 == 0:\n",
    "            print(sess.run(accuracy, feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        }))\n",
    "        step += 1\n",
    "        save_path = saver.save(sess, \"./model/model3.ckpt\")\n",
    "        print \"Model saved in file: \", save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-09-RNN3/\n",
    "RNN LSTM (回归例子)这次我们会使用 RNN 来进行回归的训练 (Regression). 会继续使用到自己创建的 sin 曲线预测一条 cos 曲线. 接下来我们先确定 RNN 的各种参数(super-parameters):\n",
    "tf.mul, tf.sub and tf.neg are deprecated in favor of tf.multiply, tf.subtract and tf.negative.\n",
    "\n",
    "使用 Matplotlib 模块来进行可视化过程, 在建立好 model 以后, 设置 plt.ion() 使 plt.show()可以连续显示.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/pymodules/python2.7/matplotlib/backend_bases.py:2407: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cost: ', 18.2259)\n",
      "('cost: ', 3.5294)\n",
      "('cost: ', 1.4404)\n",
      "('cost: ', 1.4068)\n",
      "('cost: ', 0.1946)\n",
      "('cost: ', 0.6116)\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "can't invoke \"update\" command: application has been destroyed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bba1daf84f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 每 0.3 s 刷新一次\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;31m# 打印 cost 结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/pymodules/python2.7/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/pymodules/python2.7/matplotlib/backends/backend_tkagg.pyc\u001b[0m in \u001b[0;36mstart_event_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_event_loop_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m     \u001b[0mstart_event_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_event_loop_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/pymodules/python2.7/matplotlib/backend_bases.pyc\u001b[0m in \u001b[0;36mstart_event_loop_default\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2413\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_looping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_looping\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2416\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/pymodules/python2.7/matplotlib/backends/backend_tkagg.pyc\u001b[0m in \u001b[0;36mflush_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/lib-tk/Tkinter.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;34m\"\"\"Enter event loop until all pending events have been processed by Tcl.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'update'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_idletasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \"\"\"Enter event loop until all idle callbacks have been called. This\n",
      "\u001b[0;31mTclError\u001b[0m: can't invoke \"update\" command: application has been destroyed"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_START = 0     # 建立 batch data 时候的 index\n",
    "TIME_STEPS = 20     # backpropagation through time 的 time_steps\n",
    "BATCH_SIZE = 50     \n",
    "INPUT_SIZE = 1      # sin 数据输入 size\n",
    "OUTPUT_SIZE = 1     # cos 数据输出 size\n",
    "CELL_SIZE = 10      # RNN 的 hidden unit size \n",
    "LR = 0.006          # learning rate\n",
    "def get_batch():\n",
    "    global BATCH_START, TIME_STEPS\n",
    "    # xs shape (50batch, 20steps)\n",
    "    xs = np.arange(BATCH_START, BATCH_START+TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10*np.pi)\n",
    "    seq = np.sin(xs)\n",
    "    res = np.cos(xs)\n",
    "    BATCH_START += TIME_STEPS\n",
    "    # returned seq, res and xs: shape (batch, step, input)\n",
    "    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]\n",
    "class LSTMRNN(object):\n",
    "    def __init__(self, n_steps, input_size, output_size, cell_size, batch_size):\n",
    "        self.n_steps = n_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.cell_size = cell_size\n",
    "        self.batch_size = batch_size\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs')\n",
    "            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys')\n",
    "        with tf.variable_scope('in_hidden'):\n",
    "            self.add_input_layer()\n",
    "        with tf.variable_scope('LSTM_cell'):\n",
    "            self.add_cell()\n",
    "        with tf.variable_scope('out_hidden'):\n",
    "            self.add_output_layer()\n",
    "        with tf.name_scope('cost'):\n",
    "            self.compute_cost()\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)\n",
    "    def add_input_layer(self,):\n",
    "        l_in_x = tf.reshape(self.xs, [-1, self.input_size], name='2_2D')  # (batch*n_step, in_size)\n",
    "        # Ws (in_size, cell_size)\n",
    "        Ws_in = self._weight_variable([self.input_size, self.cell_size])\n",
    "        # bs (cell_size, )\n",
    "        bs_in = self._bias_variable([self.cell_size,])\n",
    "        # l_in_y = (batch * n_steps, cell_size)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in\n",
    "        # reshape l_in_y ==> (batch, n_steps, cell_size)\n",
    "        self.l_in_y = tf.reshape(l_in_y, [-1, self.n_steps, self.cell_size], name='2_3D')\n",
    "    def add_cell(self):\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        with tf.name_scope('initial_state'):\n",
    "            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)\n",
    "    def add_output_layer(self):\n",
    "        # shape = (batch * steps, cell_size)\n",
    "        l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name='2_2D')\n",
    "        Ws_out = self._weight_variable([self.cell_size, self.output_size])\n",
    "        bs_out = self._bias_variable([self.output_size, ])\n",
    "        # shape = (batch * steps, output_size)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out\n",
    "    def compute_cost(self):\n",
    "        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [tf.reshape(self.pred, [-1], name='reshape_pred')],\n",
    "            [tf.reshape(self.ys, [-1], name='reshape_target')],\n",
    "            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],\n",
    "            average_across_timesteps=True,\n",
    "            softmax_loss_function=self.ms_error,\n",
    "            name='losses'\n",
    "        )\n",
    "        with tf.name_scope('average_cost'):\n",
    "            self.cost = tf.div(\n",
    "                tf.reduce_sum(losses, name='losses_sum'),\n",
    "                self.batch_size,\n",
    "                name='average_cost')\n",
    "            tf.summary.scalar('cost', self.cost)\n",
    "\n",
    "    def ms_error(self, y_pre, y_target):\n",
    "        #return tf.square(tf.sub(y_pre, y_target))\n",
    "        return tf.square(tf.subtract(y_pre, y_target))\n",
    "\n",
    "    def _weight_variable(self, shape, name='weights'):\n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=1.,)\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "\n",
    "    def _bias_variable(self, shape, name='biases'):\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    "if __name__ == '__main__':\n",
    "    # 搭建 LSTMRNN 模型\n",
    "    model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)\n",
    "    sess = tf.Session()\n",
    "    # 替换成下面的写法:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    # sess.run(tf.initialize_all_variables()) # tf 马上就要废弃这种写法\n",
    "   # 替换成下面的写法:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    plt.ion()   # 设置连续 plot\n",
    "    plt.show()\n",
    "    # 训练 200 次\n",
    "    for i in range(200):\n",
    "        seq, res, xs = get_batch()  # 提取 batch data\n",
    "        if i == 0:\n",
    "        # 初始化 data\n",
    "            feed_dict = {\n",
    "                    model.xs: seq,\n",
    "                    model.ys: res,\n",
    "            }\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                model.xs: seq,\n",
    "                model.ys: res,\n",
    "                model.cell_init_state: state    # 保持 state 的连续性\n",
    "            }\n",
    "        \n",
    "        # 训练\n",
    "        _, cost, state, pred = sess.run(\n",
    "            [model.train_op, model.cost, model.cell_final_state, model.pred],\n",
    "            feed_dict=feed_dict)\n",
    "        \n",
    "        # plotting\n",
    "        plt.plot(xs[0, :], res[0].flatten(), 'r', xs[0, :], pred.flatten()[:TIME_STEPS], 'b--')\n",
    "        plt.ylim((-1.2, 1.2))\n",
    "        plt.draw()\n",
    "        plt.pause(0.3)  # 每 0.3 s 刷新一次\n",
    "        # 打印 cost 结果\n",
    "        if i % 20 == 0:\n",
    "            print('cost: ', round(cost, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-12-scope/\n",
    "\n",
    "在 Tensorflow 当中有两种途径生成变量 variable, 一种是 tf.get_variable(), 另一种是 tf.Variable(). \n",
    "如果在 tf.name_scope() 的框架下使用这两种方式, 结果会如下.\n",
    "可以看出使用 tf.Variable() 定义的时候, 虽然 name 都一样, 但是为了不重复变量名, Tensorflow 输出的变量名并不是一样的. 所以, 本质上 var2, var21, var22 并不是一样的变量. 而另一方面, 使用tf.get_variable()定义的变量不会被tf.name_scope()当中的名字所影响.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-08dde2103004>:12: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "var1:0\n",
      "[ 1.]\n",
      "a_name_scope/var2:0\n",
      "[ 2.]\n",
      "a_name_scope/var2_1:0\n",
      "[ 2.0999999]\n",
      "a_name_scope/var2_2:0\n",
      "[ 2.20000005]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.name_scope(\"a_name_scope\"):\n",
    "    initializer = tf.constant_initializer(value=1)\n",
    "    var1 = tf.get_variable(name='var1', shape=[1], dtype=tf.float32, initializer=initializer)\n",
    "    var2 = tf.Variable(name='var2', initial_value=[2], dtype=tf.float32)\n",
    "    var21 = tf.Variable(name='var2', initial_value=[2.1], dtype=tf.float32)\n",
    "    var22 = tf.Variable(name='var2', initial_value=[2.2], dtype=tf.float32)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(var1.name)        # var1:0\n",
    "    print(sess.run(var1))   # [ 1.]\n",
    "    print(var2.name)        # a_name_scope/var2:0\n",
    "    print(sess.run(var2))   # [ 2.]\n",
    "    print(var21.name)       # a_name_scope/var2_1:0\n",
    "    print(sess.run(var21))  # [ 2.0999999]\n",
    "    print(var22.name)       # a_name_scope/var2_2:0\n",
    "    print(sess.run(var22))  # [ 2.20000005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要达到重复利用变量的效果, 我们就要使用 tf.variable_scope(), 并搭配 tf.get_variable() 这种方式产生和提取变量. 不像 tf.Variable() 每次都会产生新的变量, tf.get_variable() 如果遇到了同样名字的变量时, 它会单纯的提取这个同样名字的变量(避免产生新变量). 而在重复使用的时候, 一定要在代码中强调 scope.reuse_variables(), 否则系统将会报错, 以为你只是单纯的不小心重复使用到了一个变量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_variable_scope/var3:0\n",
      "[ 3.]\n",
      "a_variable_scope/var3:0\n",
      "[ 3.]\n",
      "a_variable_scope/var4:0\n",
      "[ 4.]\n",
      "a_variable_scope/var4_1:0\n",
      "[ 4.]\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"a_variable_scope\") as scope:\n",
    "    initializer = tf.constant_initializer(value=3)\n",
    "    var3 = tf.get_variable(name='var3', shape=[1], dtype=tf.float32, initializer=initializer)\n",
    "    scope.reuse_variables()\n",
    "    var3_reuse = tf.get_variable(name='var3',)\n",
    "    var4 = tf.Variable(name='var4', initial_value=[4], dtype=tf.float32)\n",
    "    var4_reuse = tf.Variable(name='var4', initial_value=[4], dtype=tf.float32)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(var3.name)            # a_variable_scope/var3:0\n",
    "    print(sess.run(var3))       # [ 3.]\n",
    "    print(var3_reuse.name)      # a_variable_scope/var3:0\n",
    "    print(sess.run(var3_reuse)) # [ 3.]\n",
    "    print(var4.name)            # a_variable_scope/var4:0\n",
    "    print(sess.run(var4))       # [ 4.]\n",
    "    print(var4_reuse.name)      # a_variable_scope/var4_1:0\n",
    "    print(sess.run(var4_reuse)) # [ 4.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-12-scope/\n",
    "RNN 例子的代码在这里, 整个 RNN 的结构已经在这里定义好了. 在 training RNN 和 test RNN 的时候, RNN 的 time_steps 会有不同的取值, 这将会影响到整个 RNN 的结构, 所以导致在 test 的时候, 不能单纯地使用 training 时建立的那个 RNN. 但是 training RNN 和 test RNN 又必须是有同样的 weights biases 的参数. 所以, 这时, 就是使用 reuse variable 的好时机.\n",
    "\n",
    "首先定义training 和 test 的不同参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    batch_size = 20\n",
    "    time_steps = 20\n",
    "    input_size = 10\n",
    "    output_size = 2\n",
    "    cell_size = 11\n",
    "    learning_rate = 0.01\n",
    "\n",
    "\n",
    "class TestConfig(TrainConfig):\n",
    "    time_steps = 1\n",
    "    \n",
    "train_config = TrainConfig()\n",
    "test_config = TestConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后让 train_rnn 和 test_rnn 在同一个 tf.variable_scope('rnn') 之下. 并且定义 scope.reuse_variables(), 使我们能把 train_rnn 的所有 weights, biases 参数全部绑定到 test_rnn 中. 这样, 不管两者的 time_steps 有多不同, 结构有多不同, train_rnn W, b 参数更新成什么样, test_rnn 的参数也更新成什么样."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnn') as scope:\n",
    "    sess = tf.Session()\n",
    "    train_rnn = RNN(train_config)\n",
    "    scope.reuse_variables()\n",
    "    test_rnn = RNN(test_config)\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization 批标准化\n",
    "https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-13-BN/\n",
    "\n",
    "atch normalization 是一种解决深度神经网络层数太多, 而没办法有效前向传递(forward propagate)的问题. 因为每一层的输出值都会有不同的 均值(mean) 和 方差(deviation), 所以输出数据的分布也不一样, 如下图, 从左到右是每一层的输入数据分布, 上排的没有 Batch normalization, 下排的有 Batch normalization.\n",
    "\n",
    "我们以前说过, 为了更有效的学习数据, 我们会对数据预处理, 进行 normalization (请参考我制作的 为什么要特征标准化). 而现在请想象, 我们可以把 “每层输出的值” 都看成 “后面一层所接收的数据”. 对每层都进行一次 normalization 会不会更好呢? 这就是 Batch normalization 方法的由来.\n",
    "https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/pymodules/python2.7/matplotlib/collections.py:548: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == 'face':\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ACTIVATION = tf.nn.relu # 每一层都使用 relu \n",
    "N_LAYERS = 7            # 一共7层隐藏层\n",
    "N_HIDDEN_UNITS = 30     # 每个层隐藏层有 30 个神经元\n",
    "def built_net(xs, ys, norm):\n",
    "    def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "        # 添加层功能\n",
    "        Weights = tf.Variable(tf.random_normal([in_size, out_size], mean=0., stddev=1.))\n",
    "        biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "        Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "        if activation_function is None:\n",
    "            outputs = Wx_plus_b\n",
    "        else:\n",
    "            outputs = activation_function(Wx_plus_b)\n",
    "        return outputs\n",
    "\n",
    "    fix_seed(1)\n",
    "\n",
    "    layers_inputs = [xs]    # 记录每层的 input\n",
    "\n",
    "    # loop 建立所有层\n",
    "    for l_n in range(N_LAYERS):\n",
    "        layer_input = layers_inputs[l_n]\n",
    "        in_size = layers_inputs[l_n].get_shape()[1].value\n",
    "\n",
    "        output = add_layer(\n",
    "            layer_input,    # input\n",
    "            in_size,        # input size\n",
    "            N_HIDDEN_UNITS, # output size\n",
    "            ACTIVATION,     # activation function\n",
    "        )\n",
    "        layers_inputs.append(output)    # 把 output 加入记录\n",
    "\n",
    "    # 建立 output layer\n",
    "    prediction = add_layer(layers_inputs[-1], 30, 1, activation_function=None)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "    return [train_op, cost, layers_inputs]\n",
    "\n",
    "\n",
    "x_data = np.linspace(-7, 10, 500)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 8, x_data.shape)\n",
    "y_data = np.square(x_data) - 5 + noise\n",
    "\n",
    "# 可视化 input data\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实现 Batch Normalization, 我们要对每一层的代码进行修改, 给 built_net 和 add_layer 都加上 norm 参数, 表示是否是 Batch Normalization 层:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ACTIVATION = tf.nn.relu # 每一层都使用 relu \n",
    "N_LAYERS = 7            # 一共7层隐藏层\n",
    "N_HIDDEN_UNITS = 30     # 每个层隐藏层有 30 个神经元\n",
    "\n",
    "def built_net(xs, ys, norm):\n",
    "    def add_layer(inputs, in_size, out_size, activation_function=None, norm=False):\n",
    "\n",
    "\n",
    "def built_net(xs, ys, norm):\n",
    "    #def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    def add_layer(inputs, in_size, out_size, activation_function=None, norm=False):           \n",
    "        # 添加层功能\n",
    "        Weights = tf.Variable(tf.random_normal([in_size, out_size], mean=0., stddev=1.))\n",
    "        biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "        Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "        if activation_function is None:\n",
    "            outputs = Wx_plus_b\n",
    "        else:\n",
    "            outputs = activation_function(Wx_plus_b)\n",
    "        return outputs\n",
    "\n",
    "    fix_seed(1)\n",
    "\n",
    "    layers_inputs = [xs]    # 记录每层的 input\n",
    "\n",
    "    # loop 建立所有层\n",
    "    for l_n in range(N_LAYERS):\n",
    "        layer_input = layers_inputs[l_n]\n",
    "        in_size = layers_inputs[l_n].get_shape()[1].value\n",
    "\n",
    "        output = add_layer(\n",
    "            layer_input,    # input\n",
    "            in_size,        # input size\n",
    "            N_HIDDEN_UNITS, # output size\n",
    "            ACTIVATION,     # activation function\n",
    "        )\n",
    "        layers_inputs.append(output)    # 把 output 加入记录\n",
    "\n",
    "    # 建立 output layer\n",
    "    prediction = add_layer(layers_inputs[-1], 30, 1, activation_function=None)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "    return [train_op, cost, layers_inputs]\n",
    "\n",
    "\n",
    "x_data = np.linspace(-7, 10, 500)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 8, x_data.shape)\n",
    "y_data = np.square(x_data) - 5 + noise\n",
    "\n",
    "# 可视化 input data\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "自编码 Autoencoder (非监督学习)\n",
    "\n",
    "\n",
    "今天的代码，我们会运用两个类型：\n",
    "\n",
    "第一，是通过Feature的压缩并解压，并将结果与原始数据进行对比，观察处理过后的数据是不是如预期跟原始数据很相像。（这里会用到MNIST数据）\n",
    "\n",
    "在压缩环节：我们要把这个Features不断压缩，经过第一个隐藏层压缩至256个 Features，再经过第二个隐藏层压缩至128个。\n",
    "在解压环节：我们将128个Features还原至256个，再经过一步还原至784个。\n",
    "在对比环节：比较原始数据与还原后的拥有 784 Features 的数据进行 cost 的对比，根据 cost 来提升我的 Autoencoder 的准确率，下图是两个隐藏层的 weights 和 biases 的定义：\n",
    "\n",
    "\n",
    "通过5个 Epoch 的训练，（通常情况下，想要得到好的的效果，我们应进行10 ~ 20个 Epoch 的训练）我们的结果如下：\n",
    "\n",
    "上面一行是真实数据，下面一行是经过 encoder 和 decoder 之后的数据，如果继续进行训练，效果会更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got <tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7feea72556d0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a63126f643f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Construct model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mencoder_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;31m# 128 Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mdecoder_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_op\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# 784 Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a63126f643f4>\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Encoder Hidden layer with sigmoid activation #1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n\u001b[0m\u001b[1;32m     43\u001b[0m                                    biases['encoder_b1']))\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Decoder Hidden layer with sigmoid activation #2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1807\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only one of transpose_b and adjoint_b can be True.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[0ma_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    649\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    174\u001b[0m                                          as_ref=False):\n\u001b[1;32m    175\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 165\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    166\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnumpy_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mproto_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FlattenToStrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/compat.pyc\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 65\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got <tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7feea72556d0>"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(1)   # set random seed\n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "X=mnist\n",
    "# Parameter\n",
    "learning_rate = 0.01\n",
    "training_epochs = 5 # 五组训练\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "# Network Parameters\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "\n",
    "\n",
    "# hidden layer settings\n",
    "n_hidden_1 = 256 # 1st layer num features\n",
    "n_hidden_2 = 128 # 2nd layer num features\n",
    "\n",
    "\n",
    "weights = {\n",
    "\t'encoder_h1':tf.Variable(tf.random_normal([n_input,n_hidden_1])),\n",
    "\t'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "\t'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2,n_hidden_1])),\n",
    "\t'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "\t}\n",
    "biases = {\n",
    "\t'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "\t'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "\t}\n",
    "\n",
    "\n",
    "#Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "    \n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X) \t\t\t# 128 Features\n",
    "decoder_op = decoder(encoder_op)\t# 784 Features\n",
    "\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\t# After \n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\t\t\t# Before\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # tf 马上就要废弃tf.initialize_all_variables()这种写法\n",
    "    # 替换成下面:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  # max(x) = 1, min(x) = 0\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # # Applying encode and decode over test set\n",
    "    encode_decode = sess.run(\n",
    "        y_pred, feed_dict={X: mnist.test.images[:examples_to_show]})\n",
    "    # Compare original images with their reconstructions\n",
    "    f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
    "    for i in range(examples_to_show):\n",
    "        a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "        a[1][i].imshow(np.reshape(encode_decode[i], (28, 28)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " tensorflow中使用LSTM去预测sinx函数\n",
    " \n",
    " http://blog.csdn.net/lujiandong1/article/details/53244699\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "第二，我们只看 encoder 压缩的过程，使用它将一个数据集压缩到只有两个Feature时，将数据放入一个二维坐标系内，特征压缩的效果如下\n",
    "\n",
    "在类型二中，我们只显示 encoder 之后的数据， 并画在一个二维直角坐标系内。做法很简单，我们将原有 784 Features 的数据压缩成仅剩 2 Features 的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'rnn_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b61ab1710f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTIME_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCELL_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b61ab1710f6d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_steps, input_size, output_size, cell_size, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_input_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM_cell'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out_hidden'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_output_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b61ab1710f6d>\u001b[0m in \u001b[0;36madd_cell\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mlstm_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforget_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m#lstm_cell = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initial_state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'rnn_cell'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "  \n",
    "BATCH_START = 0  \n",
    "TIME_STEPS = 20  \n",
    "BATCH_SIZE = 50  \n",
    "INPUT_SIZE = 1  \n",
    "OUTPUT_SIZE = 1  \n",
    "CELL_SIZE = 10  \n",
    "LR = 0.006  \n",
    "  \n",
    "  \n",
    "def get_batch():  \n",
    "    global BATCH_START, TIME_STEPS  \n",
    "    # xs shape (50batch, 20steps)  \n",
    "    xs = np.arange(BATCH_START, BATCH_START+TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10*np.pi)  \n",
    "    seq = np.sin(xs)  \n",
    "    res = np.cos(xs)  \n",
    "    BATCH_START += TIME_STEPS  \n",
    "    # plt.plot(xs[0, :], res[0, :], 'r', xs[0, :], seq[0, :], 'b--')  \n",
    "    # plt.show()  \n",
    "    # returned seq, res and xs: shape (batch, step, input)  \n",
    "    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]  \n",
    "  \n",
    "  \n",
    "class LSTMRNN(object):  \n",
    "    def __init__(self, n_steps, input_size, output_size, cell_size, batch_size):  \n",
    "        self.n_steps = n_steps  \n",
    "        self.input_size = input_size  \n",
    "        self.output_size = output_size  \n",
    "        self.cell_size = cell_size  \n",
    "        self.batch_size = batch_size  \n",
    "        with tf.name_scope('inputs'):  \n",
    "            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs')  \n",
    "            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys')  \n",
    "        with tf.variable_scope('in_hidden'):  \n",
    "            self.add_input_layer()  \n",
    "        with tf.variable_scope('LSTM_cell'):  \n",
    "            self.add_cell()  \n",
    "        with tf.variable_scope('out_hidden'):  \n",
    "            self.add_output_layer()  \n",
    "        with tf.name_scope('cost'):  \n",
    "            self.compute_cost()  \n",
    "        with tf.name_scope('train'):  \n",
    "            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)  \n",
    "  \n",
    "    def add_input_layer(self,):  \n",
    "        l_in_x = tf.reshape(self.xs, [-1, self.input_size], name='2_2D')  # (batch*n_step, in_size)  \n",
    "        # Ws (in_size, cell_size)  \n",
    "        Ws_in = self._weight_variable([self.input_size, self.cell_size])  \n",
    "        # bs (cell_size, )  \n",
    "        bs_in = self._bias_variable([self.cell_size,])  \n",
    "        # l_in_y = (batch * n_steps, cell_size)  \n",
    "        with tf.name_scope('Wx_plus_b'):  \n",
    "            l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in  \n",
    "        # reshape l_in_y ==> (batch, n_steps, cell_size)  \n",
    "        self.l_in_y = tf.reshape(l_in_y, [-1, self.n_steps, self.cell_size], name='2_3D')  \n",
    "  \n",
    "    def add_cell(self):  \n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        #lstm_cell = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=1.0)\n",
    "        with tf.name_scope('initial_state'):  \n",
    "            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)  \n",
    "        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(  \n",
    "            lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)  \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def add_output_layer(self):  \n",
    "        # shape = (batch * steps, cell_size)  \n",
    "        l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name='2_2D')  \n",
    "        Ws_out = self._weight_variable([self.cell_size, self.output_size])  \n",
    "        bs_out = self._bias_variable([self.output_size, ])  \n",
    "        # shape = (batch * steps, output_size)  \n",
    "        with tf.name_scope('Wx_plus_b'):  \n",
    "            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out  \n",
    "  \n",
    "    def compute_cost(self):  \n",
    "        losses = tf.nn.seq2seq.sequence_loss_by_example(  \n",
    "            [tf.reshape(self.pred, [-1], name='reshape_pred')],  \n",
    "            [tf.reshape(self.ys, [-1], name='reshape_target')],  \n",
    "            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],  \n",
    "            average_across_timesteps=True,  \n",
    "            softmax_loss_function=self.ms_error,  \n",
    "            name='losses'  \n",
    "        )  \n",
    "        with tf.name_scope('average_cost'):  \n",
    "            self.cost = tf.div(  \n",
    "                tf.reduce_sum(losses, name='losses_sum'),  \n",
    "                self.batch_size,  \n",
    "                name='average_cost')  \n",
    "            tf.scalar_summary('cost', self.cost)  \n",
    "  \n",
    "    def ms_error(self, y_pre, y_target):  \n",
    "        return tf.square(tf.sub(y_pre, y_target))  \n",
    "  \n",
    "    def _weight_variable(self, shape, name='weights'):  \n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=1.,)  \n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)  \n",
    "  \n",
    "    def _bias_variable(self, shape, name='biases'):  \n",
    "        initializer = tf.constant_initializer(0.1)  \n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer)  \n",
    "  \n",
    "  \n",
    "if __name__ == '__main__':  \n",
    "    model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)  \n",
    "    sess = tf.Session()  \n",
    "    merged = tf.merge_all_summaries()  \n",
    "    writer = tf.train.SummaryWriter(\"logs\", sess.graph)  \n",
    "    sess.run(tf.initialize_all_variables())  \n",
    "    # relocate to the local dir and run this line to view it on Chrome (http://0.0.0.0:6006/):  \n",
    "    # $ tensorboard --logdir='logs'  \n",
    "  \n",
    "    plt.ion()  \n",
    "    plt.show()  \n",
    "    for i in range(200):  \n",
    "        seq, res, xs = get_batch()  \n",
    "        if i == 0:  \n",
    "            feed_dict = {  \n",
    "                    model.xs: seq,  \n",
    "                    model.ys: res,  \n",
    "                    # create initial state  \n",
    "            }  \n",
    "        else:  \n",
    "            feed_dict = {  \n",
    "                model.xs: seq,  \n",
    "                model.ys: res,  \n",
    "                model.cell_init_state: state    # use last state as the initial state for this run  \n",
    "            }  \n",
    "  \n",
    "        _, cost, state, pred = sess.run(  \n",
    "            [model.train_op, model.cost, model.cell_final_state, model.pred],  \n",
    "            feed_dict=feed_dict)  \n",
    "  \n",
    "        # plotting  \n",
    "        # plt.plot(xs[0, :], res[0].flatten(), 'r', xs[0, :], pred.flatten()[:TIME_STEPS], 'b--')  \n",
    "        # plt.ylim((-1.2, 1.2))  \n",
    "        # plt.draw()  \n",
    "        # plt.pause(0.3)  \n",
    "  \n",
    "        if i % 20 == 0:  \n",
    "            print('cost: ', round(cost, 4))  \n",
    "            result = sess.run(merged, feed_dict)  \n",
    "            writer.add_summary(result, i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create some variables.\n",
    "v1 = tf.Variable(1, name=\"v1\")\n",
    "v2 = tf.Variable(2, name=\"v2\")\n",
    "\n",
    "# Add an op to initialize the variables.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "  print \"v1 = \", v1.eval()\n",
    "  print \"v2 = \", v2.eval()\n",
    "  # Save the variables to disk.\n",
    "  save_path = saver.save(sess, \"/tmp/model-2.ckpt\")\n",
    "  print \"Model saved in file: \", save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "# Create some variables.\n",
    "v1 = tf.Variable(0, name=\"v1\")\n",
    "v2 = tf.Variable(0, name=\"v2\")\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  saver.restore(sess, \"/tmp/model-2.ckpt\")\n",
    "  print \"Model restored.\"\n",
    "  print \"v1 = \", v1.eval()\n",
    "  print \"v2 = \", v2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-zh-CN.md\n",
    "\n",
    "自上而下的学习路线: 软件工程师的机器学习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model-python\n",
    "\n",
    "\n",
    "\n",
    "In( and After) TensorFlow version 0.11.0RC1, you can save and restore your model directly by calling tf.train.export_meta_graph and tf.train.import_meta_graph according to https://www.tensorflow.org/programmers_guide/meta_graph\n",
    "\n",
    "save model:\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal(shape=[10]), name='w1')\n",
    "w2 = tf.Variable(tf.truncated_normal(shape=[20]), name='w2')\n",
    "tf.add_to_collection('vars', w1)\n",
    "tf.add_to_collection('vars', w2)\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, 'my-model')\n",
    "# `save` method will call `export_meta_graph` implicitly.\n",
    "# you will get saved graph files:my-model.meta\n",
    "restore model:\n",
    "\n",
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('my-model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "all_vars = tf.get_collection('vars')\n",
    "for v in all_vars:\n",
    "    v_ = sess.run(v)\n",
    "    print(v_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.truncated_normal(shape=[10]), name='w1')\n",
    "w2 = tf.Variable(tf.truncated_normal(shape=[20]), name='w2')\n",
    "tf.add_to_collection('vars', w1)\n",
    "tf.add_to_collection('vars', w2)\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, 'my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('my-model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "all_vars = tf.get_collection('vars')\n",
    "for v in all_vars:\n",
    "    v_ = sess.run(v)\n",
    "    print(v_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TensorFlow version lower than 0.11.0RC1, the checkpoints that are saved contain values for the Variables in your model, not the model/graph itself, which means that the graph should be the same when you restore the checkpoint.\n",
    "\n",
    "Here's an example for a linear regression where there's a training loop that saves variable checkpoints and an evaluation section that will restore variables saved in a prior run and compute predictions. Of course, you can also restore variables and continue training if you'd like.\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/state_ops/variables#Variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts to the model, the model definition, saved by Supervisor as graph.pbtxt in the model directory and the numerical values of tensors, saved into checkpoint files like model.ckpt-1003418.\n",
    "\n",
    "The model definition can be restored using tf.import_graph_def, and the weights are restored using Saver.\n",
    "\n",
    "However, Saver uses special collection holding list of variables that's attached to the model Graph, and this collection is not initialized using import_graph_def, so you can't use the two together at the moment (it's on our roadmap to fix). For now, you have to use approach of Ryan Sepassi -- manually construct a graph with identical node names, and use Saver to load the weights into it.\n",
    "\n",
    "(Alternatively you could hack it by using by using import_graph_def, creating variables manually, and using tf.add_to_collection(tf.GraphKeys.VARIABLES, variable) for each variable, then using Saver)\n",
    "\n",
    "As Yaroslav said, you can hack restoring from a graph_def and checkpoint by importing the graph, manually creating variables, and then using a Saver.\n",
    "\n",
    "I implemented this for my personal use, so I though I'd share the code here.\n",
    "\n",
    "Link: https://gist.github.com/nikitakit/6ef3b72be67b86cb7868\n",
    "\n",
    "(This is, of course, a hack, and there is no guarantee that models saved this way will remain readable in future versions of TensorFlow.)\n",
    "\n",
    "\n",
    "\n",
    "If it is an internally saved model, you just specify a restorer for all variables as\n",
    "\n",
    "restorer = tf.train.Saver(tf.all_variables())\n",
    "and use it to restore variables in a current session:\n",
    "\n",
    "restorer.restore(self._sess, model_file)\n",
    "For the external model you need to specify the mapping from the its variable names to your variable names. You can view the model variable names using the command\n",
    "\n",
    "python /path/to/tensorflow/tensorflow/python/tools/inspect_checkpoint.py --file_name=/path/to/pretrained_model/model.ckpt\n",
    "The inspect_checkpoint.py script can be found in './tensorflow/python/tools' folder of the Tensorflow source.\n",
    "\n",
    "To specify the mapping, you can use my Tensorflow-Worklab, which contains a set of classes and scripts to train and retrain different models. It includes an example of retraining ResNet models, located here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If it is an internally saved model, you just specify a restorer for all variables as\n",
    "\n",
    "restorer = tf.train.Saver(tf.all_variables())\n",
    "and use it to restore variables in a current session:\n",
    "\n",
    "restorer.restore(self._sess, model_file)\n",
    "For the external model you need to specify the mapping from the its variable names to your variable names. You can view the model variable names using the command\n",
    "\n",
    "python /path/to/tensorflow/tensorflow/python/tools/inspect_checkpoint.py --file_name=/path/to/pretrained_model/model.ckpt\n",
    "\n",
    "The inspect_checkpoint.py script can be found in './tensorflow/python/tools' folder of the Tensorflow source.\n",
    "\n",
    "To specify the mapping, you can use my Tensorflow-Worklab, which contains a set of classes and scripts to train and retrain different models. It includes an example of retraining ResNet models, located here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take this easier way.\n",
    "\n",
    "Step.1 - Initialize all your variables\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1), name=\"W1\")\n",
    "B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]), name=\"B1\")\n",
    "\n",
    "Similarly, W2, B2, W3, .....\n",
    "Step.2 - Save the list inside Model Saver and Save it\n",
    "\n",
    "model_saver = tf.train.Saver()\n",
    "\n",
    "# Train the model and save it in the end\n",
    "model_saver.save(session, \"saved_models/CNN_New.ckpt\")\n",
    "Step. 3 - Restore the model\n",
    "\n",
    "with tf.Session(graph=graph_cnn) as session:\n",
    "    model_saver.restore(session, \"saved_models/CNN_New.ckpt\")\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "Step. 4 - Check Variable\n",
    "\n",
    "W1 = session.run(W1)\n",
    "print(W1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/sdemyanov/tensorflow-worklab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://stackoverflow.com/questions/37858866/how-to-restore-checkpoint-in-tensorflow-inside-ipython-or-anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You need to reset the default graph at the beginning of your call when you run again the file.\n",
    "\n",
    "If you don't reset the default graph, and run two times the line:\n",
    "\n",
    "x = tf.Variable(1, name='x')\n",
    "print x.name\n",
    "You will see the first time that x has name \"x:0\" and the second time its name is \"x_1:0\". This is what confuses tf.train.Saver:\n",
    "\n",
    "it first saves the value of x using name \"x:0\"\n",
    "then in the next run you try to load the saved value of x, but now the name of the variable is \"x_1:0\", so the saver tries to load a saved value under the name \"x_1:0\" but cannot find it, and returns an error.\n",
    "However, you can reset the default graph at the beginning using tf.reset_default_graph(). This will create an empty graph and use it as default graph.\n",
    "Here the name of x can be the same in those two graphs:\n",
    "\n",
    "# First run\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.Variable(1, name='x')\n",
    "print x.name  # prints 'x:0'\n",
    "\n",
    "# Next run\n",
    "tf.reset_default_graph()\n",
    "x = tf.Variable(1, name='x')\n",
    "print x.name  # prints 'x:0'\n",
    "The two Variables can now have the same name because they are no longer in the same graph.\n",
    "\n",
    "Another way of doing it is to create a graph at the beginning and use it as default graph:\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.Variable(1, name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ############### tensor_save_named_vars.py#######################\n",
    "tf.reset_default_graph()\n",
    "v1 = tf.Variable(11, name=\"v1\")\n",
    "v2 = tf.Variable(2, name=\"v2\")\n",
    "\n",
    "# Add an op to initialize the variables.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "  print \"v1 = \", v1.eval()\n",
    "  print \"v2 = \", v2.eval()\n",
    "  # Save the variables to disk.\n",
    "  save_path = saver.save(sess, \"/tmp/model3.ckpt\")\n",
    "  print \"Model saved in file: \", save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "############################ tensor_restore.py######################\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "# Create some variables.\n",
    "v1 = tf.Variable(0, name=\"v1\")\n",
    "v2 = tf.Variable(0, name=\"v2\")\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  saver.restore(sess, \"/tmp/model3.ckpt\")\n",
    "  print \"Model restored.\"\n",
    "  print \"v1 = \", v1.eval()\n",
    "  print \"v2 = \", v2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -l /tmp/model3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('/tmp/model3.ckpt.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('/tmp/'))\n",
    "all_vars = tf.get_collection('vars')\n",
    "for v in all_vars:\n",
    "    v_ = sess.run(v)\n",
    "    print(v_)\n",
    "    print v1.eval()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Scikit Flow get GraphDef for Android (save *.pb file)\n",
    "\n",
    "To save as pb file, you need to extract the graph_def from the constructed graph. You can do that as--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import tensor_shape, graph_util\n",
    "from tensorflow.python.platform import gfile\n",
    "sess = tf.Session()\n",
    "final_tensor_name = 'results:0'     #Replace final_tensor_name with name of the final tensor in your graph\n",
    "#########Build your graph and train########\n",
    "## Your tensorflow code to build the graph\n",
    "###########################################\n",
    "\n",
    "outpt_filename = 'output_graph.pb'\n",
    "output_graph_def = sess.graph.as_graph_def()\n",
    "with gfile.FastGFile(outpt_filename, 'wb') as f:\n",
    "  f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to convert your trained variables to constants (to avoid using ckpt files to load the weights), you can use:\n",
    "\n",
    "output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [final_tensor_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import tensor_shape, graph_util\n",
    "from tensorflow.python.platform import gfile\n",
    "sess = tf.Session()\n",
    "final_tensor_name = 'results:0'     #Replace final_tensor_name with name of the final tensor in your graph\n",
    "#########Build your graph and train########\n",
    "## Your tensorflow code to build the graph\n",
    "###########################################\n",
    "\n",
    "outpt_filename = 'output_graph.pb'\n",
    "output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [final_tensor_name])\n",
    "with gfile.FastGFile(outpt_filename, 'wb') as f:\n",
    "  f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow GraphDef pb 文件读和写 \n",
    "http://blog.csdn.net/eunicechen/article/details/51801351\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os.path\n",
    "\n",
    "from tensorflow.Python.platform import gfile\n",
    "from google.protobuf import text_format\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGs\n",
    "\n",
    "#Input Graph model file location\n",
    "tf.app.flags.DEFINE_string('model_dir', '/home/eunice/data/test_code', \"\"\"Paht to classify_image_graph_def.pb\"\"\")\n",
    "\n",
    "#Output Graph model protobuf as text format & binary format\n",
    "tf.app.flags.DEFINE_string('output_graph_txt', '/home/eunice/data/test_code/output_graph.pbtxt', \"\"\"pbtxt\"\"\")\n",
    "tf.app.flags.DEFINE_string('output_graph_pb', '/home/eunice/data/test_code/output_graph.pb', \"\"\"pb\"\"\")\n",
    "\n",
    "def convert_pb_to_pbtxt():\n",
    "  model_filename = os.path.join(FLAGS.model_dir, 'classify_image_graph_def.pb')\n",
    "  with gfile.FastGFile(model_filename, 'rb') as f:\n",
    "   graph_def = tf.GraphDef()        #创建一个GraphDef\n",
    "   graph_def.ParseFromString(f.read())   #ParseFromString(), reading message from protocol buffer binary fomat\n",
    "  \n",
    "  with gfile.FastGFile(FLAGS.output_graph_txt, 'wb') as f:\n",
    "   f.write(text_format.MessageToString(graph_def))   # MessageToString(message, as_utf8=False, as_one_line=False)  Convert protobuf message to text format\n",
    "  \n",
    "#  with gfile.FastGFile(FLAGS.output_graph_pb, 'wb') as f:\n",
    "#   f.write(graph_def.SerializeToString())  #serializes the message and returns it as a string. Note that the bytes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  tensorflow从0开始（6）——保存加载模型\n",
    "http://blog.csdn.net/searobbers_duck/article/details/51721916\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('summaries_dir', '/tmp/save_graph_logs', 'Summaries directory')\n",
    "\n",
    "data = np.arange(10,dtype=np.int32)\n",
    "with tf.Session() as sess:\n",
    "  print(\"# build graph and run\")\n",
    "  input1= tf.placeholder(tf.int32, [10], name=\"input\")\n",
    "  output1= tf.add(input1, tf.constant(100,dtype=tf.int32), name=\"output\") #  data depends on the input data\n",
    "  saved_result= tf.Variable(data, name=\"saved_result\")\n",
    "  do_save=tf.assign(saved_result,output1)\n",
    "  tf.initialize_all_variables()\n",
    "  os.system(\"rm -rf /tmp/save_graph_logs\")\n",
    "  merged = tf.merge_all_summaries()\n",
    "  train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir,\n",
    "                                        sess.graph)\n",
    "  os.system(\"rm -rf /tmp/load\")\n",
    "  tf.train.write_graph(sess.graph_def, \"/tmp/load\", \"test.pb\", False) #proto\n",
    "  # now set the data:\n",
    "  result,_=sess.run([output1,do_save], {input1: data}) # calculate output1 and assign to 'saved_result'\n",
    "  saver = tf.train.Saver(tf.all_variables())\n",
    "  saver.save(sess,\"checkpoint.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将TensorFlow的网络导出为单个文件发表于 2017-01-14 | \n",
    "http://encodets.me/2017/01/export-TensorFlow-network/\n",
    "\n",
    "有时候，我们需要将TensorFlow的模型导出为单个文件（同时包含模型架构定义与权重），方便在其他地方使用（如在c++中部署网络）。利用tf.train.write_graph()默认情况下只导出了网络的定义（没有权重），而利用tf.train.Saver().save()导出的文件graph_def与权重是分离的，因此需要采用别的方法。\n",
    "\n",
    "我们知道，graph_def文件中没有包含网络中的Variable值（通常情况存储了权重），但是却包含了constant值，所以如果我们能把Variable转换为constant，即可达到使用一个文件同时存储网络架构与权重的目标。\n",
    "\n",
    "我们可以采用以下方式冻结权重并保存网络：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "# 构造网络\n",
    "a = tf.Variable([[3],[4]], dtype=tf.float32, name='a')\n",
    "b = tf.Variable(4, dtype=tf.float32, name='b')\n",
    "# 一定要给输出tensor取一个名字！！\n",
    "output = tf.add(a, b, name='out')\n",
    "# 转换Variable为constant，并将网络写入到文件\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 这里需要填入输出tensor的名字\n",
    "    graph = convert_variables_to_constants(sess, sess.graph_def, [\"out\"])\n",
    "    tf.train.write_graph(graph, '.', 'graph.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当恢复网络时，可以使用如下方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "    with open('./graph.pb', 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read()) \n",
    "        output = tf.import_graph_def(graph_def, return_elements=['out:0']) \n",
    "        print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到之前的权重确实保存了下来!! ,  save as Txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "a = tf.Variable([[3],[4]], dtype=tf.float32, name='a')\n",
    "b = tf.Variable(4, dtype=tf.float32, name='b')\n",
    "input_tensor = tf.placeholder(tf.float32, name='input')\n",
    "output = tf.add((a+b), input_tensor, name='out')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    graph = convert_variables_to_constants(sess, sess.graph_def, [\"out\"])\n",
    "    tf.train.write_graph(graph, '.', 'graph.pb', as_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题来了，我们的网络需要能有一个输入自定义数据的接口啊！不然这玩意有什么用。。别急，当然有办法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "\n",
    "tf.reset_default_graph()\n",
    "a = tf.Variable([[3],[4]], dtype=tf.float32, name='a')\n",
    "b = tf.Variable(4, dtype=tf.float32, name='b')\n",
    "input_tensor = tf.placeholder(tf.float32, name='input')\n",
    "output = tf.add((a+b), input_tensor, name='out')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    graph = convert_variables_to_constants(sess, sess.graph_def, [\"out\"])\n",
    "    tf.train.write_graph(graph, '.', 'graph.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用上述代码重新保存网络至graph.pb，这次我们有了一个输入placeholder，下面来看看怎么恢复网络并输入自定义数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    with open('./graph.pb', 'rb') as f: \n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read()) \n",
    "        output = tf.import_graph_def(graph_def, input_map={'input:0':4.}, return_elements=['out:0'], name='a') \n",
    "        print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到结果没有问题，当然在input_map那里可以替换为新的自定义的placeholder，如下所示：看看输出，同样没有问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "new_input = tf.placeholder(tf.float32, shape=())\n",
    "with tf.Session() as sess:\n",
    "    with open('./graph.pb', 'rb') as f: \n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read()) \n",
    "        output = tf.import_graph_def(graph_def, input_map={'input:0':new_input}, return_elements=['out:0'], name='a') \n",
    "        print(sess.run(output, feed_dict={new_input:4}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外需要说明的一点是，在利用tf.train.write_graph写网络架构的时候，如果令as_text=True了，则在导入网络的时候，需要做一点小修改。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# 不使用'rb'模式\n",
    "    with open('./graph.pb', 'r') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        # 不使用graph_def.ParseFromString(f.read())\n",
    "        text_format.Merge(f.read(), graph_def)\n",
    "        output = tf.import_graph_def(graph_def, return_elements=['out:0']) \n",
    "        print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ##参考资料http://stackoverflow.com/questions/34343259/is-there-an-example-on-how-to-generate-protobuf-files-holding-trained-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/tensorflow/tensorflow/issues/616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://blog.csdn.net/searobbers_duck/article/details/51721916 \n",
    "tensorflow从0开始（6）——保存加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 徐长卿学数据分析http://www.cnblogs.com/SSSR/p/5630534.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is there an example on how to generate protobuf files holding trained Tensorflow graphs\n",
    "\n",
    "\n",
    "http://stackoverflow.com/questions/34343259/is-there-an-example-on-how-to-generate-protobuf-files-holding-trained-tensorflow\n",
    "\n",
    "I am looking at Google's example on how to deploy and use a pre-trained Tensorflow graph (model) on Android, at:\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\n",
    "\n",
    "This example uses a .pb file at: [this is a link to a file that downloads automatically] https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\n",
    "\n",
    "The example shows how to load the .pb file to a Tensorflow session and use it to perform classification, but it doesn't (?) mention how to generate such a .pb file after a graph is trained (e.g., in Python).\n",
    "\n",
    "Are there any examples on how to do that?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
