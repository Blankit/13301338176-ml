# Copyright (c) 2017 Baidu.com, Inc. All Rights Reserved
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import sys
from paddle.trainer_config_helpers import *

############### Parameters ###############
TrainData(PyData(
	files="train.list",
	load_data_module="data_provider",
	load_data_object = "processData",
	async_load_data = True)
)

TestData(PyData(
	files="test.list",
	load_data_module="data_provider",
	load_data_object = "processData")
)

############### Algorithm Configuration ###############
settings(
    learning_rate=1e-03,
    batch_size=240,
    learning_method=RMSPropOptimizer(),
    regularization=L2Regularization(8e-4),
    gradient_clipping_threshold=25,
)

############### Network Configuration ###############
feat_size = 1024
audio_size = 128
label_size = 4716
gru_size = 1024
embedding_size = 512

bias_attr = ParamAttr(initial_std=0.,l2_rate=0.)
layer_attr = ExtraAttr(drop_rate=0.5)

input = data_layer(name='feat', size=feat_size)
audio = data_layer(name='audio', size=audio_size)
label = data_layer(name='label', size=label_size)

###
input_fc = fc_layer(input=input, size=embedding_size, 
		    act=TanhActivation(), bias_attr=bias_attr)

input_fw_gru = simple_gru(input=input_fc, size=gru_size, reverse=False)
input_bw_gru = simple_gru(input=input_fc, size=gru_size, reverse=True)
input_concat = concat_layer(input=[input_fw_gru, input_bw_gru])

input_dropout = dropout_layer(input=input_concat, dropout_rate=0.5)

input_lstm_weight = fc_layer(input=input_dropout, size=1, 
		             act=SequenceSoftmaxActivation(), bias_attr=False)
input_scaled = scaling_layer(weight=input_lstm_weight, input=input_dropout)
input_lstm_pool = pooling_layer(input=input_scaled, pooling_type=SumPooling())

###
audio_fc = fc_layer(input=audio, size=embedding_size, 
		    act=TanhActivation(), bias_attr=bias_attr)

audio_fw_gru = simple_gru(input=audio_fc, size=gru_size, reverse=False)
audio_bw_gru = simple_gru(input=audio_fc, size=gru_size, reverse=True)
audio_concat = concat_layer(input=[audio_fw_gru, audio_bw_gru])

audio_dropout = dropout_layer(input=audio_concat, dropout_rate=0.5)

audio_lstm_weight = fc_layer(input=audio_dropout, size=1, 
		             act=SequenceSoftmaxActivation(),bias_attr=False)
audio_scaled = scaling_layer(weight=audio_lstm_weight, input=audio_dropout)
audio_lstm_pool = pooling_layer(input=audio_scaled, pooling_type=SumPooling())

###
lstm_pool_concat = concat_layer(input=[input_lstm_pool, audio_lstm_pool])

up_proj = fc_layer(input=lstm_pool_concat, size=8192, 
		   act=ReluActivation(), bias_attr=bias_attr)

hidden = fc_layer(input=up_proj, size=4096, 
		  act=TanhActivation(), bias_attr=bias_attr)

output = fc_layer(input=hidden, size=label_size,
                  bias_attr=bias_attr,
                  act=SigmoidActivation())

cost = multi_binary_label_cross_entropy(name='cost', input=output, label=label)

outputs(cost)
