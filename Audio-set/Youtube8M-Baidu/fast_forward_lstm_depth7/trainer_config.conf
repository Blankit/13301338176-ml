#!usr/bin/env python
import random
import hashlib
import os
import sys
import math
from paddle.trainer_config_helpers import *

lr_hid_col = 2e-1

model_type('recurrent_nn')

feat_size = 1152
label_size = 4716

define_py_data_sources2(
    train_list='trn.list',
    test_list='tst.list',
    module="video_mean_provider",
    obj='processData',
)

############################### Algorithm Configuration ###############################
Settings(
    algorithm='sgd',
    learning_method='adam',
    learning_rate=5e-4,
    learning_rate_decay_a=0,
    learning_rate_decay_b=0,
    ada_rou=0.95,
    ada_epsilon=1e-6,
    batch_size=1200,
    average_window=0.5,
    do_average_in_cpu=True,
)

lstm_size = 512
lstm_depth = 7
with_attention = False
ofsratio   = 2

default_decay_rate(8e-4)
#default_num_batches_regularization(1)
default_initial_std(1 / math.sqrt(lstm_size) / 2.5)

################################## Network Configuration ##############################

Inputs("rgb", 'audio', 'label')

Layer(name='rgb', type='data', size=1024)
Layer(name='audio', type='data', size=128)
Layer(name='label', type='data', size=label_size)

Layer(
    name = 'feat',
    inputs = ['rgb','audio'],
    type = 'concat'
)

Layer(
    name = 'feat_decode',
    type = 'slope_intercept',
    inputs = ['feat'],
    slope = 4.0/255,
    intercept = 4.0/512-2,
)

def BidirectionStackLSTMmemory(name, input, size, depth, with_attention):
    global lr_hid_col
    for i in range(1, depth + 1):
        Layer(
            name   = "{n}_hidden{idx:02d}".format(n = name,idx = i),
            type   = "mixed",
            size   = size * 4,
            bias   = Bias(initial_std=0.003,learning_rate=lr_hid_col),
            inputs = [FullMatrixProjection("{n}_hidden{idx:02d}".format(n=name,idx=i-1),learning_rate=lr_hid_col),
                      FullMatrixProjection("{n}_lstm{idx:02d}".format(n=name,idx=i-1),initial_std=0)] if i > 1 else \
                     [FullMatrixProjection(input)]
        )
        # LSTM
        Layer(
            name              = "{n}_lstm{idx:02d}".format(n=name, idx=i),
            type              = "lstmemory",
            reversed          = (i+1)%2,
            active_type       = "relu",
            active_gate_type  = "sigmoid",
            active_state_type = "tanh",
            bias              = True,
            inputs=Input("{n}_hidden{idx:02d}".format(n=name,idx=i),initial_std=0.0), 
        )
    Layer(
       name   = name+"_rep_lstm",
       type   = "max",
       bias   = False,
       inputs = ["{n}_lstm{idx:02d}".format(n=name, idx=depth)]
    )
    Layer(
       name   = name+"_rep_hidden",
       type   = "max",
       bias   = False,
       inputs = ["{n}_hidden{idx:02d}".format(n=name, idx=depth)]
    )

BidirectionStackLSTMmemory(
    name="st",
    input="feat_decode",
    size=lstm_size,
    depth=lstm_depth,
    with_attention=False)

Layer(
    name = "fc1",
    type = "mixed",
    size = 2048,
    active_type = "linear",
    bias = Bias(initial_std=0.003,learning_rate=lr_hid_col),
    inputs = [ FullMatrixProjection("st_rep_lstm",initial_std=0, learning_rate=1),
               FullMatrixProjection("st_rep_hidden",initial_std=0.001, learning_rate=lr_hid_col)]
)
    
Layer(
    name = "output",
    inputs = FullMatrixProjection("fc1",initial_std=0.001, learning_rate=1),
    type = "mixed",
    size = label_size,
    active_type = "sigmoid",
    bias = Bias(initial_std=0.003,learning_rate=lr_hid_col)
)

### cost
Layer(
    type = 'multi_binary_label_cross_entropy',
    name = 'cost',
    inputs = [Input('output'), Input('label')]
)

Outputs('cost')
