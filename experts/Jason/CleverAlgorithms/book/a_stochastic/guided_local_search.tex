% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is an algorithm description, see:
% Jason Brownlee. A Template for Standardized Algorithm Descriptions. Technical Report CA-TR-20100107-1, The Clever Algorithms Project http://www.CleverAlgorithms.com, January 2010.

% Name
% The algorithm name defines the canonical name used to refer to the technique, in addition to common aliases, abbreviations, and acronyms. The name is used in terms of the heading and sub-headings of an algorithm description.
\section{Guided Local Search} 
\label{sec:guided_local_search}
\index{Guided Local Search}

% other names
% What is the canonical name and common aliases for a technique?
% What are the common abbreviations and acronyms for a technique?
\emph{Guided Local Search, GLS.}

% Taxonomy: Lineage and locality
% The algorithm taxonomy defines where a techniques fits into the field, both the specific subfields of Computational Intelligence and Biologically Inspired Computation as well as the broader field of Artificial Intelligence. The taxonomy also provides a context for determining the relation- ships between algorithms. The taxonomy may be described in terms of a series of relationship statements or pictorially as a venn diagram or a graph with hierarchical structure.
\subsection{Taxonomy}
% To what fields of study does a technique belong?
% What are the closely related approaches to a technique?
% general
The Guided Local Search algorithm is a Metaheuristic and a Global Optimization algorithm that makes use of an embedded Local Search algorithm.
% related
It is an extension to Local Search algorithms such as Hill Climbing (Section~\ref{sec:stochastic_hill_climbing}) and is similar in strategy to the Tabu Search algorithm (Section~\ref{sec:tabu_search}) and the Iterated Local Search algorithm (Section~\ref{sec:iterated_local_search}).

% Strategy: Problem solving plan
% The strategy is an abstract description of the computational model. The strategy describes the information processing actions a technique shall take in order to achieve an objective. The strategy provides a logical separation between a computational realization (procedure) and a analogous system (metaphor). A given problem solving strategy may be realized as one of a number specific algorithms or problem solving systems. The strategy description is textual using information processing and algorithmic terminology.
\subsection{Strategy}
% What is the information processing objective of a technique?
The strategy for the Guided Local Search algorithm is to use penalties to encourage a Local Search technique to escape local optima and discover the global optima.
% What is a techniques plan of action?
A Local Search algorithm is run until it gets stuck in a local optima. The features from the local optima are evaluated and penalized, the results of which are used in an augmented cost function employed by the Local Search procedure. The Local Search is repeated a number of times using the last local optima discovered and the augmented cost function that guides exploration away from solutions with features present in  discovered local optima.

% Procedure: Abstract computation
% The algorithmic procedure summarizes the specifics of realizing a strategy as a systemized and parameterized computation. It outlines how the algorithm is organized in terms of the data structures and representations. The procedure may be described in terms of software engineering and computer science artifacts such as Pseudocode, design diagrams, and relevant mathematical equations.
\subsection{Procedure}
% What is the computational recipe for a technique?
% What are the data structures and representations used in a technique?
Algorithm~\ref{alg:guided_local_search} provides a pseudocode listing of the Guided Local Search algorithm for minimization. 
% cost function
The Local Search algorithm used by the Guided Local Search algorithm uses an augmented cost function in the form  $h(s)=g(s)+\lambda\cdot\sum_{i=1}^{M}f_i$, where 
$h(s)$ is the augmented cost function, $g(s)$ is the problem cost function,$\lambda$ is the `regularization parameter' (a coefficient for scaling the penalties), $s$ is a locally optimal solution of $M$ features, and $f_i$ is the $i$'th feature in locally optimal solution. The augmented cost function is only used by the local search procedure, the Guided Local Search algorithm uses the problem specific cost function without augmentation.

% penalties
Penalties are only updated for those features in a locally optimal solution that maximize utility, updated by adding 1 to the penalty for the future (a counter). 
% utility
The utility for a feature is calculated as $U_{feature}=\frac{C_{feature}}{1+P_{feature}}$, where $U_{feature}$ is the utility for penalizing a feature (maximizing), $C_{feature}$ is the cost of the feature, and $P_{feature}$ is the current penalty for the feature.

\begin{algorithm}[htp]
	\SetLine
	% data
	\SetKwData{NumIterations}{$Iter_{max}$}
	\SetKwData{CurrentIteration}{$Iter_{i}$}
	\SetKwData{RegularizationParameter}{$\lambda$}
	\SetKwData{Best}{$S_{best}$}
	\SetKwData{Candidate}{$S_{curr}$}
	\SetKwData{FeaturePenalties}{$f_{penalties}$}
	\SetKwData{FeatureUtilities}{$f_{utilities}$}
	% functions
	\SetKwFunction{Cost}{Cost}
	\SetKwFunction{RandomSolution}{RandomSolution}
	\SetKwFunction{LocalSearch}{LocalSearch}
	\SetKwFunction{CalculateFeatureUtilities}{CalculateFeatureUtilities}
	\SetKwFunction{UpdateFeaturePenalties}{UpdateFeaturePenalties}
  	% I/O
	\KwIn{\NumIterations, \RegularizationParameter}
	\KwOut{\Best}
  	% Algorithm
	% init
	\FeaturePenalties $\leftarrow \emptyset$\;
	\Best $\leftarrow$ \RandomSolution{}\;
	% main loop
	\ForEach{\CurrentIteration $\in$ \NumIterations} {
		% local search
		\Candidate $\leftarrow$ \LocalSearch{\Best, \RegularizationParameter, \FeaturePenalties}\;	
		% update
		\FeatureUtilities $\leftarrow$ \CalculateFeatureUtilities{\Candidate, \FeaturePenalties}\;
		\FeaturePenalties $\leftarrow$ \UpdateFeaturePenalties{\Candidate, \FeaturePenalties, \FeatureUtilities}\;
		% keep best
		\If{\Cost{\Candidate} $\leq$ \Cost{\Best}} {
			\Best $\leftarrow$ \Candidate\;
		}
	}
	\Return{\Best}\;
	% caption
	\caption{Pseudocode for Guided Local Search.}
	\label{alg:guided_local_search}
\end{algorithm}

% Heuristics: Usage guidelines
% The heuristics element describe the commonsense, best practice, and demonstrated rules for applying and configuring a parameterized algorithm. The heuristics relate to the technical details of the techniques procedure and data structures for general classes of application (neither specific implementations not specific problem instances). The heuristics are described textually, such as a series of guidelines in a bullet-point structure.
\subsection{Heuristics}
% What are the suggested configurations for a technique?
% What are the guidelines for the application of a technique to a problem instance?
\begin{itemize}
	\item The Guided Local Search procedure is independent of the Local Search procedure embedded within it. A suitable domain-specific search procedure should be identified and employed.
	\item The Guided Local Search procedure may need to be executed for thousands to hundreds-of-thousands of iterations, each iteration of which assumes a run of a Local Search algorithm to convergence.
	\item The algorithm was designed for discrete optimization problems where a solution is comprised of independently assessable `features' such as Combinatorial Optimization, although it has been applied to continuous function optimization modeled as binary strings.
	\item The $\lambda$ parameter is a scaling factor for feature penalization that must be in the same proportion to the candidate solution costs from the specific problem instance to which the algorithm is being applied. As such, the value for $\lambda$ must be meaningful when used within the augmented cost function (such as when it is added to a candidate solution cost in minimization and subtracted from a cost in the case of a maximization problem).
\end{itemize}

% The code description provides a minimal but functional version of the technique implemented with a programming language. The code description must be able to be typed into an appropriate computer, compiled or interpreted as need be, and provide a working execution of the technique. The technique implementation also includes a minimal problem instance to which it is applied, and both the problem and algorithm implementations are complete enough to demonstrate the techniques procedure. The description is presented as a programming source code listing.
\subsection{Code Listing}
% How is a technique implemented as an executable program?
% How is a technique applied to a concrete problem instance?
Listing~\ref{guided_local_search} provides an example of the Guided Local Search algorithm implemented in the Ruby Programming Language. 
% probem
The algorithm is applied to the Berlin52 instance of the Traveling Salesman Problem (TSP), taken from the TSPLIB. The problem seeks a permutation of the order to visit cities (called a tour) that minimizes the total distance traveled. The optimal tour distance for Berlin52 instance is 7542 units.

% source
The implementation of the algorithm for the TSP was based on the configuration specified by Voudouris in \cite{Voudouris1997}.
% local search
A TSP-specific local search algorithm is used called 2-opt that selects two points in a permutation and reconnects the tour, potentially untwisting the tour at the selected points. The stopping condition for 2-opt was configured to be a fixed number of non-improving moves.

% setting lambda
The equation for setting $\lambda$ for TSP instances is $\lambda = \alpha\cdot\frac{cost(optima)}{N}$, where $N$ is the number of cities, $cost(optima)$ is the cost of a local optimum found by a local search, and $\alpha\in (0,1]$ (around 0.3 for TSP and 2-opt). The cost of a local optima was fixed to the approximated value of 15000 for the Berlin52 instance. 
% utility
The utility function for features (edges) in the TSP is $U_{edge}=\frac{D_{edge}}{1+P_{edge}}$, where $U_{edge}$ is the utility for penalizing an edge (maximizing), $D_{edge}$ is the cost of the edge (distance between cities) and $P_{edge}$ is the current penalty for the edge.

% the listing
\lstinputlisting[firstline=7,language=ruby,caption=Guided Local Search in Ruby, label=guided_local_search]{../src/algorithms/stochastic/guided_local_search.rb}

% References: Deeper understanding
% The references element description includes a listing of both primary sources of information about the technique as well as useful introductory sources for novices to gain a deeper understanding of the theory and application of the technique. The description consists of hand-selected reference material including books, peer reviewed conference papers, journal articles, and potentially websites. A bullet-pointed structure is suggested.
\subsection{References}
% What are the primary sources for a technique?
% What are the suggested reference sources for learning more about a technique?

% 
% Primary Sources
% 
\subsubsection{Primary Sources}
% based on GNET
Guided Local Search emerged from an approach called GENET, which is a connectionist approach to constraint satisfaction \cite{Wang1991, Tsang1992}.
% seminal papers
Guided Local Search was presented by Voudouris and Tsang in a series of technical reports (that were later published) that described the technique and provided example applications of it to constraint satisfaction \cite{Voudouris1994}, combinatorial optimization \cite{Voudouris1995b, Voudouris1995}, and function optimization \cite{Voudouris1995a}.
% thesis
The seminal work on the technique was Voudouris' PhD dissertation \cite{Voudouris1997}.


% 
% Learn More
% 
\subsubsection{Learn More}
% review
Voudouris and Tsang provide a high-level introduction to the technique \cite{Voudouris1998}, and a contemporary summary of the approach in Glover and Kochenberger's `Handbook of metaheuristics' \cite{Glover2003a} that includes a review of the technique, application areas, and demonstration applications on a diverse set of problem instances.
% Mills' extensions
Mills et al.\ elaborated on the approach, devising an `Extended Guided Local Search' (EGLS) technique that added `aspiration criteria' and random moves to the procedure \cite{Mills2003}, work which culminated in Mills' PhD dissertation \cite{Mills2002}.  
% Lau's extensions
Lau and Tsang further extended the approach by integrating it with a Genetic Algorithm, called the `Guided Genetic Algorithm' (GGA) \cite{Lau1998}, that also culminated in a PhD dissertation by Lau \cite{Lau1999}.


