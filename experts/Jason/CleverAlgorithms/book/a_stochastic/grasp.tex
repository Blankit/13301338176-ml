% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is an algorithm description, see:
% Jason Brownlee. A Template for Standardized Algorithm Descriptions. Technical Report CA-TR-20100107-1, The Clever Algorithms Project http://www.CleverAlgorithms.com, January 2010.

% Name
% The algorithm name defines the canonical name used to refer to the technique, in addition to common aliases, abbreviations, and acronyms. The name is used in terms of the heading and sub-headings of an algorithm description.
\section{Greedy Randomized Adaptive Search} 
\label{sec:grasp}
\index{Greedy Randomized Adaptive Search}
\index{GRASP}

% other names
% What is the canonical name and common aliases for a technique?
% What are the common abbreviations and acronyms for a technique?
\emph{Greedy Randomized Adaptive Search Procedure, GRASP.}

% Taxonomy: Lineage and locality
% The algorithm taxonomy defines where a techniques fits into the field, both the specific subfields of Computational Intelligence and Biologically Inspired Computation as well as the broader field of Artificial Intelligence. The taxonomy also provides a context for determining the relation- ships between algorithms. The taxonomy may be described in terms of a series of relationship statements or pictorially as a venn diagram or a graph with hierarchical structure.
\subsection{Taxonomy}
% To what fields of study does a technique belong?
% What are the closely related approaches to a technique?
% where
The Greedy Randomized Adaptive Search Procedure is a Metaheuristic and Global Optimization algorithm, originally proposed for the Operations Research practitioners.
% relations
The iterative application of an embedded Local Search technique relate the approach to Iterative Local Search (Section~\ref{sec:iterated_local_search}) and Multi-Start techniques.

% Strategy: Problem solving plan
% The strategy is an abstract description of the computational model. The strategy describes the information processing actions a technique shall take in order to achieve an objective. The strategy provides a logical separation between a computational realization (procedure) and a analogous system (metaphor). A given problem solving strategy may be realized as one of a number specific algorithms or problem solving systems. The strategy description is textual using information processing and algorithmic terminology.
\subsection{Strategy}
% What is the information processing objective of a technique?
The objective of the Greedy Randomized Adaptive Search Procedure is to repeatedly sample stochastically greedy solutions, and then use a local search procedure to refine them to a local optima.
% What is a techniques plan of action?
The strategy of the procedure is centered on the stochastic and greedy step-wise construction mechanism that constrains the selection and order-of-inclusion of the components of a solution based on the value they are expected to provide.

% Procedure: Abstract computation
% The algorithmic procedure summarizes the specifics of realizing a strategy as a systemized and parameterized computation. It outlines how the algorithm is organized in terms of the data structures and representations. The procedure may be described in terms of software engineering and computer science artifacts such as Pseudocode, design diagrams, and relevant mathematical equations.
\subsection{Procedure}
% What is the computational recipe for a technique?
% What are the data structures and representations used in a technique?
Algorithm~\ref{alg:grasp} provides a pseudocode listing of the Greedy Randomized Adaptive Search Procedure for minimizing a cost function.

\begin{algorithm}[htp]
	\SetLine
	% data
	\SetKwData{Best}{$S_{best}$}
	\SetKwData{Candidate}{$S_{candidate}$}
	\SetKwData{Alpha}{$\alpha$}
	% functions
	\SetKwFunction{Cost}{Cost}
	\SetKwFunction{GreedyRandomizedConstruction}{GreedyRandomizedConstruction}
	\SetKwFunction{StopCondition}{StopCondition}
	\SetKwFunction{ConstructRandomSolution}{ConstructRandomSolution}
  	\SetKwFunction{LocalSearch}{LocalSearch}
	% I/O
	\KwIn{\Alpha}
	\KwOut{\Best}
  	% Algorithm
	\Best $\leftarrow$ \ConstructRandomSolution{}\;
	\While{$\neg$ \StopCondition{}} {
		% greedy randomized solution
		\Candidate $\leftarrow$ \GreedyRandomizedConstruction{\Alpha}\;
		% local search
		\Candidate $\leftarrow$ \LocalSearch{\Candidate}\;
		% keep best
		\If{\Cost{\Candidate} $<$ \Cost{\Best}} {
			\Best $\leftarrow$ \Candidate\;
		}
	}
	\Return{\Best}\;
	% caption
	\caption{Pseudocode for the GRASP.}
	\label{alg:grasp}
\end{algorithm}

Algorithm~\ref{alg:construction} provides the pseudocode the Greedy Randomized Construction function. The function involves the step-wise construction of a candidate solution using a stochastically greedy construction process. The function works by building a Restricted Candidate List (RCL) that constraints the components of a solution (features) that may be selected from each cycle. The RCL may be constrained by an explicit size, or by using a threshold ($\alpha \in [0,1]$) on the cost of adding each feature to the current candidate solution.

\begin{algorithm}[htp]
	\SetLine
	% data
	\SetKwData{Candidate}{$S_{candidate}$}
	\SetKwData{RCL}{RCL}
	\SetKwData{Alpha}{$\alpha$}
	\SetKwData{ProblemSize}{ProblemSize}
	\SetKwData{FeatureCosts}{$Feature_{costs}$}	
	\SetKwData{ProblemFeature}{$Feature_{i}$}
	\SetKwData{ProblemFeatureCost}{$F_{i}cost$}
	\SetKwData{ProblemFeatureMinCost}{$Fcost_{min}$}
	\SetKwData{ProblemFeatureMaxCost}{$Fcost_{max}$}
	\SetKwData{SelectedFeature}{$F_{selected}$}	
	% functions
	\SetKwFunction{MinCost}{MinCost}
	\SetKwFunction{MaxCost}{MaxCost}
	\SetKwFunction{CostImpactOfFeature}{CostOfAddingFeatureToSolution}
	\SetKwFunction{SelectRandomFeature}{SelectRandomFeature}

	% I/O
	\KwIn{\Alpha}
	\KwOut{\Candidate}
	% greedy randomized solution
	\Candidate $\leftarrow \emptyset$\;
	\While{\Candidate $\neq$ \ProblemSize} {
		% calculate
		\FeatureCosts $\leftarrow \emptyset$\;
		\For{\ProblemFeature $\notin$ \Candidate}{
			\FeatureCosts $\leftarrow$ \CostImpactOfFeature{\Candidate, \ProblemFeature}\;
		}
		% build RCL
		\RCL $\leftarrow \emptyset$\;
		\ProblemFeatureMinCost $\leftarrow$ \MinCost{\FeatureCosts}\;
		\ProblemFeatureMaxCost $\leftarrow$ \MaxCost{\FeatureCosts}\;
		\For{\ProblemFeatureCost $\in$ \FeatureCosts} {
			\If{\ProblemFeatureCost $\leq$ $\ProblemFeatureMinCost + \alpha \cdot (\ProblemFeatureMaxCost - \ProblemFeatureMinCost)$ } {
				\RCL $\leftarrow$ \ProblemFeature\;
			}
		}
		% select
		\Candidate $\leftarrow$ \SelectRandomFeature{\RCL}\;
	}
	\Return{\Candidate}\;
	% caption
	\caption{Pseudocode the \texttt{Greedy\-Randomized\-Construction} function.}
	\label{alg:construction}
\end{algorithm}

% Heuristics: Usage guidelines
% The heuristics element describe the commonsense, best practice, and demonstrated rules for applying and configuring a parameterized algorithm. The heuristics relate to the technical details of the techniques procedure and data structures for general classes of application (neither specific implementations not specific problem instances). The heuristics are described textually, such as a series of guidelines in a bullet-point structure.
\subsection{Heuristics}
% What are the suggested configurations for a technique?
% What are the guidelines for the application of a technique to a problem instance?
\begin{itemize}
	\item The $\alpha$ threshold defines the amount of greediness of the construction mechanism, where values close to 0 may be too greedy, and values close to 1 may be too generalized.
	\item As an alternative to using the $\alpha$ threshold, the RCL can be constrained to the top $n\%$ of candidate features that may be selected from each construction cycle.
	\item The technique was designed for discrete problem classes such as combinatorial optimization problems.
\end{itemize}

% The code description provides a minimal but functional version of the technique implemented with a programming language. The code description must be able to be typed into an appropriate computer, compiled or interpreted as need be, and provide a working execution of the technique. The technique implementation also includes a minimal problem instance to which it is applied, and both the problem and algorithm implementations are complete enough to demonstrate the techniques procedure. The description is presented as a programming source code listing.
\subsection{Code Listing}
% How is a technique implemented as an executable program?
% How is a technique applied to a concrete problem instance?
Listing~\ref{grasp} provides an example of the Greedy Randomized Adaptive Search Procedure implemented in the Ruby Programming Language. 
% problem
The algorithm is applied to the Berlin52 instance of the Traveling Salesman Problem (TSP), taken from the TSPLIB. The problem seeks a permutation of the order to visit cities (called a tour) that minimizes the total distance traveled. The optimal tour distance for Berlin52 instance is 7542 units.

% algorithm
The stochastic and greedy step-wise construction of a tour involves evaluating candidate cities by the the cost they contribute as being the next city in the tour. 
% local search
The algorithm uses a stochastic 2-opt procedure for the Local Search with a fixed number of non-improving iterations as the stopping condition.

% the listing
\lstinputlisting[firstline=7,language=ruby,caption=Greedy Randomized Adaptive Search Procedure in Ruby, label=grasp]{../src/algorithms/stochastic/grasp.rb}


% References: Deeper understanding
% The references element description includes a listing of both primary sources of information about the technique as well as useful introductory sources for novices to gain a deeper understanding of the theory and application of the technique. The description consists of hand-selected reference material including books, peer reviewed conference papers, journal articles, and potentially websites. A bullet-pointed structure is suggested.
\subsection{References}
% What are the primary sources for a technique?
% What are the suggested reference sources for learning more about a technique?

% 
% Primary Sources
% 
\subsubsection{Primary Sources}
% seminal
The seminal paper that introduces the general approach of stochastic and greedy step-wise construction of candidate solutions is by Feo and Resende \cite{Feo1989}. The general approach was inspired by greedy heuristics by Hart and Shogan \cite{Hart1987}. The seminal review paper that is cited with the preliminary paper is by Feo and Resende \cite{Feo1995}, and provides a coherent description of the GRASP technique, an example, and review of early applications.
% early papers
An early application was by Feo, Venkatraman and Bard for a machine scheduling problem \cite{Feo1991}. Other early applications to scheduling problems include technical reports \cite{Feo1993} (later published as \cite{Bard1996}) and \cite{Feo1994} (also later published as \cite{Feo1996}).

% 
% Learn More
% 
\subsubsection{Learn More}
% overview
There are a vast number of review, application, and extension papers for GRASP.
% reviews
Pitsoulis and Resende provide an extensive contemporary overview of the field as a review chapter \cite{Pitsoulis2002}, as does Resende and Ribeiro that includes a clear presentation of the use of the $\alpha$ threshold parameter instead of a fixed size for the RCL \cite{Resende2003}. Festa and Resende provide an annotated bibliography as a review chapter that provides some needed insight into large amount of study that has gone into the approach \cite{Festa2002}.
% extensions
There are numerous extensions to GRASP, not limited to the popular Reactive GRASP for adapting $\alpha$ \cite{Prais2000}, the use of long term memory to allow the technique to learn from candidate solutions discovered in previous iterations, and parallel implementations of the procedure such as `Parallel GRASP' \cite{Pardalos1995}.


