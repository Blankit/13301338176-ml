<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <meta name="GENERATOR" content="Microsoft FrontPage 4.0">
   <meta name="Author" content="Todd Neller">
   <title>Chapter 3</title>
</head>
<body bgcolor="#FFFFFF">
<table WIDTH="100%" >
<tr>
<td><img SRC="GburgCS.gif" height=84 width=182></td>

<td><font size=+2>CS 391 - Special Topic: Machine Learning</font>
<br><font size=+2>Chapter 3</font></td>
</tr>
</table>

<hr>
<br><b><u>Readings</u></b>
<ul>
<li>
1/29: Sections 3.1-3.6</li>

<li>
1/31: Sections 3.7-3.10</li>
</ul>
<b><u>Topics</u></b>
<ul>
  <li>
    <p class="MsoNormal">Reinforcement learning problem architecture</li>
  <li>
    <p class="MsoNormal">Goals, rewards, and returns</li>
  <li>
    <p class="MsoNormal">Discounting</li>
  <li>
    <p class="MsoNormal">Episodic and continuing tasks</li>
  <li>
    <p class="MsoNormal">The Markov property</li>
  <li>
    <p class="MsoNormal">Markov decision processes</li>
  <li>
    <p class="MsoNormal">State- and action-value functions</li>
  <li>
    <p class="MsoNormal">Optimal value functions</li>
  <li>
    <p class="MsoNormal">Approximating optimality</li>
</ul>
<p>
<b><u>Discussion Questions</u></b>
</p>
<ul>
  <li>
    <p class="MsoNormal">List our RL base Java classes.<span style="mso-spacerun:
yes">&nbsp; </span>How do they relate to one another?</li>
  <li>
    <p class="MsoNormal">How can one decide where to divide agent from
    environment?</li>
  <li>
    <p class="MsoNormal">For a problem involving a person or animal, where would
    you place this division and why?</li>
  <li>
    <p class="MsoNormal">Describe an example of a RL problem and sketch how the
    base classes would represent the problem.</li>
  <li>
    <p class="MsoNormal">Describe the relationship between an agent’s goal,
    the RL problem rewards, and returns?<span style="mso-spacerun: yes">&nbsp; </span>Is
    the goal implicitly or explicitly defined?<span style="mso-spacerun: yes">&nbsp;
    </span>Why?</li>
  <li>
    <p class="MsoNormal">What is an episodic task?<span style="mso-spacerun: yes">&nbsp;
    </span>What is a continuing task?</li>
  <li>
    <p class="MsoNormal">Why is the sum-of-rewards computation for return
    problematic for continuing tasks?</li>
  <li>
    <p class="MsoNormal">Write the equation for the discounted return (3.2).<span style="mso-spacerun: yes">&nbsp;
    </span>How does discounting address this problem?</li>
  <li>
    <p class="MsoNormal">Exercise 3.4</li>
  <li>
    <p class="MsoNormal">Exercise 3.5</li>
  <li>
    <p class="MsoNormal">How can we formulate an episodic task as a continuing
    task?<span style="mso-spacerun: yes">&nbsp; </span>What do we need to
    introduce?</li>
  <li>
    <p class="MsoNormal">In this unified formalism, the return (3.3) can include
    “the possibility that T = infinity or gamma = 1 (but not both)”.<span style="mso-spacerun: yes">&nbsp;
    </span>Why does each preclude the other?</li>
  <li>
    <p class="MsoNormal">What is the Markov property?<span style="mso-spacerun:
yes">&nbsp; </span>Why is this sometimes referred to as an “independence of
    path” property?</li>
  <li>
    <p class="MsoNormal">Can we use non-Markovian state signals in reinforcement
    learning?</li>
  <li>
    <p class="MsoNormal">What does it mean for a state to be an “approximation
    to a Markov state”?</li>
  <li>
    <p class="MsoNormal">What is a Markov decision process (MDP)?</li>
  <li>
    <p class="MsoNormal">What is finite in a finite MDP?</li>
  <li>
    <p class="MsoNormal">Write and explain the formal definition of the
    state-value function for policy pi (V^pi).</li>
  <li>
    <p class="MsoNormal">Write and explain the formal definition of the
    action-value function for policy pi (Q^pi).</li>
  <li>
    <p class="MsoNormal">Write and explain the formal definition of the Bellman
    equation for V^pi.</li>
  <li>
    <p class="MsoNormal">What are backup diagrams?<span style="mso-spacerun: yes">&nbsp;
    </span>What to open and solid circles represent?</li>
  <li>
    <p class="MsoNormal">Draw the backup diagrams for V^pi and Q^pi.</li>
  <li>
    <p class="MsoNormal">Exercise 3.8</li>
  <li>
    <p class="MsoNormal">Exercise 3.10</li>
  <li>
    <p class="MsoNormal">Exercise 3.12</li>
  <li>
    <p class="MsoNormal">Exercise 3.13</li>
  <li>
    <p class="MsoNormal">Define an optimal policy pi* in terms of
    state-value function V^pi* compared with V^pi for any policy pi.
    Do the same using action-value function Q^pi*.  Is it possible to
    have more than one optimal policy?</li>
   <li>
    <p class="MsoNormal">Once one has the optimal state-value function
    V*, how do you determine the optimal policy?</li>
   <li>
    <p class="MsoNormal">What are the computational difficulties (in
    time and space complexity) one faces in computing optimal policies
    for MDPs?  In practical situations, what does one generally do to
    cope with these difficulties? </li>
</ul>
<p><b><u>Programming Assignment</u></b>
<p><b><u>HW3</u></b></p>
<p>Due Monday 2/10 at the beginning of class.&nbsp; An improvised, informal
presentation of your work may be requested in class.</p>
<p>Apply one of the techniques from chapter 2 to the Jack's Car Rental problem
(Example 4.2, pp. 98-99).&nbsp; For this, you will need to create your own
environment to the specification.&nbsp; Also, note that this is an <i>associative</i>
problem, so you will need to have a simple state class.&nbsp; When you start a
trial, uniformly pick a random possible starting state.&nbsp; Since this is a
continuing problem, you will not reach a terminal (a.k.a absorbing) state.&nbsp;
Terminate each trial after a maximum number of steps of your choosing.&nbsp; DO
NOT return your agent to a naive state at the start of each trial.</p>
<p>Since this is an associative problem, it is recommended that you keep a table
to keep track of action-value Q estimates for each possible state-action
pair.&nbsp; One update rule you can use following the incremental implementation
pattern of section 2.5 is:</p>
<p>Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + alpha * ((r_{t+1} + gamma * argmax_a
Q_t(s_{t+1},a)) - Q_t(s_t,a_t))</p>
<p>where alpha is the stepsize and gamma is given in Example 4.2 as
0.9.&nbsp;&nbsp; (Underscores are used here to indicate subscripts.)&nbsp; This
is an example of what is known as Q-learning (Chapter 6).&nbsp; It is likely
that you will have to have many trials to get adequate experience to approximate
optimal behavior.&nbsp; At the end of your all of your trials, print out the
approximate optimal policy your agent has learned.&nbsp; Compare this policy
with that shown in Figure 4.4.&nbsp; A table of numbers is adequate, although a
contour plot like that of Figure 4.4 or a 3D plot (x = cars at loc. #1, y = cars
at loc. #2, z = cars moved) is preferred.</p>
<p>Reminder:&nbsp; Use/extend/implement the reinforcement base classes provided.</p>
</body>
</html>
