<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <meta name="GENERATOR" content="Microsoft FrontPage 12.0">
   <meta name="Author" content="Todd Neller">
   <title>ISLR Fourth Hour Assignments</title>
   <style type="text/css">
   .style1 {
	   background-color: #FFFF00;
   }
   .style2 {
	   text-align: left;
   }
   .style3 {
	text-decoration: underline;
}
   </style>
</head>
<body bgcolor="#FFFFFF">
&nbsp;
<table WIDTH="100%" >
<tr>
<td style="width: 212px"><img SRC="GburgCS.gif" height=84 width=182></td>

<td><font size=+2>CS 371 - Introduction to Artificial Intelligence</font>
<br><font size="+2">ISLR Weekly Assignments</font></td>
</tr>
</table>

<hr>
<br><b>Due: Fridays before midnight</b><br><b>Note: Work is to be done 
<span class="style1">in pairs</span></b>.
<h1>Index</h1>
<ul>
	<li><a href="#Introduction">Introduction</a></li>
	<li><a href="#Week_1">Week 1</a></li>
	<li><a href="#Week_2">Week 2</a></li>
	<li><a href="#Week_3">Week 3</a></li>
	<li><a href="#Week_4">Week 4</a></li>
	<li><a href="#Week_5">Week 5</a></li>
	<li><a href="#Week_6">Week 6</a></li>
	<li><a href="#Week_7">Week 7</a></li>
	<li><a href="#Week_8">Week 8</a></li>
	<li><a href="#Week_9">Week 9</a></li>
	<li><a href="#Week_10">Week 10</a></li>
</ul>
<h1><a name="Introduction">Introduction</a></h1>
<p>Gettysburg College requires a "fourth hour" component to each course that meets less than four lecture hours per week.  This course satisfies the fourth 
hour component by allocating an additional three hours per week to <strong>independent</strong> 
learning and exercises performed beyond lecture.&nbsp; Given that Computer 
Science as a discipline and especially Artificial Intelligence as a 
subdiscipline experience rapid change and introduction of new algorithmic 
techniques, your ability to read and acquire new knowledge is key to your 
lifelong learning and thus professional success.&nbsp; Consider this activity to 
be a model for the type of self-instruction you should pursue in tandem with 
your future career.</p>
<p>The basis of this semester's fourth hour requirement will be a guided, weekly 
self-study of Machine Learning techniques through the excellent, freely available book
<a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical 
Learning with Applications in R</a> (ISLR) by <a href="http://www-bcf.usc.edu/~gareth">
Gareth James</a>,&nbsp;<a href="http://www.biostat.washington.edu/~dwitten/">Daniela 
Witten</a>,&nbsp;<a href="http://www.stanford.edu/~hastie/">Trevor Hastie</a>&nbsp;and&nbsp;<a href="http://www-stat.stanford.edu/~tibs/">Robert 
Tibshirani</a>.&nbsp; Supplemented with video lectures, each week will require 
approximately 3 hours per week of reading, video, text lab exercise, and Moodle 
quiz assessment due each Friday.&nbsp; These quizzes will account for <strong>
20% of the final grade.</strong></p>
<p>While I am glad to assist students in their understanding of the material, 
the nature of this fourth hour course component requires significant 
<a href="../resources/gtd/summary.html">independent time 
management and work discipline</a> for success.&nbsp; I recommend that students 
adopt the following weekly work rhythm:</p>
<ul>
	<li>By Monday:
	<ul>
		<li>Note the week's main topics.</li>
		<li>Read Moodle quiz questions.</li>
		<li>Complete readings and video viewing, so as to be able to ask 
		questions (should you have any) during my first office hours of the 
		week.</li>
	</ul>
	</li>
	<li>By Wednesday:<ul>
		<li>Complete text lab exercises and begin Moodle assessment exercises</li>
	</ul>
	</li>
	<li>By Thursday:<ul>
		<li>Attempt completion of the entire week's work so as to leave 
	an extra day as a contingency plan for when the work is more difficult that 
	expected.</li>
	</ul>
	</li>
	<li>By Friday midnight:<ul>
		<li>Complete and submit Moodle assessment exercises.</li>
	</ul>
	</li>
</ul>
<p><span class="style3">Historical note:</span> &#8220;Machine Learning (ML)&#8221;, the 
oldest term for this material, is a Computer Science Artificial Intelligence 
subfield encompassing and making use of all relevant techniques of from 
statistical and AI roots, but giving greater attention to low bias, high 
variance techniques (e.g. artificial neural networks).&nbsp; &#8220;Data Mining&#8221; was a 
&#8220;rebranding&#8221; of ML with special attention to big data applications.&nbsp; 
&#8220;Statistical Learning&#8221;, the newest term coined by statisticians, generally gives 
greater attention to low variance, high bias techniques.&nbsp; The vast majority of 
work in these three similar areas concern regression, classification, and 
clustering problems.&nbsp; The ISLR text does an excellent job of surveying the 
landscape and helping one discern the tradeoffs and motivations for the use of 
these diverse techniques to address common problems.</p>
<h1><a name="Week_1">Week 1</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 1:<ul>
		<li>Basic definitions</li>
	</ul>
	</li>
	<li>Chapter 2: Statistical Learning<ul>
		<li>Statistical learning goal: estimate f</li>
		<li>Prediction versus model interpretability tradeoff </li>
		<li>Common problem classes: supervised learning (e.g. regression, 
		classification) versus unsupervised learning ( e.g. clustering)</li>
	</ul>
	</li>
</ul>
<ol>
	<li>If you wish to perform R exercises on your personal machine, 	<a href="https://www.r-project.org/">download and install R</a>.</li>
	<li>Download the PDF textbook
<a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical 
Learning with Applications in R</a> (ISLR) to your favorite device for reading.</li>
	<li>Import library &quot;ISLR&quot; within R.&nbsp; For my installation:<ul>
		<li>Download all datasets the
		<a href="https://cran.r-project.org/web/packages/ISLR/index.html">ISLR R 
		package</a> with all datasets for the text.</li>
	</ul>
	</li>
	<li>Read ISLR chapter 1 and chapter 2 through section 2.1.2 (pp. 1-24).</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 1: Introduction (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/introduction.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5ICcUhueCyu25slvsGp8IDTa">playlist</a>)
		<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=2wLfFB_6SKI&amp;list=PL5-da3qGB5ICcUhueCyu25slvsGp8IDTa">
			Opening Remarks and Examples</a>&nbsp;(18:18)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=LvaTokhYnDw&amp;list=PL5-da3qGB5ICcUhueCyu25slvsGp8IDTa">
			Supervised and Unsupervised Learning</a>&nbsp;(12:12)</li>
		</ul>
		</li>
		<li>Chapter 2: Statistical Learning (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/statistical_learning.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">playlist</a>)<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=WjyuiK5taS8&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">
			Statistical Learning and Regression</a>&nbsp;(11:41)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=UvxHOkYQl8g&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">
			Curse of Dimensionality and Parametric Models</a>&nbsp;(11:40) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 1.</li>
</ol>
<h1><a name="Week_2">Week 2</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 2: Statistical Learning</li>
	<ul>
		<li>Prediction versus model interpretability tradeoff</li>
		<li>Common problem classes: supervised learning (e.g. regression, 
		classification) versus unsupervised learning (e.g. clustering) </li>
		<li>Assessing model accuracy: MSE Error</li>
		<li>Bias-variance tradeoff</li>
		<li>Basic introduction to R </li>
	</ul>
</ul>
<ol>
	<li>Read sections 2.1.3 through the end of chapter 2 (p. 51).&nbsp; Do the 
	guided lab of section 2.3.&nbsp; (This isn't to be submitted, but builds 
	your ability to apply the reading in the context of R.)</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 2: Statistical Learning (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/statistical_learning.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">playlist</a>)<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=VusKAosxxyk&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">
			Assessing Model Accuracy and Bias-Variance Trade-off</a>&nbsp;(10:04)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=vVj2itVNku4&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">
			Classification Problems and K-Nearest Neighbors</a>&nbsp;(15:37)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=jwBgGS_4RQA&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">
			Lab: Introduction to R</a>&nbsp;(14:12) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 2.</li>
</ol>
<h1><a name="Week_3">Week 3</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 3:
	<ul>
		<li>Simple linear regression<ul>
			<li>Coefficient estimation</li>
			<li>Assessing the accuracy of coefficient estimates</li>
			<li>Assessing the accuracy of the model</li>
		</ul>
		</li>
	</ul>
	</li>
	<ul>
		<li>Multiple linear regression<ul>
			<li>Relationships between response and predictors</li>
			<li>Predictor selection</li>
			<li>Assessing model fit</li>
			<li>Prediction and confidence in prediction</li>
		</ul>
		</li>
	</ul>
</ul>
<ol>
	<li>Read chapter 3 through the end of section 3.2 (p. 82).&nbsp; Do the 
	guided lab of section 3.6 through 3.6.3.</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 3: Linear Regression (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/linear_regression.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">playlist</a>)
		<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=PsE9UqoWtS4&amp;list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">
			Simple Linear Regression and Confidence Intervals</a>&nbsp;(13:01)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=J6AdoiNUyWI&amp;list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">
			Hypothesis Testing</a>&nbsp;(8:24)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=1hbCJyM9ccs&amp;list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">
			Multiple Linear Regression and Interpreting Regression Coefficients</a>&nbsp;(15:38)
			</li>
			<li>
			<a href="https://www.youtube.com/watch?v=3T6RXmIHbJ4&amp;list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">
			Model Selection and Qualitative Predictors</a>&nbsp;(14:51) (Model 
			Selection)</li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 3.&nbsp; 
	You'll need these datasets: <a href="islr/week3_1.csv">week3_1.csv</a>,
	<a href="islr/week3_2.csv">week3_2.csv</a></li>
</ol>
<h1><a name="Week_4">Week 4</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 3:<ul>
		<li>Qualitative predictors</li>
		<li>Interaction terms</li>
		<li>Non-linear relationships and polynomial regression</li>
		<li>Common problems</li>
		<li>Application to advertising data</li>
		<li>Linear Regression versus K-Nearest Neighbors</li>
	</ul>
	</li>
</ul>
<ol>
	<li>Complete reading through the end of chapter 3, completing the remainder 
	of the guided lab (p. 119).</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 3: Linear Regression (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/linear_regression.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">playlist</a>)
		<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=3T6RXmIHbJ4&amp;list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">
			Model Selection and Qualitative Predictors</a>&nbsp;(14:51) 
			(Qualitative Predictors)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=IFzVxLv0TKQ&amp;list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">
			Interactions and Nonlinearity</a>&nbsp;(14:16)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=5ONFqIk3RFg&amp;list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">
			Lab: Linear Regression</a>&nbsp;(22:10) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 4.&nbsp; 
	You'll need these datasets: <a href="islr/iris.csv">iris.csv</a>,
	<a href="islr/week4_1.csv">week4_1.csv</a>, <a href="islr/week4_2.csv">
	week4_2.csv</a>, <a href="islr/week4_3.csv">week4_3.csv</a></li>
</ol>
<h1><a name="Week_5">Week 5</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 4: Classification<ul>
		<li>Logistic regression and multinomial logistic regression</li>
		<li>Linear discriminant analysis (LDA)</li>
		<li>Quadratic discriminant analysis (QDA)</li>
		<li>K-nearest neighbor classification</li>
	</ul>
	</li>
</ul>
<ol>
	<li>Read chapter 4 and complete the guided lab.</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 4: Classification (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/classification.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">playlist</a>)<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=sqq21-VIa1c&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Introduction to Classification</a>&nbsp;(10:25)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=31Q5FGRnxt4&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Logistic Regression and Maximum Likelihood</a>&nbsp;(9:07)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=MpX8rVv_u4E&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Multivariate Logistic Regression and Confounding</a>&nbsp;(9:53)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=GavRXXEHGqU&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Case-Control Sampling and Multiclass Logistic Regression</a>&nbsp;(7:28)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=RfrGiG1Hm3M&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Linear Discriminant Analysis and Bayes Theorem</a>&nbsp;(7:12)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=QG0pVJXT6EU&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Univariate Linear Discriminant Analysis</a>&nbsp;(7:37)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=X4VDZDp2vqw&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Multivariate Linear Discriminant Analysis and ROC Curves</a>&nbsp;(17:42)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=6FiNGTYAOAA&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Quadratic Discriminant Analysis and Naive Bayes</a>&nbsp;(10:07)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=TxvEVc8YNlU&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Lab: Logistic Regression</a>&nbsp;(10:14)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=2cl7JiPzkBY&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Lab: Linear Discriminant Analysis</a>&nbsp;(8:22)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=9TVVF7CS3F4&amp;list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE">
			Lab: K-Nearest Neighbors</a>&nbsp;(5:01) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 5.&nbsp; 
	You'll need these datasets: <a href="islr/iris.csv">iris.csv</a>,
	<a href="islr/week5.csv">week5.csv</a>.&nbsp; You'll need to prepare the 
	iris dataset for classification according to <a href="islr/class-data.html">these instructions</a>.</li>
</ol>
<h1><a name="Week_6">Week 6</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 5: Validation<ul>
		<li>Cross-Validation<ul>
			<li>Validation set method</li>
			<li>Leave-one-out cross validation (LOOCV)</li>
			<li>k-Fold cross validation and the bias-variance trade-off</li>
		</ul>
		</li>
		<li>The Bootstrap</li>
	</ul>
	</li>
</ul>
<ol>
	<li>Read chapter 5 and complete the guided lab.</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 5: Resampling Methods (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf">playlist</a>)<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=_2ij6eaaSl0&amp;list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf">
			Estimating Prediction Error and Validation Set Approach</a>&nbsp;(14:01)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=nZAM5OXrktY&amp;list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf">
			K-fold Cross-Validation</a>&nbsp;(13:33)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=S06JpVoNaA0&amp;list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf">
			Cross-Validation: The Right and Wrong Ways</a>&nbsp;(10:07)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=p4BYWX7PTBM&amp;list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf">
			The Bootstrap</a>&nbsp;(11:29)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=BzHz0J9a6k0&amp;list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf">
			More on the Bootstrap</a>&nbsp;(14:35)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=6dSXlqHAoMk&amp;list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf">
			Lab: Cross-Validation</a>&nbsp;(11:21)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=YVSmsWoBKnA&amp;list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf">
			Lab: The Bootstrap</a>&nbsp;(7:40) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 6.&nbsp; 
	This week, we'll look at the material beyond R to emphasize that you're not 
	limited to R for these common machine learning tasks.&nbsp; You'll use 
	Java-based Weka for polynomial regression and validation, and you use my 
	simple Java code for bootstrapping.&nbsp; For this, you will need:<ul>
		<li>A <a href="islr/weka-regression.mp4">tutorial video</a> for 
		polynomial regression and validation using Weka.</li>
		<li> <a href="islr/week4_2.csv">
	week4_2.csv</a>, <a href="islr/auto.csv">auto.csv</a></li>
		<li>A <a href="islr/bootstrap/bootstrap.mp4">tutorial video</a> for 
		bootstrapping with my Bootstrap.java code.</li>
		<li><a href="islr/bootstrap/Bootstrap.java">Bootstrap.java</a></li>
	</ul>
	</li>
</ol>
<h1><a name="Week_7">Week 7</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 7: Moving Beyond Linearity<ul>
		<li>Polynomial regression</li>
		<li>Step and basis functions</li>
		<li>Regression and smoothing splines</li>
		<li>Local regressions</li>
		<li>Generalized Additive Models (GAMs)</li>
	</ul>
	</li>
</ul>
<ol>
	<li>Read chapter 7 and complete the guided lab.</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 7: Moving Beyond Linearity (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/nonlinear.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">playlist</a>)<ul>
			<li>&nbsp;<a href="https://www.youtube.com/watch?v=gtXQXA7qF3c&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">Polynomial 
			Regression and Step Functions</a>&nbsp;(14:59)<a href="https://www.youtube.com/watch?v=7ZIqzTNB8lk&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=7ZIqzTNB8lk&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			Piecewise Polynomials and Splines</a>&nbsp;(13:13)<a href="https://www.youtube.com/watch?v=mxXHJa1DsWQ&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=mxXHJa1DsWQ&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			Smoothing Splines</a>&nbsp;(10:10)<a href="https://www.youtube.com/watch?v=N2hBXqPiegQ&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=N2hBXqPiegQ&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			Local Regression and Generalized Additive Models</a>&nbsp;(10:45)<a href="https://www.youtube.com/watch?v=uQBnDGu6TYU&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=uQBnDGu6TYU&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			Lab: Polynomials</a>&nbsp;(21:11)<a href="https://www.youtube.com/watch?v=DCn83aXXuHc&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=DCn83aXXuHc&amp;list=PL5-da3qGB5IBn84fvhh-u2MU80jvo8OoR">
			Lab: Splines and Generalized Additive Models</a>&nbsp;(12:15) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 
	7.&nbsp;
	For this, you will need datasets <a href="islr/week7_1.csv">week7_1.csv</a>,
	<a href="islr/week7_2.csv">week7_2.csv</a>, <a href="islr/week7_3.csv">
	week7_3.csv</a></li>
</ol>
<h1><a name="Week_8">Week 8</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 8: Tree-Based Methods<ul>
		<li>Regression and Classification Decision Tree</li>
		<li>Bagging</li>
		<li>Random Forests</li>
		<li>Boosting</li>
	</ul>
	</li>
</ul>
<ol>
	<li>Read chapter 8 and complete the guided lab.</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 8: Tree-Based Methods (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/trees.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">playlist</a>)
		<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=6ENTbK3yQUQ&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			Decision Trees</a>&nbsp;(14:37)<a href="https://www.youtube.com/watch?v=GfPR7Xhdokc&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=GfPR7Xhdokc&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			Pruning a Decision Tree</a>&nbsp;(11:45)<a href="https://www.youtube.com/watch?v=hPEJoITBbQ4&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=hPEJoITBbQ4&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			Classification Trees and Comparison with Linear Models</a>&nbsp;(11:00)<a href="https://www.youtube.com/watch?v=lq_xzBRIWm4&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=lq_xzBRIWm4&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			Bootstrap Aggregation (Bagging) and Random Forests</a>&nbsp;(13:45)<a href="https://www.youtube.com/watch?v=U3MdBNysk9w&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=U3MdBNysk9w&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			Boosting and Variable Importance</a>&nbsp;(12:03)<a href="https://www.youtube.com/watch?v=0wZUXtvAtDc&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=0wZUXtvAtDc&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			Lab: Decision Trees</a>&nbsp;(10:13)<a href="https://www.youtube.com/watch?v=IY7oWGXb77o&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=IY7oWGXb77o&amp;list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh">
			Lab: Random Forests and Boosting</a>&nbsp;(15:35) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 8.&nbsp; 
	For this, you will need dataset <a href="islr/carseats.csv">carseats.csv</a> 
	and the Weka application.</li>
</ol>
<h1><a name="Week_9">Week 9</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 9: Support Vector Machines<ul>
		<li>Maximal margin classifier</li>
		<li>Support vector classifier</li>
		<li>Support vector machines</li>
		<li>&nbsp;1-vs.-1 and 1-vs.-all classification with &gt;2 classes</li>
	</ul>
	</li>
</ul>
<ol>
	<li>Read chapter 9 and complete the guided lab.</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 9: Support Vector Machines (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/svm.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">playlist</a>)
		<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=QpbynqiTCsY&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			Maximal Margin Classifier</a>&nbsp;(11:35)<a href="https://www.youtube.com/watch?v=xKsTsGE7KpI&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=xKsTsGE7KpI&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			Support Vector Classifier</a>&nbsp;(8:04)<a href="https://www.youtube.com/watch?v=dm32QvCW7wE&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=dm32QvCW7wE&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			Kernels and Support Vector Machines</a>&nbsp;(15:04)<a href="https://www.youtube.com/watch?v=mI18GD4_ysE&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=mI18GD4_ysE&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			Example and Comparison with Logistic Regression</a>&nbsp;(14:47)<a href="https://www.youtube.com/watch?v=qhyyufR0930&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=qhyyufR0930&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			Lab: Support Vector Machine for Classification</a>&nbsp;(10:13)<a href="https://www.youtube.com/watch?v=L3n2VF7yKkk&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=L3n2VF7yKkk&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o">
			Lab: Nonlinear Support Vector Machine</a>&nbsp;(7:54) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 9. 
	For this, you will need dataset <a href="islr/rollHold.csv">rollHold.csv</a> 
	and the RStudio application with the e1071 library.</li>
</ol>
<h1><a name="Week_10">Week 10</a></h1>
<p>Main Topics:</p>
<ul>
	<li>Chapter 10: Unsupervised Learning<ul>
		<li>Principal Component Analysis</li>
		<li>K-means Clustering</li>
		<li>Hierarchical Clustering</li>
	</ul>
	</li>
</ul>
<ol>
	<li>Read chapter 10 and complete the guided lab.</li>
	<li>Optionally watch these supplementary videos:<ul>
		<li>Chapter 10: Unsupervised Learning (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/unsupervised.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">playlist</a>)
		<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=ipyxSYXgzjQ&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			Unsupervised Learning and Principal Components Analysis</a>&nbsp;(12:37)<a href="https://www.youtube.com/watch?v=dbuSGWCgdzw&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=dbuSGWCgdzw&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			Exploring Principal Components Analysis and Proportion of Variance 
			Explained</a>&nbsp;(17:39)<a href="https://www.youtube.com/watch?v=aIybuNt9ps4&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=aIybuNt9ps4&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			K-means Clustering</a>&nbsp;(17:17)<a href="https://www.youtube.com/watch?v=Tuuc9Y06tAc&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=Tuuc9Y06tAc&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			Hierarchical Clustering</a>&nbsp;(14:45)<a href="https://www.youtube.com/watch?v=yUJcTpWNY_o&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=yUJcTpWNY_o&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			Breast Cancer Example of Hierarchical Clustering</a>&nbsp;(9:24)<a href="https://www.youtube.com/watch?v=lFHISDj_4EQ&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=lFHISDj_4EQ&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			Lab: Principal Components Analysis</a>&nbsp;(6:28)<a href="https://www.youtube.com/watch?v=YDubYJsZ9iM&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=YDubYJsZ9iM&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			Lab: K-means Clustering</a>&nbsp;(6:31)<a href="https://www.youtube.com/watch?v=4u3zvtfqb7w&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			</a></li>
			<li>
			<a href="https://www.youtube.com/watch?v=4u3zvtfqb7w&amp;list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2">
			Lab: Hierarchical Clustering</a>&nbsp;(6:33) </li>
		</ul>
		</li>
	</ul>
	</li>
	<li>Take the weekly Moodle quiz to assess your learning by Friday of week 
	10.&nbsp; For this, you will need dataset <a href="islr/iris.csv">iris.csv</a>.</li>
</ol>
<!--
<ol>
	<li>Read ISLR chapter 2 (remainder) and chapter 3 through 3.1.2 p. 67 
	paragraph 1 (pp. 24-67).&nbsp;
	<ul>
		<li>Please note that for the lab of 2.3 (&quot;Introduction to R&quot;) you should 
		be not only reading, but working through the lab example (with your own 
		variant experimentation to gain comfort with R).</li>
	</ul>
	</li>
	<li>Watch these supplementary videos:</li>
	<ul>
		<li>Chapter 2: Statistical Learning (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/statistical_learning.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">playlist</a>)<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=VusKAosxxyk&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">
			Assessing Model Accuracy and Bias-Variance Trade-off</a>&nbsp;(10:04)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=vVj2itVNku4&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">
			Classification Problems and K-Nearest Neighbors</a>&nbsp;(15:37)</li>
			<li>
			<a href="https://www.youtube.com/watch?v=jwBgGS_4RQA&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">
			Lab: Introduction to R</a>&nbsp;(14:12)</li>
		</ul>
		</li>
		<li>Chapter 3: Linear Regression (<a href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/linear_regression.pdf">slides</a>,&nbsp;<a href="https://www.youtube.com/playlist?list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">playlist</a>)<ul>
			<li>
			<a href="https://www.youtube.com/watch?v=PsE9UqoWtS4&amp;list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9">
			Simple Linear Regression and Confidence Intervals</a>&nbsp;(13:01) </li>
		</ul>
		</li>
	</ul>
	<li>Take the weekly Moodle quiz to assess your learning.</li>
</ol>
-->
<hr>
<p class="style2">
<strong>Acknowledgements</strong>: My thanks to
<a href="http://www-stat.stanford.edu/~hastie/">Trevor Hastie</a>,
<a href="http://www-stat.stanford.edu/~tibs/">Robert Tibshirani</a>, and
<a href="http://www-stat.stanford.edu/~jhf">Jerome Friedman</a> for their 
excellent, freely available book
<a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of 
Statistical Learning</a> (ESL), to <a href="http://www-bcf.usc.edu/~gareth">
Gareth James</a>,&nbsp;<a href="http://www.biostat.washington.edu/~dwitten/">Daniela 
Witten</a>,&nbsp;<a href="http://www.stanford.edu/~hastie/">Trevor Hastie</a>&nbsp;and&nbsp;<a href="http://www-stat.stanford.edu/~tibs/">Robert 
Tibshirani</a> for their excellent, freely available book
<a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical 
Learning with Applications in R</a> (ISLR) that provides a gentler, more 
accessible introduction to the important concepts of ESL, to
<a href="http://www-stat.stanford.edu/~hastie/">Trevor Hastie</a> and
<a href="http://www-stat.stanford.edu/~tibs/">Robert Tibshirani</a> for making
<a href="https://www.youtube.com/channel/UC4OWDcPB1peiBXDfCSZ3h-w">ISLR lecture 
video available via YouTube</a>, and to
<a href="https://www.linkedin.com/in/justmarkham">Kevin Markham</a> for creating 
an
<a href="http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/">
index to these videos and associated slides</a>.<p><a href="http://cs.gettysburg.edu/~tneller/">Todd Neller</a>
<br><img SRC="tneller-email.gif" height=20 width=143>
</body>
</html>